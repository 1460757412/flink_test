2023-03-31 10:34:11  [ main:2 ] - [ ERROR ]  Could not load service provider for factories.
java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.table.planner.delegation.DefaultDialectFactory could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232)
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185)
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:623)
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:378)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:295)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:266)
	at org.apache.flink.table.api.TableEnvironment.create(TableEnvironment.java:95)
	at org.example.Main.main(Main.java:12)
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/table/delegation/ExtendedOperationExecutor
	at java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671)
	at java.lang.Class.getConstructor0(Class.java:3075)
	at java.lang.Class.newInstance(Class.java:412)
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)
	... 9 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.delegation.ExtendedOperationExecutor
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 14 more
2023-03-31 10:35:07  [ main:2 ] - [ ERROR ]  Could not load service provider for factories.
java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Provider org.apache.flink.table.planner.delegation.DefaultDialectFactory could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232)
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185)
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:623)
	at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:378)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:295)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:266)
	at org.apache.flink.table.api.TableEnvironment.create(TableEnvironment.java:95)
	at org.example.Main.main(Main.java:12)
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/table/delegation/ExtendedOperationExecutor
	at java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671)
	at java.lang.Class.getConstructor0(Class.java:3075)
	at java.lang.Class.newInstance(Class.java:412)
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)
	... 9 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.delegation.ExtendedOperationExecutor
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 14 more
2023-03-31 11:13:13  [ main:0 ] - [ INFO ]  Found configuration file file:/Users/renzhuo/IdeaProjects/Flink_test/target/classes/hive-site.xml
2023-03-31 11:13:14  [ main:429 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-03-31 11:13:14  [ main:527 ] - [ INFO ]  Setting hive conf dir as src/main/resources
2023-03-31 11:13:14  [ main:941 ] - [ INFO ]  Created HiveCatalog 'batch_test'
2023-03-31 11:13:14  [ main:959 ] - [ INFO ]  Trying to connect to metastore with URI thrift://t-d-datastorage-srv03:9083
2023-03-31 11:13:14  [ main:980 ] - [ INFO ]  Opened a connection to metastore, current connections: 1
2023-03-31 11:13:14  [ main:1234 ] - [ INFO ]  Connected to metastore.
2023-03-31 11:13:14  [ main:1297 ] - [ INFO ]  Connected to Hive metastore
2023-03-31 11:13:17  [ main:4247 ] - [ WARN ]  HiveConf of name hive.vectorized.use.checked.expressions does not exist
2023-03-31 11:13:17  [ main:4248 ] - [ WARN ]  HiveConf of name hive.strict.checks.no.partition.filter does not exist
2023-03-31 11:13:17  [ main:4249 ] - [ WARN ]  HiveConf of name hive.strict.checks.orderby.no.limit does not exist
2023-03-31 11:13:17  [ main:4249 ] - [ WARN ]  HiveConf of name hive.vectorized.adaptor.usage.mode does not exist
2023-03-31 11:13:17  [ main:4249 ] - [ WARN ]  HiveConf of name hive.vectorized.input.format.excludes does not exist
2023-03-31 11:13:17  [ main:4249 ] - [ WARN ]  HiveConf of name hive.strict.checks.bucketing does not exist
2023-03-31 11:13:17  [ main:4260 ] - [ INFO ]  Trying to connect to metastore with URI thrift://t-d-datastorage-srv02:9083
2023-03-31 11:13:17  [ main:4265 ] - [ INFO ]  Opened a connection to metastore, current connections: 2
2023-03-31 11:13:18  [ main:4384 ] - [ INFO ]  Connected to metastore.
2023-03-31 11:13:18  [ main:4598 ] - [ INFO ]  Closed a connection to metastore, current connections: 1
2023-03-31 11:15:20  [ main:0 ] - [ INFO ]  Found configuration file file:/Users/renzhuo/IdeaProjects/Flink_test/target/classes/hive-site.xml
2023-03-31 11:15:20  [ main:456 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-03-31 11:15:20  [ main:536 ] - [ INFO ]  Setting hive conf dir as src/main/resources
2023-03-31 11:15:21  [ main:938 ] - [ INFO ]  Created HiveCatalog 'batch_test'
2023-03-31 11:15:21  [ main:954 ] - [ INFO ]  Trying to connect to metastore with URI thrift://t-d-datastorage-srv02:9083
2023-03-31 11:15:21  [ main:974 ] - [ INFO ]  Opened a connection to metastore, current connections: 1
2023-03-31 11:15:21  [ main:1082 ] - [ INFO ]  Connected to metastore.
2023-03-31 11:15:21  [ main:1142 ] - [ INFO ]  Connected to Hive metastore
2023-03-31 11:15:24  [ main:4054 ] - [ INFO ]  Trying to connect to metastore with URI thrift://t-d-datastorage-srv02:9083
2023-03-31 11:15:24  [ main:4057 ] - [ INFO ]  Opened a connection to metastore, current connections: 2
2023-03-31 11:15:24  [ main:4224 ] - [ INFO ]  Connected to metastore.
2023-03-31 11:15:24  [ main:4429 ] - [ INFO ]  Closed a connection to metastore, current connections: 1
2023-03-31 11:18:27  [ main:0 ] - [ INFO ]  Found configuration file file:/Users/renzhuo/IdeaProjects/Flink_test/target/classes/hive-site.xml
2023-03-31 11:18:27  [ main:409 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-03-31 11:18:27  [ main:494 ] - [ INFO ]  Setting hive conf dir as src/main/resources
2023-03-31 11:18:28  [ main:889 ] - [ INFO ]  Created HiveCatalog 'batch_test'
2023-03-31 11:18:28  [ main:905 ] - [ INFO ]  Trying to connect to metastore with URI thrift://t-d-datastorage-srv03:9083
2023-03-31 11:18:28  [ main:993 ] - [ INFO ]  Opened a connection to metastore, current connections: 1
2023-03-31 11:18:28  [ main:1117 ] - [ INFO ]  Connected to metastore.
2023-03-31 11:18:28  [ main:1180 ] - [ INFO ]  Connected to Hive metastore
2023-03-31 11:21:29  [ main:0 ] - [ INFO ]  Found configuration file file:/Users/renzhuo/IdeaProjects/Flink_test/target/classes/hive-site.xml
2023-03-31 11:21:30  [ main:441 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-03-31 11:21:30  [ main:524 ] - [ INFO ]  Setting hive conf dir as src/main/resources
2023-03-31 11:21:30  [ main:912 ] - [ INFO ]  Created HiveCatalog 'batch_test'
2023-03-31 11:21:30  [ main:929 ] - [ INFO ]  Trying to connect to metastore with URI thrift://t-d-datastorage-srv03:9083
2023-03-31 11:21:31  [ main:1235 ] - [ INFO ]  Opened a connection to metastore, current connections: 1
2023-03-31 11:21:31  [ main:1319 ] - [ INFO ]  Connected to metastore.
2023-03-31 11:21:31  [ main:1399 ] - [ INFO ]  Connected to Hive metastore
2023-03-31 11:21:34  [ main:4714 ] - [ INFO ]  Trying to connect to metastore with URI thrift://t-d-datastorage-srv02:9083
2023-03-31 11:21:34  [ main:4738 ] - [ INFO ]  Opened a connection to metastore, current connections: 2
2023-03-31 11:21:34  [ main:4747 ] - [ INFO ]  Connected to metastore.
2023-03-31 11:21:34  [ main:4951 ] - [ INFO ]  Closed a connection to metastore, current connections: 1
2023-03-31 11:24:14  [ main:0 ] - [ INFO ]  Found configuration file file:/Users/renzhuo/IdeaProjects/Flink_test/target/classes/hive-site.xml
2023-03-31 11:24:14  [ main:18 ] - [ DEBUG ]  Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:454)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:425)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:502)
	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.findHadoopBinary(HiveConf.java:3192)
	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.<clinit>(HiveConf.java:549)
	at org.apache.hadoop.hive.conf.HiveConf.<clinit>(HiveConf.java:143)
	at org.apache.flink.table.catalog.hive.HiveCatalog.createHiveConf(HiveCatalog.java:251)
	at org.apache.flink.table.catalog.hive.HiveCatalog.<init>(HiveCatalog.java:179)
	at org.apache.flink.table.catalog.hive.factories.HiveCatalogFactory.createCatalog(HiveCatalogFactory.java:76)
	at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:289)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1292)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1122)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:742)
	at org.example.Main.main(Main.java:16)
2023-03-31 11:24:14  [ main:37 ] - [ DEBUG ]  setsid is not available on this machine. So not using it.
2023-03-31 11:24:14  [ main:38 ] - [ DEBUG ]  setsid exited with exit code 0
2023-03-31 11:24:14  [ main:246 ] - [ DEBUG ]  Handling deprecation for all properties in config...
2023-03-31 11:24:14  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.exec.reducers.bytes.per.reducer
2023-03-31 11:24:14  [ main:248 ] - [ DEBUG ]  Handling deprecation for hive.server2.tez.sessions.init.threads
2023-03-31 11:24:14  [ main:248 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.createtable.group.grants
2023-03-31 11:24:14  [ main:248 ] - [ DEBUG ]  Handling deprecation for datanucleus.storeManagerType
2023-03-31 11:24:14  [ main:248 ] - [ DEBUG ]  Handling deprecation for hive.aux.jars.path
2023-03-31 11:24:14  [ main:248 ] - [ DEBUG ]  Handling deprecation for hive.llap.zk.registry.user
2023-03-31 11:24:14  [ main:248 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggregate.stats.false.positive.probability
2023-03-31 11:24:14  [ main:248 ] - [ DEBUG ]  Handling deprecation for hive.exec.stagingdir
2023-03-31 11:24:14  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.merge.rcfile.block.level
2023-03-31 11:24:14  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.execution.mode
2023-03-31 11:24:14  [ main:250 ] - [ DEBUG ]  Handling deprecation for hive.exec.default.partition.name
2023-03-31 11:24:14  [ main:255 ] - [ DEBUG ]  Handling deprecation for mapreduce.input.fileinputformat.split.minsize.per.rack
2023-03-31 11:24:14  [ main:255 ] - [ DEBUG ]  Handling deprecation for hive.metastore.event.expiry.duration
2023-03-31 11:24:14  [ main:255 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.default.compress
2023-03-31 11:24:14  [ main:255 ] - [ DEBUG ]  Handling deprecation for hive.exec.mode.local.auto.input.files.max
2023-03-31 11:24:14  [ main:255 ] - [ DEBUG ]  Handling deprecation for hive.cbo.cnf.maxnodes
2023-03-31 11:24:14  [ main:255 ] - [ DEBUG ]  Handling deprecation for hive.stats.key.prefix
2023-03-31 11:24:14  [ main:255 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.orc.time.counters
2023-03-31 11:24:14  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.orc.splits.ms.footer.cache.ppd.enabled
2023-03-31 11:24:14  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.tez.task.scale.memory.reserve-fraction.min
2023-03-31 11:24:14  [ main:257 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled
2023-03-31 11:24:14  [ main:257 ] - [ DEBUG ]  Handling deprecation for hive.optimize.skewjoin.compiletime
2023-03-31 11:24:14  [ main:257 ] - [ DEBUG ]  Handling deprecation for hive.smbjoin.cache.rows
2023-03-31 11:24:14  [ main:257 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.mapjoin.overflow.repeated.threshold
2023-03-31 11:24:14  [ main:257 ] - [ DEBUG ]  Handling deprecation for hive.server2.metrics.enabled
2023-03-31 11:24:14  [ main:257 ] - [ DEBUG ]  Handling deprecation for hive.tez.log.level
2023-03-31 11:24:14  [ main:257 ] - [ DEBUG ]  Handling deprecation for hive.merge.mapfiles
2023-03-31 11:24:14  [ main:257 ] - [ DEBUG ]  Handling deprecation for hive.exec.post.hooks
2023-03-31 11:24:14  [ main:257 ] - [ DEBUG ]  Handling deprecation for hive.metastore.client.socket.lifetime
2023-03-31 11:24:14  [ main:257 ] - [ DEBUG ]  Handling deprecation for fs.har.impl
2023-03-31 11:24:14  [ main:260 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.max.variance
2023-03-31 11:24:14  [ main:261 ] - [ DEBUG ]  Handling deprecation for mapreduce.input.fileinputformat.split.minsize
2023-03-31 11:24:14  [ main:261 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.quorum
2023-03-31 11:24:14  [ main:261 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.ldap.groupMembershipKey
2023-03-31 11:24:14  [ main:261 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.catalog.cache.size
2023-03-31 11:24:14  [ main:261 ] - [ DEBUG ]  Handling deprecation for stream.stderr.reporter.prefix
2023-03-31 11:24:14  [ main:261 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.hybridgrace.memcheckfrequency
2023-03-31 11:24:14  [ main:261 ] - [ DEBUG ]  Handling deprecation for hive.optimize.index.filter.compact.maxsize
2023-03-31 11:24:14  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.exec.counters.pull.interval
2023-03-31 11:24:14  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.security.command.whitelist
2023-03-31 11:24:14  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.metastore.end.function.listeners
2023-03-31 11:24:14  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.llap.zk.sm.connectionString
2023-03-31 11:24:14  [ main:263 ] - [ DEBUG ]  Handling deprecation for hive.downloaded.resources.dir
2023-03-31 11:24:14  [ main:263 ] - [ DEBUG ]  Handling deprecation for hive.join.emit.interval
2023-03-31 11:24:14  [ main:263 ] - [ DEBUG ]  Handling deprecation for hive.llap.am.liveness.connection.timeout.ms
2023-03-31 11:24:14  [ main:263 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.zerocopy
2023-03-31 11:24:14  [ main:263 ] - [ DEBUG ]  Handling deprecation for hive.metastore.fshandler.threads
2023-03-31 11:24:14  [ main:263 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.client.connect.retry.limit
2023-03-31 11:24:14  [ main:263 ] - [ DEBUG ]  Handling deprecation for hive.compute.query.using.stats
2023-03-31 11:24:14  [ main:263 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.block.padding.tolerance
2023-03-31 11:24:14  [ main:263 ] - [ DEBUG ]  Handling deprecation for hive.lazysimple.extended_boolean_literal
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.orc.splits.include.file.footer
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.error.on.empty.partition
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.prewarm.enabled
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.allocator.direct
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.io.rcfile.record.buffer.size
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hadoop.bin.path
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.default.rcfile.serde
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.llap.management.acl.blocked
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for datanucleus.schema.validateConstraints
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.users.in.admin.role
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.createtable.owner.grants
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.multi.insert.move.tasks.share.dependencies
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.autogen.columnalias.prefix.includefuncname
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.tez.max.partition.factor
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.port
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.orc.cache.stripe.details.size
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.task.scheduler.wait.queue.size
2023-03-31 11:24:14  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggr.stats.cache.entries
2023-03-31 11:24:14  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.exec.max.created.files
2023-03-31 11:24:14  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.cli.prompt
2023-03-31 11:24:14  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.stats.deserialization.factor
2023-03-31 11:24:14  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.enforce.stats
2023-03-31 11:24:14  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.metadata.export.location
2023-03-31 11:24:14  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.log.explain.output
2023-03-31 11:24:14  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.optimize.skewjoin
2023-03-31 11:24:14  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.default.fileformat
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for hive.llap.client.consistent.splits
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.optimized.hashtable.wbsize
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for hive.server2.tez.session.lifetime
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for hive.security.metastore.authorization.auth.reads
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggr.stats.memory.ttl
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.NonTransactionalRead
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for hive.optimize.remove.identity.project
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for hive.timedout.txn.reaper.start
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.rpc.port
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.cache.ttl
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for hive.exec.infer.bucket.sort.num.buckets.power.two
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for hive.compactor.worker.threads
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for hive.exim.strict.repl.tables
2023-03-31 11:24:14  [ main:266 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggregate.stats.cache.size
2023-03-31 11:24:14  [ main:267 ] - [ DEBUG ]  Handling deprecation for hive.llap.management.acl
2023-03-31 11:24:14  [ main:267 ] - [ DEBUG ]  Handling deprecation for hive.stats.collect.tablekeys
2023-03-31 11:24:14  [ main:267 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.use.vectorized.input.format
2023-03-31 11:24:14  [ main:267 ] - [ DEBUG ]  Handling deprecation for hive.optimize.cte.materialize.threshold
2023-03-31 11:24:14  [ main:267 ] - [ DEBUG ]  Handling deprecation for hive.display.partition.cols.separately
2023-03-31 11:24:14  [ main:268 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.future.timeout
2023-03-31 11:24:14  [ main:268 ] - [ DEBUG ]  Handling deprecation for hive.metastore.disallow.incompatible.col.type.changes
2023-03-31 11:24:14  [ main:268 ] - [ DEBUG ]  Handling deprecation for hive.server2.async.exec.shutdown.timeout
2023-03-31 11:24:14  [ main:268 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.max.idle.time
2023-03-31 11:24:14  [ main:268 ] - [ DEBUG ]  Handling deprecation for hive.test.dummystats.aggregator
2023-03-31 11:24:14  [ main:268 ] - [ DEBUG ]  Handling deprecation for hive.test.mode
2023-03-31 11:24:14  [ main:268 ] - [ DEBUG ]  Handling deprecation for hive.querylog.enable.plan.progress
2023-03-31 11:24:14  [ main:268 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.cookie.auth.enabled
2023-03-31 11:24:14  [ main:268 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.worker.keepalive.time
2023-03-31 11:24:14  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.cache.clean.until
2023-03-31 11:24:14  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.delegation.token.lifetime
2023-03-31 11:24:14  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.metastore.archive.intermediate.archived
2023-03-31 11:24:14  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.metastore.warehouse.dir
2023-03-31 11:24:14  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.hwi.listen.host
2023-03-31 11:24:14  [ main:274 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.ldap.guidKey
2023-03-31 11:24:14  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.stats.collect.scancols
2023-03-31 11:24:14  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.hwi.war.file
2023-03-31 11:24:14  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.tez.input.format
2023-03-31 11:24:14  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.test.dummystats.publisher
2023-03-31 11:24:14  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.metastore.port
2023-03-31 11:24:14  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.spark.dynamic.partition.pruning
2023-03-31 11:24:14  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.strict.checks.large.query
2023-03-31 11:24:14  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.cookie.is.httponly
2023-03-31 11:24:14  [ main:280 ] - [ DEBUG ]  Handling deprecation for hive.metastore.uris
2023-03-31 11:24:14  [ main:280 ] - [ DEBUG ]  Handling deprecation for hive.querylog.location
2023-03-31 11:24:14  [ main:280 ] - [ DEBUG ]  Handling deprecation for hive.localize.resource.num.wait.attempts
2023-03-31 11:24:14  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.default.stripe.size
2023-03-31 11:24:14  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.querylog.plan.progress.interval
2023-03-31 11:24:14  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.limit.optimize.enable
2023-03-31 11:24:14  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.exec.job.debug.timeout
2023-03-31 11:24:14  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.createtable.role.grants
2023-03-31 11:24:14  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.decode.partition.name
2023-03-31 11:24:14  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.metastore.partition.inherit.table.properties
2023-03-31 11:24:14  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.cluster.delegation.token.store.class
2023-03-31 11:24:14  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.metastore.metrics.enabled
2023-03-31 11:24:14  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.default.row.index.stride
2023-03-31 11:24:14  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.cookie.max.age
2023-03-31 11:24:14  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.client.port
2023-03-31 11:24:14  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.alias
2023-03-31 11:24:14  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.exponential.backoff.slot.length
2023-03-31 11:24:14  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.mapjoin.native.enabled
2023-03-31 11:24:14  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.server2.tez.default.queues
2023-03-31 11:24:14  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.compat
2023-03-31 11:24:14  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.mapred.partitioner
2023-03-31 11:24:14  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.allocator.alloc.min
2023-03-31 11:24:14  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.async.log.enabled
2023-03-31 11:24:14  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.merge.smallfiles.avgsize
2023-03-31 11:24:14  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.client.user
2023-03-31 11:24:14  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.hbase.wal.enabled
2023-03-31 11:24:14  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.entity.capture.transform
2023-03-31 11:24:14  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.allow.udf.load.on.demand
2023-03-31 11:24:14  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.server2.logging.operation.enabled
2023-03-31 11:24:14  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.index.blockfilter.file
2023-03-31 11:24:14  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.lockmgr.zookeeper.default.partition.name
2023-03-31 11:24:14  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.wait.queue.comparator.class.name
2023-03-31 11:24:14  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.support.concurrency
2023-03-31 11:24:14  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.output.service.port
2023-03-31 11:24:14  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.orc.cache.use.soft.references
2023-03-31 11:24:14  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.file.max.footer
2023-03-31 11:24:14  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.cli.tez.session.async
2023-03-31 11:24:14  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.test.mode.prefix
2023-03-31 11:24:14  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.cli.print.header
2023-03-31 11:24:14  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.server2.table.type.mapping
2023-03-31 11:24:14  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.tez.task.scale.memory.reserve.fraction.max
2023-03-31 11:24:14  [ main:285 ] - [ DEBUG ]  Handling deprecation for hive.metastore.event.db.listener.timetolive
2023-03-31 11:24:14  [ main:285 ] - [ DEBUG ]  Handling deprecation for hive.exec.tasklog.debug.timeout
2023-03-31 11:24:14  [ main:285 ] - [ DEBUG ]  Handling deprecation for hive.hashtable.loadfactor
2023-03-31 11:24:14  [ main:285 ] - [ DEBUG ]  Handling deprecation for hive.metastore.filter.hook
2023-03-31 11:24:14  [ main:285 ] - [ DEBUG ]  Handling deprecation for hive.mapred.local.mem
2023-03-31 11:24:14  [ main:285 ] - [ DEBUG ]  Handling deprecation for hive.optimize.union.remove
2023-03-31 11:24:14  [ main:285 ] - [ DEBUG ]  Handling deprecation for hive.server2.global.init.file.location
2023-03-31 11:24:14  [ main:285 ] - [ DEBUG ]  Handling deprecation for hive.metastore.client.drop.partitions.using.expressions
2023-03-31 11:24:14  [ main:286 ] - [ DEBUG ]  Handling deprecation for hive.outerjoin.supports.filters
2023-03-31 11:24:14  [ main:286 ] - [ DEBUG ]  Handling deprecation for hive.script.auto.progress
2023-03-31 11:24:14  [ main:286 ] - [ DEBUG ]  Handling deprecation for hive.exec.dynamic.partition
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.metastore.failure.retries
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.metastore.try.direct.sql
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.tez.container.max.java.heap.fraction
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.intermediate.compression.type
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.hbase.generatehfiles
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.analyze.stmt.collect.partlevel.stats
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.stats.join.factor
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.metastore.pre.event.listeners
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.server2.map.fair.scheduler.queue
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.stats.column.autogather
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.localize.resource.wait.interval
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.optimize.reducededuplication
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.am.liveness.heartbeat.interval.ms
2023-03-31 11:24:14  [ main:287 ] - [ DEBUG ]  Handling deprecation for hive.optimize.index.filter.compact.minsize
2023-03-31 11:24:14  [ main:288 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.decoding.metrics.percentiles.intervals
2023-03-31 11:24:14  [ main:288 ] - [ DEBUG ]  Handling deprecation for hive.metastore.sasl.enabled
2023-03-31 11:24:14  [ main:289 ] - [ DEBUG ]  Handling deprecation for hive.exec.copyfile.maxsize
2023-03-31 11:24:14  [ main:291 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.enabled
2023-03-31 11:24:14  [ main:292 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.manager
2023-03-31 11:24:14  [ main:292 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.compression.strategy
2023-03-31 11:24:14  [ main:292 ] - [ DEBUG ]  Handling deprecation for hive.rpc.query.plan
2023-03-31 11:24:14  [ main:292 ] - [ DEBUG ]  Handling deprecation for hive.tez.bucket.pruning.compat
2023-03-31 11:24:14  [ main:292 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.spnego.principal
2023-03-31 11:24:14  [ main:293 ] - [ DEBUG ]  Handling deprecation for hive.merge.mapredfiles
2023-03-31 11:24:14  [ main:293 ] - [ DEBUG ]  Handling deprecation for hive.cache.expr.evaluation
2023-03-31 11:24:14  [ main:293 ] - [ DEBUG ]  Handling deprecation for yarn.bin.path
2023-03-31 11:24:14  [ main:293 ] - [ DEBUG ]  Handling deprecation for hive.counters.group.name
2023-03-31 11:24:14  [ main:293 ] - [ DEBUG ]  Handling deprecation for datanucleus.transactionIsolation
2023-03-31 11:24:14  [ main:293 ] - [ DEBUG ]  Handling deprecation for hive.in.test
2023-03-31 11:24:14  [ main:293 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.spnego.keytab
2023-03-31 11:24:14  [ main:293 ] - [ DEBUG ]  Handling deprecation for hive.groupby.skewindata
2023-03-31 11:24:14  [ main:293 ] - [ DEBUG ]  Handling deprecation for hive.metastore.txn.store.impl
2023-03-31 11:24:14  [ main:294 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.clean.until
2023-03-31 11:24:14  [ main:294 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.hybridgrace.hashtable
2023-03-31 11:24:14  [ main:294 ] - [ DEBUG ]  Handling deprecation for hive.stats.reliable
2023-03-31 11:24:14  [ main:294 ] - [ DEBUG ]  Handling deprecation for hive.metastore.batch.retrieve.max
2023-03-31 11:24:14  [ main:294 ] - [ DEBUG ]  Handling deprecation for hive.entity.separator
2023-03-31 11:24:14  [ main:294 ] - [ DEBUG ]  Handling deprecation for hive.binary.record.max.length
2023-03-31 11:24:14  [ main:295 ] - [ DEBUG ]  Handling deprecation for hive.llap.object.cache.enabled
2023-03-31 11:24:14  [ main:295 ] - [ DEBUG ]  Handling deprecation for hive.exec.max.dynamic.partitions
2023-03-31 11:24:14  [ main:295 ] - [ DEBUG ]  Handling deprecation for hive.map.groupby.sorted
2023-03-31 11:24:14  [ main:295 ] - [ DEBUG ]  Handling deprecation for hive.hashtable.initialCapacity
2023-03-31 11:24:14  [ main:295 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.check.memory.rows
2023-03-31 11:24:14  [ main:295 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.task.preemption.metrics.intervals
2023-03-31 11:24:14  [ main:296 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.shuffle.dir.watcher.enabled
2023-03-31 11:24:14  [ main:296 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.allocator.arena.count
2023-03-31 11:24:14  [ main:296 ] - [ DEBUG ]  Handling deprecation for hive.server2.idle.operation.timeout
2023-03-31 11:24:14  [ main:296 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.default.block.size
2023-03-31 11:24:14  [ main:297 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.hdfs.read
2023-03-31 11:24:14  [ main:297 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.communicator.connection.timeout.ms
2023-03-31 11:24:14  [ main:297 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.server.connect.timeout
2023-03-31 11:24:14  [ main:297 ] - [ DEBUG ]  Handling deprecation for hive.server2.parallel.ops.in.session
2023-03-31 11:24:14  [ main:297 ] - [ DEBUG ]  Handling deprecation for hive.server2.transport.mode
2023-03-31 11:24:14  [ main:297 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.path
2023-03-31 11:24:14  [ main:298 ] - [ DEBUG ]  Handling deprecation for hive.metastore.execute.setugi
2023-03-31 11:24:14  [ main:298 ] - [ DEBUG ]  Handling deprecation for hive.index.compact.query.max.entries
2023-03-31 11:24:14  [ main:298 ] - [ DEBUG ]  Handling deprecation for hive.transpose.aggr.join
2023-03-31 11:24:14  [ main:298 ] - [ DEBUG ]  Handling deprecation for mapreduce.input.fileinputformat.split.maxsize
2023-03-31 11:24:14  [ main:298 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.bucket.cache.size
2023-03-31 11:24:14  [ main:298 ] - [ DEBUG ]  Handling deprecation for hive.exec.drop.ignorenonexistent
2023-03-31 11:24:14  [ main:299 ] - [ DEBUG ]  Handling deprecation for hive.groupby.limit.extrastep
2023-03-31 11:24:14  [ main:299 ] - [ DEBUG ]  Handling deprecation for hive.spark.dynamic.partition.pruning.max.data.size
2023-03-31 11:24:14  [ main:299 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggr.stats.invalidator.frequency
2023-03-31 11:24:14  [ main:299 ] - [ DEBUG ]  Handling deprecation for hive.serdes.using.metastore.for.schema
2023-03-31 11:24:14  [ main:299 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.allocator.mmap
2023-03-31 11:24:14  [ main:299 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.use.lrfu
2023-03-31 11:24:14  [ main:299 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.resultset.max.fetch.size
2023-03-31 11:24:14  [ main:299 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.use.ssl
2023-03-31 11:24:14  [ main:300 ] - [ DEBUG ]  Handling deprecation for hive.test.mode.nosamplelist
2023-03-31 11:24:14  [ main:302 ] - [ DEBUG ]  Handling deprecation for hive.merge.sparkfiles
2023-03-31 11:24:14  [ main:304 ] - [ DEBUG ]  Handling deprecation for hive.exim.uri.scheme.whitelist
2023-03-31 11:24:14  [ main:306 ] - [ DEBUG ]  Handling deprecation for hive.conf.hidden.list
2023-03-31 11:24:14  [ main:306 ] - [ DEBUG ]  Handling deprecation for hive.exec.query.redactor.hooks
2023-03-31 11:24:14  [ main:306 ] - [ DEBUG ]  Handling deprecation for hive.exec.log4j.file
2023-03-31 11:24:14  [ main:306 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.sasl.qop
2023-03-31 11:24:14  [ main:306 ] - [ DEBUG ]  Handling deprecation for hive.compactor.delta.num.threshold
2023-03-31 11:24:14  [ main:306 ] - [ DEBUG ]  Handling deprecation for hive.exec.plan
2023-03-31 11:24:14  [ main:306 ] - [ DEBUG ]  Handling deprecation for hive.script.serde
2023-03-31 11:24:14  [ main:306 ] - [ DEBUG ]  Handling deprecation for hive.log4j.file
2023-03-31 11:24:14  [ main:306 ] - [ DEBUG ]  Handling deprecation for hive.ddl.createtablelike.properties.whitelist
2023-03-31 11:24:14  [ main:308 ] - [ DEBUG ]  Handling deprecation for mapreduce.input.fileinputformat.split.minsize.per.node
2023-03-31 11:24:14  [ main:309 ] - [ DEBUG ]  Handling deprecation for hive.optimize.bucketmapjoin
2023-03-31 11:24:14  [ main:309 ] - [ DEBUG ]  Handling deprecation for hive.map.aggr.hash.percentmemory
2023-03-31 11:24:14  [ main:309 ] - [ DEBUG ]  Handling deprecation for hive.exec.job.debug.capture.stacktraces
2023-03-31 11:24:14  [ main:309 ] - [ DEBUG ]  Handling deprecation for hive.metastore.server.max.message.size
2023-03-31 11:24:14  [ main:309 ] - [ DEBUG ]  Handling deprecation for hive.cluster.delegation.token.store.zookeeper.acl
2023-03-31 11:24:14  [ main:309 ] - [ DEBUG ]  Handling deprecation for hive.service.metrics.file.location
2023-03-31 11:24:14  [ main:309 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.use.row.serde.deserialize
2023-03-31 11:24:14  [ main:309 ] - [ DEBUG ]  Handling deprecation for hive.server2.compile.lock.timeout
2023-03-31 11:24:14  [ main:309 ] - [ DEBUG ]  Handling deprecation for hive.sample.seednumber
2023-03-31 11:24:14  [ main:309 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.client.retry.delay.seconds
2023-03-31 11:24:14  [ main:309 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.mapjoin.minmax.enabled
2023-03-31 11:24:14  [ main:310 ] - [ DEBUG ]  Handling deprecation for hive.metastore.event.clean.freq
2023-03-31 11:24:14  [ main:310 ] - [ DEBUG ]  Handling deprecation for hive.server2.session.hook
2023-03-31 11:24:14  [ main:310 ] - [ DEBUG ]  Handling deprecation for hive.mapred.reduce.tasks.speculative.execution
2023-03-31 11:24:14  [ main:310 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.sortmerge.join.bigtable.selection.policy
2023-03-31 11:24:14  [ main:310 ] - [ DEBUG ]  Handling deprecation for hive.stageid.rearrange
2023-03-31 11:24:14  [ main:310 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.groupby.flush.percent
2023-03-31 11:24:14  [ main:310 ] - [ DEBUG ]  Handling deprecation for hive.timedout.txn.reaper.interval
2023-03-31 11:24:14  [ main:310 ] - [ DEBUG ]  Handling deprecation for hive.exec.temporary.table.storage
2023-03-31 11:24:14  [ main:310 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.groupby.maxentries
2023-03-31 11:24:14  [ main:310 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.optimized.hashtable
2023-03-31 11:24:14  [ main:311 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggregate.stats.max.variance
2023-03-31 11:24:14  [ main:317 ] - [ DEBUG ]  Handling deprecation for hive.limit.optimize.fetch.max
2023-03-31 11:24:14  [ main:318 ] - [ DEBUG ]  Handling deprecation for hive.security.authenticator.manager
2023-03-31 11:24:14  [ main:318 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.num.file.cleaner.threads
2023-03-31 11:24:14  [ main:318 ] - [ DEBUG ]  Handling deprecation for hive.client.stats.publishers
2023-03-31 11:24:14  [ main:318 ] - [ DEBUG ]  Handling deprecation for hive.test.fail.compaction
2023-03-31 11:24:14  [ main:318 ] - [ DEBUG ]  Handling deprecation for hive.exec.parallel
2023-03-31 11:24:14  [ main:318 ] - [ DEBUG ]  Handling deprecation for hive.io.rcfile.record.interval
2023-03-31 11:24:14  [ main:318 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.lrfu.lambda
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.exec.submitviachild
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.fetch.task.conversion
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.service.metrics.class
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.udtf.auto.progress
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.archive.enabled
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.server2.builtin.udf.whitelist
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.rpc.max.size
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.spnego.principal
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.test.authz.sstd.hs2.mode
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.convert.join.bucket.mapjoin.tez
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.server2.async.exec.threads
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.execution.engine
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.allocator.mmap.path
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.download.permanent.fns
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.tez.container.size
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.ConnectionPassword
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.max.historic.queries
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.server2.use.SSL
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.reducesink.new.enabled
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.optimize.null.scan
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.compactor.max.num.delta
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.llap.zk.sm.keytab.file
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.compactor.history.retention.attempted
2023-03-31 11:24:14  [ main:319 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.size
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.smalltable.filesize
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.session.silent
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.query.string
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.port
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.min.worker.threads
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.join.use.nonstaged
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.compactor.initiator.failed.compacts.threshold
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.ldap.groupClassKey
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.server2.tez.sessions.per.default.queue
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.optimize.point.lookup
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.server2.idle.session.check.operation
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.port
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.llap.allow.permanent.fns
2023-03-31 11:24:14  [ main:320 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.web.ssl
2023-03-31 11:24:14  [ main:321 ] - [ DEBUG ]  Handling deprecation for hive.server2.logging.operation.log.location
2023-03-31 11:24:14  [ main:322 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.hybridgrace.minnumpartitions
2023-03-31 11:24:14  [ main:323 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.ConnectionURL
2023-03-31 11:24:14  [ main:324 ] - [ DEBUG ]  Handling deprecation for hive.semantic.analyzer.hook
2023-03-31 11:24:14  [ main:326 ] - [ DEBUG ]  Handling deprecation for hive.metastore.server.tcp.keepalive
2023-03-31 11:24:14  [ main:326 ] - [ DEBUG ]  Handling deprecation for hive.service.metrics.reporter
2023-03-31 11:24:14  [ main:326 ] - [ DEBUG ]  Handling deprecation for hive.hmshandler.force.reload.conf
2023-03-31 11:24:14  [ main:327 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.rpc.threads
2023-03-31 11:24:14  [ main:327 ] - [ DEBUG ]  Handling deprecation for hive.io.rcfile.column.number.conf
2023-03-31 11:24:14  [ main:327 ] - [ DEBUG ]  Handling deprecation for hive.map.aggr.hash.min.reduction
2023-03-31 11:24:14  [ main:327 ] - [ DEBUG ]  Handling deprecation for hive.compactor.job.queue
2023-03-31 11:24:14  [ main:327 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.cpu
2023-03-31 11:24:14  [ main:327 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.clean.extra.nodes
2023-03-31 11:24:14  [ main:327 ] - [ DEBUG ]  Handling deprecation for hive.optimize.metadataonly
2023-03-31 11:24:14  [ main:328 ] - [ DEBUG ]  Handling deprecation for datanucleus.schema.validateColumns
2023-03-31 11:24:14  [ main:328 ] - [ DEBUG ]  Handling deprecation for hive.insert.into.multilevel.dirs
2023-03-31 11:24:14  [ main:328 ] - [ DEBUG ]  Handling deprecation for hive.added.archives.path
2023-03-31 11:24:14  [ main:328 ] - [ DEBUG ]  Handling deprecation for hive.hmshandler.retry.attempts
2023-03-31 11:24:14  [ main:328 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.memory.pool
2023-03-31 11:24:14  [ main:328 ] - [ DEBUG ]  Handling deprecation for hive.prewarm.numcontainers
2023-03-31 11:24:14  [ main:328 ] - [ DEBUG ]  Handling deprecation for datanucleus.identifierFactory
2023-03-31 11:24:14  [ main:328 ] - [ DEBUG ]  Handling deprecation for hive.cli.errors.ignore
2023-03-31 11:24:14  [ main:328 ] - [ DEBUG ]  Handling deprecation for hive.multigroupby.singlereducer
2023-03-31 11:24:14  [ main:328 ] - [ DEBUG ]  Handling deprecation for hive.llap.execution.mode
2023-03-31 11:24:14  [ main:328 ] - [ DEBUG ]  Handling deprecation for hive.txn.manager.dump.lock.state.on.acquire.timeout
2023-03-31 11:24:14  [ main:328 ] - [ DEBUG ]  Handling deprecation for hive.compactor.history.retention.succeeded
2023-03-31 11:24:14  [ main:329 ] - [ DEBUG ]  Handling deprecation for hive.llap.enable.grace.join.in.llap
2023-03-31 11:24:14  [ main:329 ] - [ DEBUG ]  Handling deprecation for hive.conf.restricted.list
2023-03-31 11:24:14  [ main:329 ] - [ DEBUG ]  Handling deprecation for hive.optimize.sampling.orderby.number
2023-03-31 11:24:14  [ main:330 ] - [ DEBUG ]  Handling deprecation for hive.fetch.task.aggr
2023-03-31 11:24:14  [ main:330 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.sortmerge.join.to.mapjoin
2023-03-31 11:24:14  [ main:330 ] - [ DEBUG ]  Handling deprecation for hive.optimize.limittranspose
2023-03-31 11:24:14  [ main:330 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.memory.mode
2023-03-31 11:24:14  [ main:330 ] - [ DEBUG ]  Handling deprecation for hive.txn.timeout
2023-03-31 11:24:14  [ main:330 ] - [ DEBUG ]  Handling deprecation for hive.warehouse.subdir.inherit.perms
2023-03-31 11:24:14  [ main:330 ] - [ DEBUG ]  Handling deprecation for hive.stats.fetch.partition.stats
2023-03-31 11:24:14  [ main:330 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.use.fileid.path
2023-03-31 11:24:14  [ main:331 ] - [ DEBUG ]  Handling deprecation for hive.auto.progress.timeout
2023-03-31 11:24:14  [ main:331 ] - [ DEBUG ]  Handling deprecation for hive.cbo.returnpath.hiveop
2023-03-31 11:24:14  [ main:331 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.threadpool.size
2023-03-31 11:24:14  [ main:331 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.dictionary.key.size.threshold
2023-03-31 11:24:14  [ main:333 ] - [ DEBUG ]  Handling deprecation for hive.exec.scratchdir
2023-03-31 11:24:14  [ main:335 ] - [ DEBUG ]  Handling deprecation for hive.metastore.server.max.threads
2023-03-31 11:24:14  [ main:335 ] - [ DEBUG ]  Handling deprecation for hive.limit.optimize.limit.file
2023-03-31 11:24:14  [ main:335 ] - [ DEBUG ]  Handling deprecation for hive.exec.script.allow.partial.consumption
2023-03-31 11:24:14  [ main:335 ] - [ DEBUG ]  Handling deprecation for hive.metastore.try.direct.sql.ddl
2023-03-31 11:24:14  [ main:335 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.keystore.password
2023-03-31 11:24:14  [ main:335 ] - [ DEBUG ]  Handling deprecation for hive.scratchdir.lock
2023-03-31 11:24:14  [ main:335 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.namespace
2023-03-31 11:24:14  [ main:336 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.hybridgrace.minwbsize
2023-03-31 11:24:14  [ main:336 ] - [ DEBUG ]  Handling deprecation for hive.server2.long.polling.timeout
2023-03-31 11:24:14  [ main:337 ] - [ DEBUG ]  Handling deprecation for hive.debug.localtask
2023-03-31 11:24:14  [ main:337 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.use.spnego
2023-03-31 11:24:14  [ main:338 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.createtable.user.grants
2023-03-31 11:24:14  [ main:338 ] - [ DEBUG ]  Handling deprecation for hive.server.tcp.keepalive
2023-03-31 11:24:14  [ main:338 ] - [ DEBUG ]  Handling deprecation for hive.service.metrics.file.frequency
2023-03-31 11:24:14  [ main:338 ] - [ DEBUG ]  Handling deprecation for hive.optimize.ppd
2023-03-31 11:24:14  [ main:338 ] - [ DEBUG ]  Handling deprecation for hive.exec.script.maxerrsize
2023-03-31 11:24:14  [ main:338 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.worker.keepalive.time
2023-03-31 11:24:14  [ main:339 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.acl.blocked
2023-03-31 11:24:14  [ main:340 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.connect.timeout
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.session.id
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.enforce.bucketmapjoin
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.server2.allow.user.substitution
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.join.noconditionaltask
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.input.format
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.optimize.index.autoupdate
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.ssl.protocol.blacklist
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.stats.fetch.column.stats
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.tez.dynamic.partition.pruning
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.exec.max.dynamic.partitions.pernode
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.compactor.cleaner.run.interval
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.skewjoin.mapjoin.map.tasks
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for mapreduce.job.reduces
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.metastore.schema.verification.record.version
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.compactor.abortedtxn.threshold
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.scheduler.timeout.seconds
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.map.aggr
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.support.quoted.identifiers
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.optimize.filter.stats.reduction
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for javax.jdo.PersistenceManagerFactoryClass
2023-03-31 11:24:14  [ main:341 ] - [ DEBUG ]  Handling deprecation for hive.compactor.initiator.on
2023-03-31 11:24:14  [ main:342 ] - [ DEBUG ]  Handling deprecation for hive.orc.row.index.stride.dictionary.check
2023-03-31 11:24:14  [ main:342 ] - [ DEBUG ]  Handling deprecation for hive.metastore.fs.handler.class
2023-03-31 11:24:14  [ main:342 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.task.factory
2023-03-31 11:24:14  [ main:342 ] - [ DEBUG ]  Handling deprecation for hive.lock.numretries
2023-03-31 11:24:14  [ main:342 ] - [ DEBUG ]  Handling deprecation for hive.typecheck.on.insert
2023-03-31 11:24:14  [ main:342 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.join
2023-03-31 11:24:14  [ main:342 ] - [ DEBUG ]  Handling deprecation for hive.server2.support.dynamic.service.discovery
2023-03-31 11:24:14  [ main:342 ] - [ DEBUG ]  Handling deprecation for hive.optimize.distinct.rewrite
2023-03-31 11:24:14  [ main:342 ] - [ DEBUG ]  Handling deprecation for hive.metastore.authorization.storage.checks
2023-03-31 11:24:14  [ main:342 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.skip.corrupt.data
2023-03-31 11:24:14  [ main:342 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.base.delta.ratio
2023-03-31 11:24:14  [ main:342 ] - [ DEBUG ]  Handling deprecation for hive.metastore.fastpath
2023-03-31 11:24:14  [ main:343 ] - [ DEBUG ]  Handling deprecation for hive.llap.zk.sm.principal
2023-03-31 11:24:14  [ main:345 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.optimized.hashtable.probe.percent
2023-03-31 11:24:14  [ main:346 ] - [ DEBUG ]  Handling deprecation for datanucleus.cache.level2
2023-03-31 11:24:14  [ main:346 ] - [ DEBUG ]  Handling deprecation for hive.server2.builtin.udf.blacklist
2023-03-31 11:24:14  [ main:346 ] - [ DEBUG ]  Handling deprecation for hive.test.fail.heartbeater
2023-03-31 11:24:14  [ main:346 ] - [ DEBUG ]  Handling deprecation for datanucleus.schema.validateTables
2023-03-31 11:24:14  [ main:346 ] - [ DEBUG ]  Handling deprecation for hive.metastore.kerberos.principal
2023-03-31 11:24:14  [ main:346 ] - [ DEBUG ]  Handling deprecation for datanucleus.rdbms.useLegacyNativeValueStrategy
2023-03-31 11:24:14  [ main:346 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.scheduler.node.reenable.min.timeout.ms
2023-03-31 11:24:14  [ main:346 ] - [ DEBUG ]  Handling deprecation for hive.llap.file.cleanup.delay.seconds
2023-03-31 11:24:14  [ main:346 ] - [ DEBUG ]  Handling deprecation for hive.llap.management.rpc.port
2023-03-31 11:24:14  [ main:346 ] - [ DEBUG ]  Handling deprecation for hive.optimize.ppd.storage
2023-03-31 11:24:14  [ main:346 ] - [ DEBUG ]  Handling deprecation for hive.support.special.characters.tablename
2023-03-31 11:24:14  [ main:346 ] - [ DEBUG ]  Handling deprecation for hive.llap.validate.acls
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.mv.files.thread
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.llap.skip.compile.udf.check
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.index.compact.binary.search
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.local.fs.read
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.hybridgrace.bloomfilter
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.max.full
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.enabled
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.optimize.correlation
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.cookie.is.secure
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.reorder.nway.joins
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.merge.orcfile.stripe.level
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.exec.compress.output
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.user.install.directory
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.stats.list.num.entries
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.sqlstd.confwhitelist.append
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.insert.into.external.tables
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.groupby.checkinterval
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.explain.dependency.append.tasktype
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.optimize.bucketingsorting
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.login.timeout
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.scratch.dir.permission
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.cli.print.current.db
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.hashtable.key.count.adjustment
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.exec.failure.hooks
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.metastore.integral.jdo.pushdown
2023-03-31 11:24:14  [ main:347 ] - [ DEBUG ]  Handling deprecation for hive.txn.retryable.sqlex.regex
2023-03-31 11:24:14  [ main:348 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.keytab.file
2023-03-31 11:24:14  [ main:348 ] - [ DEBUG ]  Handling deprecation for hive.io.exception.handlers
2023-03-31 11:24:14  [ main:348 ] - [ DEBUG ]  Handling deprecation for hive.jobname.length
2023-03-31 11:24:14  [ main:348 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.enforce.tree
2023-03-31 11:24:14  [ main:348 ] - [ DEBUG ]  Handling deprecation for hive.added.jars.path
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.direct.sql.max.query.length
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.bind.host
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.server2.tez.initialize.default.sessions
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.metastore.client.socket.timeout
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.compactor.history.retention.failed
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.DetachAllOnCommit
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.txn.max.open.batch
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.compactor.check.interval
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.server2.close.session.on.disconnect
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.yarn.container.mb
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.optimize.ppd.windowing
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.query.id
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.transactional.table.scan
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.compactor.delta.pct.threshold
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.reduce.enabled
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.ConnectionDriverName
2023-03-31 11:24:14  [ main:349 ] - [ DEBUG ]  Handling deprecation for hive.current.database
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.metastore.orm.retrieveMapNullsAsEmptyStrings
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.stats.max.variable.length
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.llap.orc.gap.cache
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.start.cleanup.scratchdir
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.merge.tezfiles
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.exec.rcfile.use.explicit.header
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.metastore.initial.metadata.count.enabled
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.split.strategy
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.server2.async.exec.keepalive.time
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.optimize.index.filter
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.sqlstd.confwhitelist
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.optimize.dynamic.partition.hashjoin
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.optimize.listbucketing
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.default.serde
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.connection.basesleeptime
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.host
2023-03-31 11:24:14  [ main:350 ] - [ DEBUG ]  Handling deprecation for hive.metastore.ds.connection.url.hook
2023-03-31 11:24:14  [ main:351 ] - [ DEBUG ]  Handling deprecation for hive.query.result.fileformat
2023-03-31 11:24:14  [ main:351 ] - [ DEBUG ]  Handling deprecation for hive.metastore.partition.name.whitelist.pattern
2023-03-31 11:24:14  [ main:351 ] - [ DEBUG ]  Handling deprecation for hive.stats.map.num.entries
2023-03-31 11:24:14  [ main:351 ] - [ DEBUG ]  Handling deprecation for hive.tez.dynamic.partition.pruning.max.event.size
2023-03-31 11:24:14  [ main:351 ] - [ DEBUG ]  Handling deprecation for hive.cbo.enable
2023-03-31 11:24:14  [ main:351 ] - [ DEBUG ]  Handling deprecation for hive.optimize.constant.propagation
2023-03-31 11:24:14  [ main:351 ] - [ DEBUG ]  Handling deprecation for hive.exec.mode.local.auto
2023-03-31 11:24:14  [ main:351 ] - [ DEBUG ]  Handling deprecation for hive.optimize.reducededuplication.min.reducer
2023-03-31 11:24:14  [ main:351 ] - [ DEBUG ]  Handling deprecation for hive.transform.escape.input
2023-03-31 11:24:14  [ main:351 ] - [ DEBUG ]  Handling deprecation for hive.orc.splits.ms.footer.cache.enabled
2023-03-31 11:24:14  [ main:351 ] - [ DEBUG ]  Handling deprecation for hive.optimize.point.lookup.min
2023-03-31 11:24:14  [ main:352 ] - [ DEBUG ]  Handling deprecation for hive.server2.max.start.attempts
2023-03-31 11:24:14  [ main:353 ] - [ DEBUG ]  Handling deprecation for hive.exec.dynamic.partition.mode
2023-03-31 11:24:14  [ main:355 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.max.worker.threads
2023-03-31 11:24:14  [ main:355 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.network
2023-03-31 11:24:14  [ main:356 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.fpp
2023-03-31 11:24:14  [ main:356 ] - [ DEBUG ]  Handling deprecation for hive.exec.driver.run.hooks
2023-03-31 11:24:14  [ main:356 ] - [ DEBUG ]  Handling deprecation for hive.conf.validation
2023-03-31 11:24:14  [ main:357 ] - [ DEBUG ]  Handling deprecation for hive.exec.pre.hooks
2023-03-31 11:24:14  [ main:357 ] - [ DEBUG ]  Handling deprecation for hive.unlock.numretries
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.script.operator.id.env.var
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.session.history.enabled
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.added.files.path
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.Multithreaded
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.task.scheduler.enable.preemption
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.num.executors
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.file.metadata.threads
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.rework.mapredwork
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.metastore.client.connect.retry.delay
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.optimize.groupby
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.cache.max.full
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.connection.class
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.service.principal
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.exec.check.crossproducts
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.server.read.socket.timeout
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.exec.reducers.max
2023-03-31 11:24:14  [ main:358 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.service.refresh.interval.sec
2023-03-31 11:24:14  [ main:359 ] - [ DEBUG ]  Handling deprecation for hive.fetch.task.conversion.threshold
2023-03-31 11:24:14  [ main:360 ] - [ DEBUG ]  Handling deprecation for hive.exec.perf.logger
2023-03-31 11:24:14  [ main:360 ] - [ DEBUG ]  Handling deprecation for hive.limit.row.max.size
2023-03-31 11:24:14  [ main:360 ] - [ DEBUG ]  Handling deprecation for hive.metastore.thrift.compact.protocol.enabled
2023-03-31 11:24:14  [ main:360 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.max.output.size
2023-03-31 11:24:14  [ main:360 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.rpc.server.address
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for datanucleus.plugin.pluginRegistryBundleCheck
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.join.noconditionaltask.size
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.script.operator.truncate.env
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.join.cache.size
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.metastore.dbaccess.ssl.properties
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.exec.parallel.thread.number
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.client.password
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.driver.parallel.compilation
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.skewjoin.key
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.max.reader.wait
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for datanucleus.rdbms.initializeColumnInfo
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.security.metastore.authenticator.manager
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.llap.remote.token.requires.signing
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.cache.max.writer.wait
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.default.fileformat.managed
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.reloadable.aux.jars.path
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.tez.bucket.pruning
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.request.header.size
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.index.compact.file.ignore.hdfs
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.llap.cache.allow.synthetic.fileid
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.max.threads
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.hash.table.inflation.factor
2023-03-31 11:24:14  [ main:361 ] - [ DEBUG ]  Handling deprecation for hive.optimize.limittranspose.reductiontuples
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.test.rollbacktxn
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.scheduler.num.schedulable.tasks.per.node
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.acl
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.hmshandler.retry.interval
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.memory.size
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggr.stats.hbase.ttl
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.exec.local.scratchdir
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.max.message.size
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.mapred.mode
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.strict.checks.type.safety
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.default.buffer.size
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.server2.async.exec.async.compile
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.stats.gather.num.threads
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.max.input.size
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.limit.pushdown.memory.usage
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.metastore.archive.intermediate.original
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.exec.mode.local.auto.inputbytes.max
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.enforce.vectorized
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.localtask.max.memory.usage
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.tez.enable.memory.manager
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.writeset.reaper.interval
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.support.sql11.reserved.keywords
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.metastore.batch.retrieve.table.partition.max
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.use.vector.serde.deserialize
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.tez.dynamic.partition.pruning.max.data.size
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.metadata.move.exported.metadata.to.trash
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.cli.pretty.output.num.cols
2023-03-31 11:24:14  [ main:362 ] - [ DEBUG ]  Handling deprecation for hive.orc.splits.allow.synthetic.fileid
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.session.timeout
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.fetch.output.serde
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.order.columnalignment
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.hbase.snapshot.restoredir
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.log.trace.id
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.skewjoin.mapjoin.min.split
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.resultset.use.unique.column.names
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.output.service.send.buffer.size
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.connection.max.retries
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.server2.session.check.interval
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.compute.splits.in.am
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.max.partitions
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.compactor.worker.timeout
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.stats.filter.in.factor
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for parquet.memory.pool.ratio
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.kerberos.principal
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.new.job.grouping.set.cardinality
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.exec.schema.evolution
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.client.stats.counters
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.enforce.sortmergebucketmapjoin
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.direct.sql.max.elements.values.clause
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.tez.smb.number.waves
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.max.writer.wait
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.spnego.keytab
2023-03-31 11:24:14  [ main:363 ] - [ DEBUG ]  Handling deprecation for hive.ppd.recognizetransivity
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.allow.uber
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.server2.llap.concurrent.queries
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.io.rcfile.tolerate.corruptions
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.keystore.path
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.secret.bits
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for stream.stderr.reporter.enabled
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.autogen.columnalias.prefix.label
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.auth
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.metastore.event.listeners
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.repl.task.factory
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.int.timestamp.conversion.in.seconds
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.tez.auto.reducer.parallelism
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.metastore.rawstore.impl
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.security.metastore.authorization.manager
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.orc.splits.include.fileid
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.jar.path
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.communicator.num.threads
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.communicator.connection.sleep.between.retries.ms
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.server2.tez.session.lifetime.jitter
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggregate.stats.max.partitions
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.mapjoin.native.multikey.only.enabled
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.orc.compute.splits.num.threads
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.limit.query.max.table.partition
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.exec.rowoffset
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.web.port
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.stats.default.publisher
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.script.recordwriter
2023-03-31 11:24:14  [ main:364 ] - [ DEBUG ]  Handling deprecation for hive.service.metrics.hadoop2.component
2023-03-31 11:24:14  [ main:365 ] - [ DEBUG ]  Handling deprecation for hive.ppd.remove.duplicatefilters
2023-03-31 11:24:14  [ main:365 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.yarn.shuffle.port
2023-03-31 11:24:14  [ main:365 ] - [ DEBUG ]  Handling deprecation for hive.server2.keystore.password
2023-03-31 11:24:14  [ main:365 ] - [ DEBUG ]  Handling deprecation for hive.strict.checks.cartesian.product
2023-03-31 11:24:14  [ main:365 ] - [ DEBUG ]  Handling deprecation for hive.server2.logging.operation.level
2023-03-31 11:24:14  [ main:365 ] - [ DEBUG ]  Handling deprecation for hive.variable.substitute
2023-03-31 11:24:14  [ main:366 ] - [ DEBUG ]  Handling deprecation for hive.txn.manager
2023-03-31 11:24:14  [ main:366 ] - [ DEBUG ]  Handling deprecation for datanucleus.cache.level2.type
2023-03-31 11:24:14  [ main:366 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.rpc.num.handlers
2023-03-31 11:24:14  [ main:366 ] - [ DEBUG ]  Handling deprecation for hive.metastore.stats.ndv.densityfunction
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.direct.sql.max.elements.in.clause
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.metastore.direct.sql.batch.size
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.vcpus.per.instance
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.intermediate.compression.codec
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.metastore.server.min.threads
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.tez.exec.print.summary
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.count.open.txns.interval
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.exec.compress.intermediate
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.enabled
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.metastore.expression.proxy
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.optimize.partition.columns.separate
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.script.recordreader
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.stats.autogather
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.optimize.sort.dynamic.partition
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.metastore.init.hooks
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.metastore.dml.events
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.metastore.thrift.framed.transport.enabled
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.log.every.n.records
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.scheduler.locality.delay
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.txn.heartbeat.threadpool.size
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.index.compact.query.max.size
2023-03-31 11:24:14  [ main:367 ] - [ DEBUG ]  Handling deprecation for hive.heartbeat.interval
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.reduce.groupby.enabled
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.lock.sleep.between.retries
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.test.mode.samplefreq
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.stats.dbclass
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.scheduler.node.disable.backoff.factor
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.response.header.size
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.conf.internal.variable.list
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.exec.concatenate.check.index
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.exec.rcfile.use.sync.cache
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for datanucleus.connectionPoolingType
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.map.aggr.hash.force.flush.memory.threshold
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.metastore.cache.pinobjtypes
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.optimize.limittranspose.reductionpercentage
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.fileformat.check
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.server2.async.exec.wait.queue.size
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.stats.default.aggregator
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.explain.user
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.server2.keystore.path
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.client.retry.limit
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.encoding.strategy
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.metastore.schema.verification
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.llap.am.liveness.connection.sleep.between.retries.ms
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.resultset.serialize.in.tasks
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.metastore.connect.retries
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.cluster.delegation.token.store.zookeeper.connectString
2023-03-31 11:24:14  [ main:368 ] - [ DEBUG ]  Handling deprecation for hive.exec.infer.bucket.sort
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.ttl
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.index.compact.file
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.exec.submit.local.task.via.child
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.hwi.listen.port
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.metastore.token.signature
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.memory.per.instance.mb
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.metastore.archive.intermediate.extracted
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.cluster.delegation.token.store.zookeeper.znode
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.service.metrics.hadoop2.frequency
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.orc.splits.directory.batch.ms
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.cache.max.reader.wait
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.hdfs.write
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.kerberos.keytab
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.tez.cpu.vcores
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.msck.path.validation
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.followby.map.aggr.hash.percentmemory
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.tez.task.scale.memory.reserve.fraction
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.parquet.timestamp.skip.conversion
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.lock.manager
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.tez.exec.inplace.progress
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.variable.substitute.depth
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.mapper.cannot.span.multiple.partitions
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.merge.size.per.task
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.table.parameters.default
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.optimize.sampling.orderby.percent
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.ignore.mapjoin.hint
2023-03-31 11:24:14  [ main:369 ] - [ DEBUG ]  Handling deprecation for hive.compactor.history.reaper.interval
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.lock.mapred.only.operation
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.tez.min.partition.factor
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.default.block.padding
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.rpc.sasl.mechanisms
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.optimize.sampling.orderby
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.metastore.kerberos.keytab.file
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.groupby.mapaggr.checkinterval
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.exec.script.trust
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.followby.gby.localtask.max.memory.usage
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.ConnectionUserName
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.spark.job.monitor.timeout
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.exec.show.job.failure.debug.info
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.scheduler.node.reenable.max.timeout.ms
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.max.open.txns
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.groupby.orderby.position.alias
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.extended
2023-03-31 11:24:14  [ main:370 ] - [ DEBUG ]  Handling deprecation for hive.script.operator.env.blacklist
2023-03-31 11:24:14  [ main:371 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.local.fs.write
2023-03-31 11:24:14  [ main:372 ] - [ DEBUG ]  Handling deprecation for hive.in.tez.test
2023-03-31 11:24:14  [ main:372 ] - [ DEBUG ]  Handling deprecation for hive.tez.input.generate.consistent.splits
2023-03-31 11:24:14  [ main:373 ] - [ DEBUG ]  Handling deprecation for hive.optimize.bucketmapjoin.sortedmerge
2023-03-31 11:24:14  [ main:373 ] - [ DEBUG ]  Handling deprecation for hive.stats.ndv.error
2023-03-31 11:24:14  [ main:373 ] - [ DEBUG ]  Handling deprecation for hive.server2.enable.doAs
2023-03-31 11:24:14  [ main:373 ] - [ DEBUG ]  Handling deprecation for hive.server2.zookeeper.namespace
2023-03-31 11:24:14  [ main:373 ] - [ DEBUG ]  Handling deprecation for hive.stats.atomic
2023-03-31 11:24:14  [ main:373 ] - [ DEBUG ]  Handling deprecation for hive.server2.zookeeper.publish.configs
2023-03-31 11:24:14  [ main:373 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.work.dirs
2023-03-31 11:24:14  [ main:373 ] - [ DEBUG ]  Handling deprecation for datanucleus.schema.autoCreateAll
2023-03-31 11:24:14  [ main:373 ] - [ DEBUG ]  Handling deprecation for hive.optimize.index.groupby
2023-03-31 11:24:14  [ main:373 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.sortmerge.join
2023-03-31 11:24:14  [ main:373 ] - [ DEBUG ]  Handling deprecation for hive.server2.xsrf.filter.enabled
2023-03-31 11:24:14  [ main:373 ] - [ DEBUG ]  Handling deprecation for hive.server2.idle.session.timeout
2023-03-31 11:24:14  [ main:373 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.allocator.alloc.max
2023-03-31 11:24:14  [ main:548 ] - [ DEBUG ]  field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
2023-03-31 11:24:14  [ main:561 ] - [ DEBUG ]  field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
2023-03-31 11:24:14  [ main:562 ] - [ DEBUG ]  field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[GetGroups])
2023-03-31 11:24:14  [ main:562 ] - [ DEBUG ]  field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
2023-03-31 11:24:14  [ main:562 ] - [ DEBUG ]  field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
2023-03-31 11:24:14  [ main:564 ] - [ DEBUG ]  UgiMetrics, User and group related metrics
2023-03-31 11:24:14  [ main:587 ] - [ DEBUG ]  Setting hadoop.security.token.service.use_ip to true
2023-03-31 11:24:14  [ main:614 ] - [ DEBUG ]   Creating new Groups object
2023-03-31 11:24:14  [ main:615 ] - [ DEBUG ]  Trying to load the custom-built native-hadoop library...
2023-03-31 11:24:14  [ main:618 ] - [ DEBUG ]  Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
2023-03-31 11:24:14  [ main:618 ] - [ DEBUG ]  java.library.path=/Users/renzhuo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
2023-03-31 11:24:14  [ main:618 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-03-31 11:24:14  [ main:619 ] - [ DEBUG ]  Falling back to shell based
2023-03-31 11:24:14  [ main:619 ] - [ DEBUG ]  Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
2023-03-31 11:24:14  [ main:637 ] - [ DEBUG ]  Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
2023-03-31 11:24:14  [ main:645 ] - [ DEBUG ]  hadoop login
2023-03-31 11:24:14  [ main:648 ] - [ DEBUG ]  hadoop login commit
2023-03-31 11:24:14  [ main:653 ] - [ DEBUG ]  using local user:UnixPrincipal: renzhuo
2023-03-31 11:24:14  [ main:653 ] - [ DEBUG ]  Using user: "UnixPrincipal: renzhuo" with name renzhuo
2023-03-31 11:24:14  [ main:653 ] - [ DEBUG ]  User entry: "renzhuo"
2023-03-31 11:24:14  [ main:654 ] - [ DEBUG ]  UGI loginUser:renzhuo (auth:SIMPLE)
2023-03-31 11:24:14  [ main:703 ] - [ INFO ]  Setting hive conf dir as src/main/resources
2023-03-31 11:24:15  [ main:766 ] - [ DEBUG ]  sampler.classes = ; loaded no samplers
2023-03-31 11:24:15  [ main:775 ] - [ DEBUG ]  span.receiver.classes = ; loaded no span receivers
2023-03-31 11:24:15  [ main:776 ] - [ DEBUG ]  Loading filesystems
2023-03-31 11:24:15  [ main:788 ] - [ DEBUG ]  file:// = class org.apache.hadoop.fs.LocalFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-common/3.0.0/hadoop-common-3.0.0.jar
2023-03-31 11:24:15  [ main:792 ] - [ DEBUG ]  viewfs:// = class org.apache.hadoop.fs.viewfs.ViewFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-common/3.0.0/hadoop-common-3.0.0.jar
2023-03-31 11:24:15  [ main:794 ] - [ DEBUG ]  ftp:// = class org.apache.hadoop.fs.ftp.FTPFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-common/3.0.0/hadoop-common-3.0.0.jar
2023-03-31 11:24:15  [ main:798 ] - [ DEBUG ]  har:// = class org.apache.hadoop.fs.HarFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-common/3.0.0/hadoop-common-3.0.0.jar
2023-03-31 11:24:15  [ main:799 ] - [ DEBUG ]  http:// = class org.apache.hadoop.fs.http.HttpFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-common/3.0.0/hadoop-common-3.0.0.jar
2023-03-31 11:24:15  [ main:799 ] - [ DEBUG ]  https:// = class org.apache.hadoop.fs.http.HttpsFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-common/3.0.0/hadoop-common-3.0.0.jar
2023-03-31 11:24:15  [ main:808 ] - [ DEBUG ]  hdfs:// = class org.apache.hadoop.hdfs.DistributedFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.0.0/hadoop-hdfs-client-3.0.0.jar
2023-03-31 11:24:15  [ main:1049 ] - [ DEBUG ]  webhdfs:// = class org.apache.hadoop.hdfs.web.WebHdfsFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.0.0/hadoop-hdfs-client-3.0.0.jar
2023-03-31 11:24:15  [ main:1054 ] - [ DEBUG ]  swebhdfs:// = class org.apache.hadoop.hdfs.web.SWebHdfsFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.0.0/hadoop-hdfs-client-3.0.0.jar
2023-03-31 11:24:15  [ main:1058 ] - [ DEBUG ]  nullscan:// = class org.apache.hadoop.hive.ql.io.NullScanFileSystem from /Users/renzhuo/.m2/repository/org/apachemvn/hive/hive-exec/2.1.1/hive-exec-2.1.1.jar
2023-03-31 11:24:15  [ main:1059 ] - [ DEBUG ]  file:// = class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem from /Users/renzhuo/.m2/repository/org/apachemvn/hive/hive-exec/2.1.1/hive-exec-2.1.1.jar
2023-03-31 11:24:15  [ main:1059 ] - [ DEBUG ]  Looking for FS supporting file
2023-03-31 11:24:15  [ main:1059 ] - [ DEBUG ]  looking for configuration option fs.file.impl
2023-03-31 11:24:15  [ main:1078 ] - [ DEBUG ]  Looking in service filesystems for implementation class
2023-03-31 11:24:15  [ main:1079 ] - [ DEBUG ]  FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2023-03-31 11:24:15  [ main:1136 ] - [ INFO ]  Created HiveCatalog 'batch_test'
2023-03-31 11:24:15  [ main:1151 ] - [ INFO ]  Trying to connect to metastore with URI thrift://t-d-datastorage-srv01:9083
2023-03-31 11:24:15  [ main:1475 ] - [ INFO ]  Opened a connection to metastore, current connections: 1
2023-03-31 11:24:15  [ main:1527 ] - [ INFO ]  Connected to metastore.
2023-03-31 11:24:15  [ main:1582 ] - [ INFO ]  Connected to Hive metastore
2023-03-31 11:24:17  [ main:2902 ] - [ DEBUG ]  Plan after converting SqlNode to RelNode
LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
  LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:17  [ main:3701 ] - [ DEBUG ]  iteration: 1
2023-03-31 11:24:17  [ main:3723 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:17  [ main:3725 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:17  [ main:3725 ] - [ DEBUG ]  For final plan, using rel#7:LogicalSink.NONE.any.[](input=HepRelVertex#6,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:17  [ main:3726 ] - [ DEBUG ]  For final plan, using rel#5:LogicalProject.NONE.any.[](input=HepRelVertex#4,inputs=0..2)
2023-03-31 11:24:17  [ main:3726 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3779 ] - [ DEBUG ]  optimize convert table references before rewriting sub-queries to semi-join cost 25 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3780 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3780 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3781 ] - [ DEBUG ]  For final plan, using rel#12:LogicalSink.NONE.any.[](input=HepRelVertex#11,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3781 ] - [ DEBUG ]  For final plan, using rel#10:LogicalProject.NONE.any.[](input=HepRelVertex#9,inputs=0..2)
2023-03-31 11:24:18  [ main:3781 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3781 ] - [ DEBUG ]  optimize rewrite sub-queries to semi-join cost 2 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3782 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3782 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3782 ] - [ DEBUG ]  For final plan, using rel#17:LogicalSink.NONE.any.[](input=HepRelVertex#16,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3782 ] - [ DEBUG ]  For final plan, using rel#15:LogicalProject.NONE.any.[](input=HepRelVertex#14,inputs=0..2)
2023-03-31 11:24:18  [ main:3783 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3783 ] - [ DEBUG ]  optimize sub-queries remove cost 2 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3783 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3784 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3784 ] - [ DEBUG ]  For final plan, using rel#22:LogicalSink.NONE.any.[](input=HepRelVertex#21,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3784 ] - [ DEBUG ]  For final plan, using rel#20:LogicalProject.NONE.any.[](input=HepRelVertex#19,inputs=0..2)
2023-03-31 11:24:18  [ main:3784 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3784 ] - [ DEBUG ]  optimize convert table references after sub-queries removed cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3785 ] - [ DEBUG ]  optimize subquery_rewrite cost 87 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3785 ] - [ DEBUG ]  iteration: 1
2023-03-31 11:24:18  [ main:3785 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3785 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3785 ] - [ DEBUG ]  For final plan, using rel#27:LogicalSink.NONE.any.[](input=HepRelVertex#26,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3785 ] - [ DEBUG ]  For final plan, using rel#25:LogicalProject.NONE.any.[](input=HepRelVertex#24,inputs=0..2)
2023-03-31 11:24:18  [ main:3785 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3786 ] - [ DEBUG ]  optimize convert correlate to temporal table join cost 0 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3786 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3786 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3786 ] - [ DEBUG ]  For final plan, using rel#32:LogicalSink.NONE.any.[](input=HepRelVertex#31,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3786 ] - [ DEBUG ]  For final plan, using rel#30:LogicalProject.NONE.any.[](input=HepRelVertex#29,inputs=0..2)
2023-03-31 11:24:18  [ main:3786 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3787 ] - [ DEBUG ]  optimize convert enumerable table scan cost 0 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3789 ] - [ DEBUG ]  optimize temporal_join_rewrite cost 4 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3801 ] - [ DEBUG ]  optimize decorrelate cost 10 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3803 ] - [ DEBUG ]  call#1: Apply rule [ReduceExpressionsRule(Project)] to [rel#35:LogicalProject.NONE.any.[](input=HepRelVertex#34,inputs=0..2)]
2023-03-31 11:24:18  [ main:3833 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3836 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ReduceExpressionsRule(Project)                                                 1              28,934
* Total                                                                        1              28,934

2023-03-31 11:24:18  [ main:3837 ] - [ DEBUG ]  For final plan, using rel#37:LogicalSink.NONE.any.[](input=HepRelVertex#36,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3838 ] - [ DEBUG ]  For final plan, using rel#35:LogicalProject.NONE.any.[](input=HepRelVertex#34,inputs=0..2)
2023-03-31 11:24:18  [ main:3838 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3838 ] - [ DEBUG ]  optimize default_rewrite cost 37 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3838 ] - [ DEBUG ]  iteration: 1
2023-03-31 11:24:18  [ main:3838 ] - [ DEBUG ]  iteration: 1
2023-03-31 11:24:18  [ main:3839 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3839 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3839 ] - [ DEBUG ]  For final plan, using rel#42:LogicalSink.NONE.any.[](input=HepRelVertex#41,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3839 ] - [ DEBUG ]  For final plan, using rel#40:LogicalProject.NONE.any.[](input=HepRelVertex#39,inputs=0..2)
2023-03-31 11:24:18  [ main:3839 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3840 ] - [ DEBUG ]  optimize join predicate rewrite cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3840 ] - [ DEBUG ]  call#2: Apply rule [ReduceExpressionsRule(Project)] to [rel#45:LogicalProject.NONE.any.[](input=HepRelVertex#44,inputs=0..2)]
2023-03-31 11:24:18  [ main:3840 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3841 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ReduceExpressionsRule(Project)                                                 1                 105
* Total                                                                        1                 105

2023-03-31 11:24:18  [ main:3841 ] - [ DEBUG ]  For final plan, using rel#47:LogicalSink.NONE.any.[](input=HepRelVertex#46,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3841 ] - [ DEBUG ]  For final plan, using rel#45:LogicalProject.NONE.any.[](input=HepRelVertex#44,inputs=0..2)
2023-03-31 11:24:18  [ main:3841 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3841 ] - [ DEBUG ]  optimize other predicate rewrite cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3841 ] - [ DEBUG ]  iteration: 2
2023-03-31 11:24:18  [ main:3841 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3842 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3842 ] - [ DEBUG ]  For final plan, using rel#52:LogicalSink.NONE.any.[](input=HepRelVertex#51,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3842 ] - [ DEBUG ]  For final plan, using rel#50:LogicalProject.NONE.any.[](input=HepRelVertex#49,inputs=0..2)
2023-03-31 11:24:18  [ main:3842 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3843 ] - [ DEBUG ]  optimize join predicate rewrite cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3843 ] - [ DEBUG ]  call#3: Apply rule [ReduceExpressionsRule(Project)] to [rel#55:LogicalProject.NONE.any.[](input=HepRelVertex#54,inputs=0..2)]
2023-03-31 11:24:18  [ main:3843 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3843 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ReduceExpressionsRule(Project)                                                 1                  36
* Total                                                                        1                  36

2023-03-31 11:24:18  [ main:3843 ] - [ DEBUG ]  For final plan, using rel#57:LogicalSink.NONE.any.[](input=HepRelVertex#56,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3843 ] - [ DEBUG ]  For final plan, using rel#55:LogicalProject.NONE.any.[](input=HepRelVertex#54,inputs=0..2)
2023-03-31 11:24:18  [ main:3843 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3844 ] - [ DEBUG ]  optimize other predicate rewrite cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3844 ] - [ DEBUG ]  iteration: 3
2023-03-31 11:24:18  [ main:3844 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3844 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3844 ] - [ DEBUG ]  For final plan, using rel#62:LogicalSink.NONE.any.[](input=HepRelVertex#61,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3844 ] - [ DEBUG ]  For final plan, using rel#60:LogicalProject.NONE.any.[](input=HepRelVertex#59,inputs=0..2)
2023-03-31 11:24:18  [ main:3845 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3845 ] - [ DEBUG ]  optimize join predicate rewrite cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3847 ] - [ DEBUG ]  call#4: Apply rule [ReduceExpressionsRule(Project)] to [rel#65:LogicalProject.NONE.any.[](input=HepRelVertex#64,inputs=0..2)]
2023-03-31 11:24:18  [ main:3848 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3851 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ReduceExpressionsRule(Project)                                                 1                  40
* Total                                                                        1                  40

2023-03-31 11:24:18  [ main:3852 ] - [ DEBUG ]  For final plan, using rel#67:LogicalSink.NONE.any.[](input=HepRelVertex#66,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3852 ] - [ DEBUG ]  For final plan, using rel#65:LogicalProject.NONE.any.[](input=HepRelVertex#64,inputs=0..2)
2023-03-31 11:24:18  [ main:3852 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3853 ] - [ DEBUG ]  optimize other predicate rewrite cost 5 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3853 ] - [ DEBUG ]  iteration: 4
2023-03-31 11:24:18  [ main:3854 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3854 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3855 ] - [ DEBUG ]  For final plan, using rel#72:LogicalSink.NONE.any.[](input=HepRelVertex#71,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3855 ] - [ DEBUG ]  For final plan, using rel#70:LogicalProject.NONE.any.[](input=HepRelVertex#69,inputs=0..2)
2023-03-31 11:24:18  [ main:3855 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3855 ] - [ DEBUG ]  optimize join predicate rewrite cost 2 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3857 ] - [ DEBUG ]  call#5: Apply rule [ReduceExpressionsRule(Project)] to [rel#75:LogicalProject.NONE.any.[](input=HepRelVertex#74,inputs=0..2)]
2023-03-31 11:24:18  [ main:3857 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3859 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ReduceExpressionsRule(Project)                                                 1                 101
* Total                                                                        1                 101

2023-03-31 11:24:18  [ main:3859 ] - [ DEBUG ]  For final plan, using rel#77:LogicalSink.NONE.any.[](input=HepRelVertex#76,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3859 ] - [ DEBUG ]  For final plan, using rel#75:LogicalProject.NONE.any.[](input=HepRelVertex#74,inputs=0..2)
2023-03-31 11:24:18  [ main:3859 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3860 ] - [ DEBUG ]  optimize other predicate rewrite cost 4 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3860 ] - [ DEBUG ]  iteration: 5
2023-03-31 11:24:18  [ main:3860 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3862 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3862 ] - [ DEBUG ]  For final plan, using rel#82:LogicalSink.NONE.any.[](input=HepRelVertex#81,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3862 ] - [ DEBUG ]  For final plan, using rel#80:LogicalProject.NONE.any.[](input=HepRelVertex#79,inputs=0..2)
2023-03-31 11:24:18  [ main:3863 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3864 ] - [ DEBUG ]  optimize join predicate rewrite cost 3 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3865 ] - [ DEBUG ]  call#6: Apply rule [ReduceExpressionsRule(Project)] to [rel#85:LogicalProject.NONE.any.[](input=HepRelVertex#84,inputs=0..2)]
2023-03-31 11:24:18  [ main:3865 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3866 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ReduceExpressionsRule(Project)                                                 1                  41
* Total                                                                        1                  41

2023-03-31 11:24:18  [ main:3866 ] - [ DEBUG ]  For final plan, using rel#87:LogicalSink.NONE.any.[](input=HepRelVertex#86,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3866 ] - [ DEBUG ]  For final plan, using rel#85:LogicalProject.NONE.any.[](input=HepRelVertex#84,inputs=0..2)
2023-03-31 11:24:18  [ main:3866 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3867 ] - [ DEBUG ]  optimize other predicate rewrite cost 2 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3868 ] - [ DEBUG ]  optimize predicate rewrite cost 29 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3868 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3869 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3869 ] - [ DEBUG ]  For final plan, using rel#92:LogicalSink.NONE.any.[](input=HepRelVertex#91,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3869 ] - [ DEBUG ]  For final plan, using rel#90:LogicalProject.NONE.any.[](input=HepRelVertex#89,inputs=0..2)
2023-03-31 11:24:18  [ main:3869 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3869 ] - [ DEBUG ]  optimize push predicate into table scan cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3872 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3873 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3873 ] - [ DEBUG ]  For final plan, using rel#97:LogicalSink.NONE.any.[](input=HepRelVertex#96,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3874 ] - [ DEBUG ]  For final plan, using rel#95:LogicalProject.NONE.any.[](input=HepRelVertex#94,inputs=0..2)
2023-03-31 11:24:18  [ main:3874 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3874 ] - [ DEBUG ]  optimize prune empty after predicate push down cost 5 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3875 ] - [ DEBUG ]  optimize predicate_pushdown cost 37 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3875 ] - [ DEBUG ]  iteration: 1
2023-03-31 11:24:18  [ main:3876 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3876 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3877 ] - [ DEBUG ]  For final plan, using rel#102:LogicalSink.NONE.any.[](input=HepRelVertex#101,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3877 ] - [ DEBUG ]  For final plan, using rel#100:LogicalProject.NONE.any.[](input=HepRelVertex#99,inputs=0..2)
2023-03-31 11:24:18  [ main:3877 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3877 ] - [ DEBUG ]  optimize simplify and push down join predicates cost 2 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3877 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3878 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:3878 ] - [ DEBUG ]  For final plan, using rel#107:LogicalSink.NONE.any.[](input=HepRelVertex#106,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3879 ] - [ DEBUG ]  For final plan, using rel#105:LogicalProject.NONE.any.[](input=HepRelVertex#104,inputs=0..2)
2023-03-31 11:24:18  [ main:3879 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3879 ] - [ DEBUG ]  optimize deal with possible null join keys cost 2 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3879 ] - [ DEBUG ]  optimize join_rewrite cost 4 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:3881 ] - [ DEBUG ]  call#7: Apply rule [ProjectRemoveRule] to [rel#110:LogicalProject.NONE.any.[](input=HepRelVertex#109,inputs=0..2)]
2023-03-31 11:24:18  [ main:3883 ] - [ DEBUG ]  call#7: Rule ProjectRemoveRule arguments [rel#110:LogicalProject.NONE.any.[](input=HepRelVertex#109,inputs=0..2)] produced rel#109:HepRelVertex(rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01]))
2023-03-31 11:24:18  [ main:3885 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:3886 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ProjectRemoveRule                                                              1               1,817
* Total                                                                        1               1,817

2023-03-31 11:24:18  [ main:3886 ] - [ DEBUG ]  For final plan, using rel#112:LogicalSink.NONE.any.[](input=HepRelVertex#109,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:3886 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:24:18  [ main:3887 ] - [ DEBUG ]  optimize project_rewrite cost 6 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:24:18  [ main:4060 ] - [ DEBUG ]  PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@2ad99cf3; COST = {inf}
2023-03-31 11:24:18  [ main:4063 ] - [ DEBUG ]  Pop match: rule [FlinkLogicalTableSourceScanConverter(in:NONE,out:LOGICAL)] rels [#1]
2023-03-31 11:24:18  [ main:4064 ] - [ DEBUG ]  call#18: Apply rule [FlinkLogicalTableSourceScanConverter(in:NONE,out:LOGICAL)] to [rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])]
2023-03-31 11:24:18  [ main:4065 ] - [ DEBUG ]  Transform to: rel#119 via FlinkLogicalTableSourceScanConverter(in:NONE,out:LOGICAL)
2023-03-31 11:24:18  [ main:4177 ] - [ DEBUG ]  call#18 generated 1 successors: [rel#119:FlinkLogicalTableSourceScan.LOGICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)]
2023-03-31 11:24:18  [ main:4178 ] - [ DEBUG ]  PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@2ad99cf3; COST = {inf}
2023-03-31 11:24:18  [ main:4178 ] - [ DEBUG ]  Pop match: rule [FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)] rels [#115]
2023-03-31 11:24:18  [ main:4178 ] - [ DEBUG ]  call#28: Apply rule [FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)] to [rel#115:LogicalSink.NONE.any.[](input=RelSubset#114,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)]
2023-03-31 11:24:18  [ main:4179 ] - [ DEBUG ]  Transform to: rel#121 via FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)
2023-03-31 11:24:18  [ main:4256 ] - [ DEBUG ]  call#28 generated 1 successors: [rel#121:FlinkLogicalSink.LOGICAL.any.[](input=RelSubset#120,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)]
2023-03-31 11:24:18  [ main:4257 ] - [ DEBUG ]  PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@2ad99cf3; COST = {3.1489166E7 rows, 3.1489166E7 cpu, 5.66804988E8 io, 0.0 network, 0.0 memory}
2023-03-31 11:24:18  [ main:4257 ] - [ DEBUG ]  Rule Attempts Info for VolcanoPlanner
2023-03-31 11:24:18  [ main:4259 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
FlinkLogicalTableSourceScanConverter(in:NONE,out:LOGICAL)                      1             113,112
FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)                                 1              78,433
* Total                                                                        2             191,545

2023-03-31 11:24:18  [ main:4297 ] - [ DEBUG ]  Cheapest plan:
FlinkLogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]): rowcount = 1.5744583E7, cumulative cost = {3.1489166E7 rows, 3.1489166E7 cpu, 5.66804988E8 io, 0.0 network, 0.0 memory}, id = 122
  FlinkLogicalTableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number]): rowcount = 1.5744583E7, cumulative cost = {1.5744583E7 rows, 1.5744583E7 cpu, 5.66804988E8 io, 0.0 network, 0.0 memory}, id = 119

2023-03-31 11:24:18  [ main:4300 ] - [ DEBUG ]  Provenance:
rel#122:FlinkLogicalSink.LOGICAL.any.[](input=FlinkLogicalTableSourceScan#119,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
  direct
    rel#121:FlinkLogicalSink.LOGICAL.any.[](input=RelSubset#120,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
      call#28 rule [FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)]
        rel#115:LogicalSink.NONE.any.[](input=RelSubset#114,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
          no parent
rel#119:FlinkLogicalTableSourceScan.LOGICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)
  call#18 rule [FlinkLogicalTableSourceScanConverter(in:NONE,out:LOGICAL)]
    rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
      no parent

2023-03-31 11:24:18  [ main:4303 ] - [ DEBUG ]  optimize logical cost 413 ms.
optimize result: 
FlinkLogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- FlinkLogicalTableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number])

2023-03-31 11:24:18  [ main:4304 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:4305 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:4305 ] - [ DEBUG ]  For final plan, using rel#124:FlinkLogicalSink.LOGICAL.any.[](input=HepRelVertex#123,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:4305 ] - [ DEBUG ]  For final plan, using rel#119:FlinkLogicalTableSourceScan.LOGICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:4306 ] - [ DEBUG ]  optimize logical_rewrite cost 2 ms.
optimize result: 
FlinkLogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- FlinkLogicalTableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number])

2023-03-31 11:24:18  [ main:4313 ] - [ DEBUG ]  optimize time_indicator cost 7 ms.
optimize result: 
FlinkLogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- FlinkLogicalTableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number])

2023-03-31 11:24:18  [ main:4323 ] - [ DEBUG ]  PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@2ad99cf3; COST = {inf}
2023-03-31 11:24:18  [ main:4323 ] - [ DEBUG ]  Pop match: rule [BatchPhysicalTableSourceScanRule(in:LOGICAL,out:BATCH_PHYSICAL)] rels [#119]
2023-03-31 11:24:18  [ main:4323 ] - [ DEBUG ]  call#47: Apply rule [BatchPhysicalTableSourceScanRule(in:LOGICAL,out:BATCH_PHYSICAL)] to [rel#119:FlinkLogicalTableSourceScan.LOGICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)]
2023-03-31 11:24:18  [ main:4325 ] - [ DEBUG ]  Transform to: rel#132 via BatchPhysicalTableSourceScanRule(in:LOGICAL,out:BATCH_PHYSICAL)
2023-03-31 11:24:18  [ main:4409 ] - [ DEBUG ]  call#47 generated 1 successors: [rel#132:BatchPhysicalTableSourceScan.BATCH_PHYSICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)]
2023-03-31 11:24:18  [ main:4409 ] - [ DEBUG ]  PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@2ad99cf3; COST = {inf}
2023-03-31 11:24:18  [ main:4410 ] - [ DEBUG ]  Pop match: rule [BatchPhysicalSinkRule(in:LOGICAL,out:BATCH_PHYSICAL)] rels [#128]
2023-03-31 11:24:18  [ main:4410 ] - [ DEBUG ]  call#72: Apply rule [BatchPhysicalSinkRule(in:LOGICAL,out:BATCH_PHYSICAL)] to [rel#128:FlinkLogicalSink.LOGICAL.any.[](input=RelSubset#127,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)]
2023-03-31 11:24:18  [ main:4412 ] - [ DEBUG ]  Transform to: rel#134 via BatchPhysicalSinkRule(in:LOGICAL,out:BATCH_PHYSICAL)
2023-03-31 11:24:18  [ main:4452 ] - [ DEBUG ]  call#72 generated 1 successors: [rel#134:BatchPhysicalSink.BATCH_PHYSICAL.any.[](input=RelSubset#133,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)]
2023-03-31 11:24:18  [ main:4452 ] - [ DEBUG ]  PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@2ad99cf3; COST = {3.1489166E7 rows, 1.5744583E7 cpu, 5.66804988E8 io, 0.0 network, 0.0 memory}
2023-03-31 11:24:18  [ main:4453 ] - [ DEBUG ]  Rule Attempts Info for VolcanoPlanner
2023-03-31 11:24:18  [ main:4453 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
FlinkLogicalTableSourceScanConverter(in:NONE,out:LOGICAL)                      1             113,112
BatchPhysicalTableSourceScanRule(in:LOGICAL,out:BATCH_PHYSICAL)                   1              85,849
FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)                                 1              78,433
BatchPhysicalSinkRule(in:LOGICAL,out:BATCH_PHYSICAL)                           1              42,591
* Total                                                                        4             319,985

2023-03-31 11:24:18  [ main:4485 ] - [ DEBUG ]  Cheapest plan:
BatchPhysicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]): rowcount = 1.5744583E7, cumulative cost = {3.1489166E7 rows, 1.5744583E7 cpu, 5.66804988E8 io, 0.0 network, 0.0 memory}, id = 135
  BatchPhysicalTableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number]): rowcount = 1.5744583E7, cumulative cost = {1.5744583E7 rows, 0.0 cpu, 5.66804988E8 io, 0.0 network, 0.0 memory}, id = 132

2023-03-31 11:24:18  [ main:4485 ] - [ DEBUG ]  Provenance:
rel#135:BatchPhysicalSink.BATCH_PHYSICAL.any.[](input=BatchPhysicalTableSourceScan#132,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
  direct
    rel#134:BatchPhysicalSink.BATCH_PHYSICAL.any.[](input=RelSubset#133,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
      call#72 rule [BatchPhysicalSinkRule(in:LOGICAL,out:BATCH_PHYSICAL)]
        rel#128:FlinkLogicalSink.LOGICAL.any.[](input=RelSubset#127,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
          no parent
rel#132:BatchPhysicalTableSourceScan.BATCH_PHYSICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)
  call#47 rule [BatchPhysicalTableSourceScanRule(in:LOGICAL,out:BATCH_PHYSICAL)]
    rel#119:FlinkLogicalTableSourceScan.LOGICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)
      no parent

2023-03-31 11:24:18  [ main:4486 ] - [ DEBUG ]  optimize physical cost 172 ms.
optimize result: 
Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- TableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number])

2023-03-31 11:24:18  [ main:4486 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:24:18  [ main:4486 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:24:18  [ main:4486 ] - [ DEBUG ]  For final plan, using rel#137:BatchPhysicalSink.BATCH_PHYSICAL.any.[](input=HepRelVertex#136,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:4486 ] - [ DEBUG ]  For final plan, using rel#132:BatchPhysicalTableSourceScan.BATCH_PHYSICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)
2023-03-31 11:24:18  [ main:4487 ] - [ DEBUG ]  optimize physical_rewrite cost 0 ms.
optimize result: 
Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- TableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number])

2023-03-31 11:24:18  [ main:4601 ] - [ INFO ]  Trying to connect to metastore with URI thrift://t-d-datastorage-srv04:9083
2023-03-31 11:24:18  [ main:4634 ] - [ INFO ]  Opened a connection to metastore, current connections: 2
2023-03-31 11:24:18  [ main:4667 ] - [ INFO ]  Connected to metastore.
2023-03-31 11:24:19  [ main:4942 ] - [ DEBUG ]  DDL: struct test_01 { string stamp, string event, string credit_number}
2023-03-31 11:24:19  [ main:4975 ] - [ INFO ]  Closed a connection to metastore, current connections: 1
2023-03-31 11:24:19  [ main:4983 ] - [ DEBUG ]  Looking for FS supporting hdfs
2023-03-31 11:24:19  [ main:4983 ] - [ DEBUG ]  looking for configuration option fs.hdfs.impl
2023-03-31 11:24:19  [ main:4983 ] - [ DEBUG ]  Looking in service filesystems for implementation class
2023-03-31 11:24:19  [ main:4983 ] - [ DEBUG ]  FS for hdfs is class org.apache.hadoop.hdfs.DistributedFileSystem
2023-03-31 11:24:19  [ main:5000 ] - [ DEBUG ]  dfs.client.use.legacy.blockreader.local = false
2023-03-31 11:24:19  [ main:5000 ] - [ DEBUG ]  dfs.client.read.shortcircuit = false
2023-03-31 11:24:19  [ main:5000 ] - [ DEBUG ]  dfs.client.domain.socket.data.traffic = false
2023-03-31 11:24:19  [ main:5000 ] - [ DEBUG ]  dfs.domain.socket.path = 
2023-03-31 11:24:19  [ main:5009 ] - [ DEBUG ]  Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2023-03-31 11:24:19  [ main:5031 ] - [ DEBUG ]  multipleLinearRandomRetry = null
2023-03-31 11:24:19  [ main:5056 ] - [ DEBUG ]  rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@30a9e3db
2023-03-31 11:24:19  [ main:5063 ] - [ DEBUG ]  getting client out of cache: org.apache.hadoop.ipc.Client@7fef0b40
2023-03-31 11:24:19  [ main:5452 ] - [ DEBUG ]  Both short-circuit local reads and UNIX domain socket are disabled.
2023-03-31 11:24:19  [ main:5464 ] - [ DEBUG ]  DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
2023-03-31 11:24:19  [ main:5495 ] - [ DEBUG ]  The ping interval is 60000 ms.
2023-03-31 11:24:19  [ main:5497 ] - [ DEBUG ]  Connecting to Tdsop/172.22.17.21:9820
2023-03-31 11:24:19  [ main:5519 ] - [ DEBUG ]  Failed to connect to server: Tdsop/172.22.17.21:9820: try once and fail.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:715)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1552)
	at org.apache.hadoop.ipc.Client.call(Client.java:1383)
	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy175.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:874)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy176.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1697)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1491)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1488)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1503)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1668)
	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.getNumFiles(HiveSourceFileEnumerator.java:118)
	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$0(HiveTableSource.java:146)
	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:89)
	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:144)
	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:114)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:106)
	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:49)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:250)
	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:58)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:82)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:81)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:185)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1665)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:752)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:872)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:742)
	at org.example.Main.main(Main.java:16)
2023-03-31 11:24:19  [ main:5521 ] - [ DEBUG ]  closing ipc connection to Tdsop/172.22.17.21:9820: Connection refused
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:715)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1552)
	at org.apache.hadoop.ipc.Client.call(Client.java:1383)
	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy175.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:874)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy176.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1697)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1491)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1488)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1503)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1668)
	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.getNumFiles(HiveSourceFileEnumerator.java:118)
	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$0(HiveTableSource.java:146)
	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:89)
	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:144)
	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:114)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:106)
	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:49)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:250)
	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:58)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:82)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:81)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:185)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1665)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:752)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:872)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:742)
	at org.example.Main.main(Main.java:16)
2023-03-31 11:24:19  [ main:5523 ] - [ DEBUG ]  IPC Client (2145961947) connection to Tdsop/172.22.17.21:9820 from renzhuo: closed
2023-03-31 11:24:19  [ main:5683 ] - [ DEBUG ]  Exception while invoking call #0 ClientNamenodeProtocolTranslatorPB.getFileInfo over null. Not retrying because try once and fail.
java.net.ConnectException: Call From Mac.local/127.0.0.1 to Tdsop:9820 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:824)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:754)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1495)
	at org.apache.hadoop.ipc.Client.call(Client.java:1437)
	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy175.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:874)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy176.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1697)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1491)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1488)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1503)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1668)
	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.getNumFiles(HiveSourceFileEnumerator.java:118)
	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$0(HiveTableSource.java:146)
	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:89)
	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:144)
	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:114)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:106)
	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:49)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:250)
	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:58)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:82)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:81)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:185)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1665)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:752)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:872)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:742)
	at org.example.Main.main(Main.java:16)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:715)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1552)
	at org.apache.hadoop.ipc.Client.call(Client.java:1383)
	... 51 more
2023-03-31 11:24:19  [ pool-1-thread-1:5691 ] - [ DEBUG ]  stopping client from cache: org.apache.hadoop.ipc.Client@7fef0b40
2023-03-31 11:24:19  [ pool-1-thread-1:5695 ] - [ DEBUG ]  removing client from cache: org.apache.hadoop.ipc.Client@7fef0b40
2023-03-31 11:24:19  [ pool-1-thread-1:5696 ] - [ DEBUG ]  stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@7fef0b40
2023-03-31 11:24:19  [ pool-1-thread-1:5696 ] - [ DEBUG ]  Stopping client
2023-03-31 11:24:19  [ Thread-2:5698 ] - [ DEBUG ]  ShutdownHookManger complete shutdown.
2023-03-31 11:25:55  [ main:0 ] - [ INFO ]  Found configuration file file:/Users/renzhuo/IdeaProjects/Flink_test/target/classes/hive-site.xml
2023-03-31 11:25:55  [ main:17 ] - [ DEBUG ]  Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:454)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:425)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:502)
	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.findHadoopBinary(HiveConf.java:3192)
	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.<clinit>(HiveConf.java:549)
	at org.apache.hadoop.hive.conf.HiveConf.<clinit>(HiveConf.java:143)
	at org.apache.flink.table.catalog.hive.HiveCatalog.createHiveConf(HiveCatalog.java:251)
	at org.apache.flink.table.catalog.hive.HiveCatalog.<init>(HiveCatalog.java:179)
	at org.apache.flink.table.catalog.hive.factories.HiveCatalogFactory.createCatalog(HiveCatalogFactory.java:76)
	at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:289)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1292)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1122)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:742)
	at org.example.Main.main(Main.java:16)
2023-03-31 11:25:55  [ main:31 ] - [ DEBUG ]  setsid is not available on this machine. So not using it.
2023-03-31 11:25:55  [ main:31 ] - [ DEBUG ]  setsid exited with exit code 0
2023-03-31 11:25:55  [ main:195 ] - [ DEBUG ]  Handling deprecation for all properties in config...
2023-03-31 11:25:55  [ main:196 ] - [ DEBUG ]  Handling deprecation for hive.exec.reducers.bytes.per.reducer
2023-03-31 11:25:55  [ main:196 ] - [ DEBUG ]  Handling deprecation for hive.server2.tez.sessions.init.threads
2023-03-31 11:25:55  [ main:196 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.createtable.group.grants
2023-03-31 11:25:55  [ main:196 ] - [ DEBUG ]  Handling deprecation for datanucleus.storeManagerType
2023-03-31 11:25:55  [ main:197 ] - [ DEBUG ]  Handling deprecation for hive.aux.jars.path
2023-03-31 11:25:55  [ main:197 ] - [ DEBUG ]  Handling deprecation for hive.llap.zk.registry.user
2023-03-31 11:25:55  [ main:197 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggregate.stats.false.positive.probability
2023-03-31 11:25:55  [ main:197 ] - [ DEBUG ]  Handling deprecation for hive.exec.stagingdir
2023-03-31 11:25:55  [ main:197 ] - [ DEBUG ]  Handling deprecation for hive.merge.rcfile.block.level
2023-03-31 11:25:55  [ main:198 ] - [ DEBUG ]  Handling deprecation for hive.execution.mode
2023-03-31 11:25:55  [ main:198 ] - [ DEBUG ]  Handling deprecation for hive.exec.default.partition.name
2023-03-31 11:25:55  [ main:201 ] - [ DEBUG ]  Handling deprecation for mapreduce.input.fileinputformat.split.minsize.per.rack
2023-03-31 11:25:55  [ main:201 ] - [ DEBUG ]  Handling deprecation for hive.metastore.event.expiry.duration
2023-03-31 11:25:55  [ main:201 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.default.compress
2023-03-31 11:25:55  [ main:202 ] - [ DEBUG ]  Handling deprecation for hive.exec.mode.local.auto.input.files.max
2023-03-31 11:25:55  [ main:202 ] - [ DEBUG ]  Handling deprecation for hive.cbo.cnf.maxnodes
2023-03-31 11:25:55  [ main:202 ] - [ DEBUG ]  Handling deprecation for hive.stats.key.prefix
2023-03-31 11:25:55  [ main:202 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.orc.time.counters
2023-03-31 11:25:55  [ main:203 ] - [ DEBUG ]  Handling deprecation for hive.orc.splits.ms.footer.cache.ppd.enabled
2023-03-31 11:25:55  [ main:203 ] - [ DEBUG ]  Handling deprecation for hive.tez.task.scale.memory.reserve-fraction.min
2023-03-31 11:25:55  [ main:204 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled
2023-03-31 11:25:55  [ main:204 ] - [ DEBUG ]  Handling deprecation for hive.optimize.skewjoin.compiletime
2023-03-31 11:25:55  [ main:204 ] - [ DEBUG ]  Handling deprecation for hive.smbjoin.cache.rows
2023-03-31 11:25:55  [ main:204 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.mapjoin.overflow.repeated.threshold
2023-03-31 11:25:55  [ main:204 ] - [ DEBUG ]  Handling deprecation for hive.server2.metrics.enabled
2023-03-31 11:25:55  [ main:204 ] - [ DEBUG ]  Handling deprecation for hive.tez.log.level
2023-03-31 11:25:55  [ main:204 ] - [ DEBUG ]  Handling deprecation for hive.merge.mapfiles
2023-03-31 11:25:55  [ main:204 ] - [ DEBUG ]  Handling deprecation for hive.exec.post.hooks
2023-03-31 11:25:55  [ main:204 ] - [ DEBUG ]  Handling deprecation for hive.metastore.client.socket.lifetime
2023-03-31 11:25:55  [ main:204 ] - [ DEBUG ]  Handling deprecation for fs.har.impl
2023-03-31 11:25:55  [ main:204 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.max.variance
2023-03-31 11:25:55  [ main:204 ] - [ DEBUG ]  Handling deprecation for mapreduce.input.fileinputformat.split.minsize
2023-03-31 11:25:55  [ main:204 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.quorum
2023-03-31 11:25:55  [ main:205 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.ldap.groupMembershipKey
2023-03-31 11:25:55  [ main:206 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.catalog.cache.size
2023-03-31 11:25:55  [ main:206 ] - [ DEBUG ]  Handling deprecation for stream.stderr.reporter.prefix
2023-03-31 11:25:55  [ main:206 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.hybridgrace.memcheckfrequency
2023-03-31 11:25:55  [ main:206 ] - [ DEBUG ]  Handling deprecation for hive.optimize.index.filter.compact.maxsize
2023-03-31 11:25:55  [ main:206 ] - [ DEBUG ]  Handling deprecation for hive.exec.counters.pull.interval
2023-03-31 11:25:55  [ main:206 ] - [ DEBUG ]  Handling deprecation for hive.security.command.whitelist
2023-03-31 11:25:55  [ main:207 ] - [ DEBUG ]  Handling deprecation for hive.metastore.end.function.listeners
2023-03-31 11:25:55  [ main:207 ] - [ DEBUG ]  Handling deprecation for hive.llap.zk.sm.connectionString
2023-03-31 11:25:55  [ main:207 ] - [ DEBUG ]  Handling deprecation for hive.downloaded.resources.dir
2023-03-31 11:25:55  [ main:207 ] - [ DEBUG ]  Handling deprecation for hive.join.emit.interval
2023-03-31 11:25:55  [ main:207 ] - [ DEBUG ]  Handling deprecation for hive.llap.am.liveness.connection.timeout.ms
2023-03-31 11:25:55  [ main:207 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.zerocopy
2023-03-31 11:25:55  [ main:207 ] - [ DEBUG ]  Handling deprecation for hive.metastore.fshandler.threads
2023-03-31 11:25:55  [ main:207 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.client.connect.retry.limit
2023-03-31 11:25:55  [ main:207 ] - [ DEBUG ]  Handling deprecation for hive.compute.query.using.stats
2023-03-31 11:25:55  [ main:207 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.block.padding.tolerance
2023-03-31 11:25:55  [ main:207 ] - [ DEBUG ]  Handling deprecation for hive.lazysimple.extended_boolean_literal
2023-03-31 11:25:55  [ main:207 ] - [ DEBUG ]  Handling deprecation for hive.orc.splits.include.file.footer
2023-03-31 11:25:55  [ main:207 ] - [ DEBUG ]  Handling deprecation for hive.error.on.empty.partition
2023-03-31 11:25:55  [ main:208 ] - [ DEBUG ]  Handling deprecation for hive.prewarm.enabled
2023-03-31 11:25:55  [ main:208 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.allocator.direct
2023-03-31 11:25:55  [ main:208 ] - [ DEBUG ]  Handling deprecation for hive.io.rcfile.record.buffer.size
2023-03-31 11:25:55  [ main:208 ] - [ DEBUG ]  Handling deprecation for hadoop.bin.path
2023-03-31 11:25:55  [ main:208 ] - [ DEBUG ]  Handling deprecation for hive.default.rcfile.serde
2023-03-31 11:25:55  [ main:208 ] - [ DEBUG ]  Handling deprecation for hive.llap.management.acl.blocked
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for datanucleus.schema.validateConstraints
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.users.in.admin.role
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.createtable.owner.grants
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.multi.insert.move.tasks.share.dependencies
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.autogen.columnalias.prefix.includefuncname
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.tez.max.partition.factor
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.port
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.orc.cache.stripe.details.size
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.task.scheduler.wait.queue.size
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggr.stats.cache.entries
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.exec.max.created.files
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.cli.prompt
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.stats.deserialization.factor
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.enforce.stats
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.metadata.export.location
2023-03-31 11:25:55  [ main:209 ] - [ DEBUG ]  Handling deprecation for hive.log.explain.output
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.optimize.skewjoin
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.default.fileformat
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.llap.client.consistent.splits
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.optimized.hashtable.wbsize
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.server2.tez.session.lifetime
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.security.metastore.authorization.auth.reads
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggr.stats.memory.ttl
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.NonTransactionalRead
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.optimize.remove.identity.project
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.timedout.txn.reaper.start
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.rpc.port
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.cache.ttl
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.exec.infer.bucket.sort.num.buckets.power.two
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.compactor.worker.threads
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.exim.strict.repl.tables
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggregate.stats.cache.size
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.llap.management.acl
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.stats.collect.tablekeys
2023-03-31 11:25:55  [ main:210 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.use.vectorized.input.format
2023-03-31 11:25:55  [ main:211 ] - [ DEBUG ]  Handling deprecation for hive.optimize.cte.materialize.threshold
2023-03-31 11:25:55  [ main:211 ] - [ DEBUG ]  Handling deprecation for hive.display.partition.cols.separately
2023-03-31 11:25:55  [ main:211 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.future.timeout
2023-03-31 11:25:55  [ main:211 ] - [ DEBUG ]  Handling deprecation for hive.metastore.disallow.incompatible.col.type.changes
2023-03-31 11:25:55  [ main:211 ] - [ DEBUG ]  Handling deprecation for hive.server2.async.exec.shutdown.timeout
2023-03-31 11:25:55  [ main:211 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.max.idle.time
2023-03-31 11:25:55  [ main:211 ] - [ DEBUG ]  Handling deprecation for hive.test.dummystats.aggregator
2023-03-31 11:25:55  [ main:211 ] - [ DEBUG ]  Handling deprecation for hive.test.mode
2023-03-31 11:25:55  [ main:211 ] - [ DEBUG ]  Handling deprecation for hive.querylog.enable.plan.progress
2023-03-31 11:25:55  [ main:211 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.cookie.auth.enabled
2023-03-31 11:25:55  [ main:213 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.worker.keepalive.time
2023-03-31 11:25:55  [ main:214 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.cache.clean.until
2023-03-31 11:25:55  [ main:215 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.delegation.token.lifetime
2023-03-31 11:25:55  [ main:215 ] - [ DEBUG ]  Handling deprecation for hive.metastore.archive.intermediate.archived
2023-03-31 11:25:55  [ main:215 ] - [ DEBUG ]  Handling deprecation for hive.metastore.warehouse.dir
2023-03-31 11:25:55  [ main:215 ] - [ DEBUG ]  Handling deprecation for hive.hwi.listen.host
2023-03-31 11:25:55  [ main:216 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.ldap.guidKey
2023-03-31 11:25:55  [ main:216 ] - [ DEBUG ]  Handling deprecation for hive.stats.collect.scancols
2023-03-31 11:25:55  [ main:216 ] - [ DEBUG ]  Handling deprecation for hive.hwi.war.file
2023-03-31 11:25:55  [ main:216 ] - [ DEBUG ]  Handling deprecation for hive.tez.input.format
2023-03-31 11:25:55  [ main:216 ] - [ DEBUG ]  Handling deprecation for hive.test.dummystats.publisher
2023-03-31 11:25:55  [ main:216 ] - [ DEBUG ]  Handling deprecation for hive.metastore.port
2023-03-31 11:25:55  [ main:216 ] - [ DEBUG ]  Handling deprecation for hive.spark.dynamic.partition.pruning
2023-03-31 11:25:55  [ main:217 ] - [ DEBUG ]  Handling deprecation for hive.strict.checks.large.query
2023-03-31 11:25:55  [ main:217 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.cookie.is.httponly
2023-03-31 11:25:55  [ main:219 ] - [ DEBUG ]  Handling deprecation for hive.metastore.uris
2023-03-31 11:25:55  [ main:219 ] - [ DEBUG ]  Handling deprecation for hive.querylog.location
2023-03-31 11:25:55  [ main:219 ] - [ DEBUG ]  Handling deprecation for hive.localize.resource.num.wait.attempts
2023-03-31 11:25:55  [ main:219 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.default.stripe.size
2023-03-31 11:25:55  [ main:219 ] - [ DEBUG ]  Handling deprecation for hive.querylog.plan.progress.interval
2023-03-31 11:25:55  [ main:219 ] - [ DEBUG ]  Handling deprecation for hive.limit.optimize.enable
2023-03-31 11:25:55  [ main:219 ] - [ DEBUG ]  Handling deprecation for hive.exec.job.debug.timeout
2023-03-31 11:25:55  [ main:219 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.createtable.role.grants
2023-03-31 11:25:55  [ main:220 ] - [ DEBUG ]  Handling deprecation for hive.decode.partition.name
2023-03-31 11:25:55  [ main:220 ] - [ DEBUG ]  Handling deprecation for hive.metastore.partition.inherit.table.properties
2023-03-31 11:25:55  [ main:220 ] - [ DEBUG ]  Handling deprecation for hive.cluster.delegation.token.store.class
2023-03-31 11:25:55  [ main:220 ] - [ DEBUG ]  Handling deprecation for hive.metastore.metrics.enabled
2023-03-31 11:25:55  [ main:220 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.default.row.index.stride
2023-03-31 11:25:55  [ main:220 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.cookie.max.age
2023-03-31 11:25:55  [ main:220 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.client.port
2023-03-31 11:25:55  [ main:220 ] - [ DEBUG ]  Handling deprecation for hive.alias
2023-03-31 11:25:55  [ main:220 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.exponential.backoff.slot.length
2023-03-31 11:25:55  [ main:220 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.mapjoin.native.enabled
2023-03-31 11:25:55  [ main:220 ] - [ DEBUG ]  Handling deprecation for hive.server2.tez.default.queues
2023-03-31 11:25:55  [ main:220 ] - [ DEBUG ]  Handling deprecation for hive.compat
2023-03-31 11:25:55  [ main:220 ] - [ DEBUG ]  Handling deprecation for hive.mapred.partitioner
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.allocator.alloc.min
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.async.log.enabled
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.merge.smallfiles.avgsize
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.client.user
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.hbase.wal.enabled
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.entity.capture.transform
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.allow.udf.load.on.demand
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.server2.logging.operation.enabled
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.index.blockfilter.file
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.lockmgr.zookeeper.default.partition.name
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.wait.queue.comparator.class.name
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.support.concurrency
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.output.service.port
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.orc.cache.use.soft.references
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.file.max.footer
2023-03-31 11:25:55  [ main:221 ] - [ DEBUG ]  Handling deprecation for hive.cli.tez.session.async
2023-03-31 11:25:55  [ main:222 ] - [ DEBUG ]  Handling deprecation for hive.test.mode.prefix
2023-03-31 11:25:55  [ main:222 ] - [ DEBUG ]  Handling deprecation for hive.cli.print.header
2023-03-31 11:25:55  [ main:222 ] - [ DEBUG ]  Handling deprecation for hive.server2.table.type.mapping
2023-03-31 11:25:55  [ main:222 ] - [ DEBUG ]  Handling deprecation for hive.tez.task.scale.memory.reserve.fraction.max
2023-03-31 11:25:55  [ main:222 ] - [ DEBUG ]  Handling deprecation for hive.metastore.event.db.listener.timetolive
2023-03-31 11:25:55  [ main:222 ] - [ DEBUG ]  Handling deprecation for hive.exec.tasklog.debug.timeout
2023-03-31 11:25:55  [ main:222 ] - [ DEBUG ]  Handling deprecation for hive.hashtable.loadfactor
2023-03-31 11:25:55  [ main:223 ] - [ DEBUG ]  Handling deprecation for hive.metastore.filter.hook
2023-03-31 11:25:55  [ main:223 ] - [ DEBUG ]  Handling deprecation for hive.mapred.local.mem
2023-03-31 11:25:55  [ main:223 ] - [ DEBUG ]  Handling deprecation for hive.optimize.union.remove
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.server2.global.init.file.location
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.metastore.client.drop.partitions.using.expressions
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.outerjoin.supports.filters
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.script.auto.progress
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.exec.dynamic.partition
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.metastore.failure.retries
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.metastore.try.direct.sql
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.tez.container.max.java.heap.fraction
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.intermediate.compression.type
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.hbase.generatehfiles
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.analyze.stmt.collect.partlevel.stats
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.stats.join.factor
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.metastore.pre.event.listeners
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.server2.map.fair.scheduler.queue
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.stats.column.autogather
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.localize.resource.wait.interval
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.optimize.reducededuplication
2023-03-31 11:25:55  [ main:224 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.am.liveness.heartbeat.interval.ms
2023-03-31 11:25:55  [ main:225 ] - [ DEBUG ]  Handling deprecation for hive.optimize.index.filter.compact.minsize
2023-03-31 11:25:55  [ main:226 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.decoding.metrics.percentiles.intervals
2023-03-31 11:25:55  [ main:226 ] - [ DEBUG ]  Handling deprecation for hive.metastore.sasl.enabled
2023-03-31 11:25:55  [ main:226 ] - [ DEBUG ]  Handling deprecation for hive.exec.copyfile.maxsize
2023-03-31 11:25:55  [ main:226 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.enabled
2023-03-31 11:25:55  [ main:226 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.manager
2023-03-31 11:25:55  [ main:226 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.compression.strategy
2023-03-31 11:25:55  [ main:226 ] - [ DEBUG ]  Handling deprecation for hive.rpc.query.plan
2023-03-31 11:25:55  [ main:226 ] - [ DEBUG ]  Handling deprecation for hive.tez.bucket.pruning.compat
2023-03-31 11:25:55  [ main:226 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.spnego.principal
2023-03-31 11:25:55  [ main:226 ] - [ DEBUG ]  Handling deprecation for hive.merge.mapredfiles
2023-03-31 11:25:55  [ main:228 ] - [ DEBUG ]  Handling deprecation for hive.cache.expr.evaluation
2023-03-31 11:25:55  [ main:228 ] - [ DEBUG ]  Handling deprecation for yarn.bin.path
2023-03-31 11:25:55  [ main:228 ] - [ DEBUG ]  Handling deprecation for hive.counters.group.name
2023-03-31 11:25:55  [ main:228 ] - [ DEBUG ]  Handling deprecation for datanucleus.transactionIsolation
2023-03-31 11:25:55  [ main:228 ] - [ DEBUG ]  Handling deprecation for hive.in.test
2023-03-31 11:25:55  [ main:228 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.spnego.keytab
2023-03-31 11:25:55  [ main:228 ] - [ DEBUG ]  Handling deprecation for hive.groupby.skewindata
2023-03-31 11:25:55  [ main:228 ] - [ DEBUG ]  Handling deprecation for hive.metastore.txn.store.impl
2023-03-31 11:25:55  [ main:228 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.clean.until
2023-03-31 11:25:55  [ main:228 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.hybridgrace.hashtable
2023-03-31 11:25:55  [ main:229 ] - [ DEBUG ]  Handling deprecation for hive.stats.reliable
2023-03-31 11:25:55  [ main:229 ] - [ DEBUG ]  Handling deprecation for hive.metastore.batch.retrieve.max
2023-03-31 11:25:55  [ main:229 ] - [ DEBUG ]  Handling deprecation for hive.entity.separator
2023-03-31 11:25:55  [ main:229 ] - [ DEBUG ]  Handling deprecation for hive.binary.record.max.length
2023-03-31 11:25:55  [ main:229 ] - [ DEBUG ]  Handling deprecation for hive.llap.object.cache.enabled
2023-03-31 11:25:55  [ main:229 ] - [ DEBUG ]  Handling deprecation for hive.exec.max.dynamic.partitions
2023-03-31 11:25:55  [ main:229 ] - [ DEBUG ]  Handling deprecation for hive.map.groupby.sorted
2023-03-31 11:25:55  [ main:229 ] - [ DEBUG ]  Handling deprecation for hive.hashtable.initialCapacity
2023-03-31 11:25:55  [ main:229 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.check.memory.rows
2023-03-31 11:25:55  [ main:229 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.task.preemption.metrics.intervals
2023-03-31 11:25:55  [ main:230 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.shuffle.dir.watcher.enabled
2023-03-31 11:25:55  [ main:230 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.allocator.arena.count
2023-03-31 11:25:55  [ main:230 ] - [ DEBUG ]  Handling deprecation for hive.server2.idle.operation.timeout
2023-03-31 11:25:55  [ main:230 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.default.block.size
2023-03-31 11:25:55  [ main:230 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.hdfs.read
2023-03-31 11:25:55  [ main:230 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.communicator.connection.timeout.ms
2023-03-31 11:25:55  [ main:230 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.server.connect.timeout
2023-03-31 11:25:55  [ main:230 ] - [ DEBUG ]  Handling deprecation for hive.server2.parallel.ops.in.session
2023-03-31 11:25:55  [ main:231 ] - [ DEBUG ]  Handling deprecation for hive.server2.transport.mode
2023-03-31 11:25:55  [ main:231 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.path
2023-03-31 11:25:55  [ main:231 ] - [ DEBUG ]  Handling deprecation for hive.metastore.execute.setugi
2023-03-31 11:25:55  [ main:231 ] - [ DEBUG ]  Handling deprecation for hive.index.compact.query.max.entries
2023-03-31 11:25:55  [ main:231 ] - [ DEBUG ]  Handling deprecation for hive.transpose.aggr.join
2023-03-31 11:25:55  [ main:231 ] - [ DEBUG ]  Handling deprecation for mapreduce.input.fileinputformat.split.maxsize
2023-03-31 11:25:55  [ main:231 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.bucket.cache.size
2023-03-31 11:25:55  [ main:231 ] - [ DEBUG ]  Handling deprecation for hive.exec.drop.ignorenonexistent
2023-03-31 11:25:55  [ main:231 ] - [ DEBUG ]  Handling deprecation for hive.groupby.limit.extrastep
2023-03-31 11:25:55  [ main:232 ] - [ DEBUG ]  Handling deprecation for hive.spark.dynamic.partition.pruning.max.data.size
2023-03-31 11:25:55  [ main:232 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggr.stats.invalidator.frequency
2023-03-31 11:25:55  [ main:233 ] - [ DEBUG ]  Handling deprecation for hive.serdes.using.metastore.for.schema
2023-03-31 11:25:55  [ main:234 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.allocator.mmap
2023-03-31 11:25:55  [ main:236 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.use.lrfu
2023-03-31 11:25:55  [ main:236 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.resultset.max.fetch.size
2023-03-31 11:25:55  [ main:236 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.use.ssl
2023-03-31 11:25:55  [ main:236 ] - [ DEBUG ]  Handling deprecation for hive.test.mode.nosamplelist
2023-03-31 11:25:55  [ main:236 ] - [ DEBUG ]  Handling deprecation for hive.merge.sparkfiles
2023-03-31 11:25:55  [ main:236 ] - [ DEBUG ]  Handling deprecation for hive.exim.uri.scheme.whitelist
2023-03-31 11:25:55  [ main:236 ] - [ DEBUG ]  Handling deprecation for hive.conf.hidden.list
2023-03-31 11:25:55  [ main:236 ] - [ DEBUG ]  Handling deprecation for hive.exec.query.redactor.hooks
2023-03-31 11:25:55  [ main:237 ] - [ DEBUG ]  Handling deprecation for hive.exec.log4j.file
2023-03-31 11:25:55  [ main:237 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.sasl.qop
2023-03-31 11:25:55  [ main:237 ] - [ DEBUG ]  Handling deprecation for hive.compactor.delta.num.threshold
2023-03-31 11:25:55  [ main:237 ] - [ DEBUG ]  Handling deprecation for hive.exec.plan
2023-03-31 11:25:55  [ main:237 ] - [ DEBUG ]  Handling deprecation for hive.script.serde
2023-03-31 11:25:55  [ main:237 ] - [ DEBUG ]  Handling deprecation for hive.log4j.file
2023-03-31 11:25:55  [ main:237 ] - [ DEBUG ]  Handling deprecation for hive.ddl.createtablelike.properties.whitelist
2023-03-31 11:25:55  [ main:239 ] - [ DEBUG ]  Handling deprecation for mapreduce.input.fileinputformat.split.minsize.per.node
2023-03-31 11:25:55  [ main:239 ] - [ DEBUG ]  Handling deprecation for hive.optimize.bucketmapjoin
2023-03-31 11:25:55  [ main:239 ] - [ DEBUG ]  Handling deprecation for hive.map.aggr.hash.percentmemory
2023-03-31 11:25:55  [ main:239 ] - [ DEBUG ]  Handling deprecation for hive.exec.job.debug.capture.stacktraces
2023-03-31 11:25:55  [ main:239 ] - [ DEBUG ]  Handling deprecation for hive.metastore.server.max.message.size
2023-03-31 11:25:55  [ main:240 ] - [ DEBUG ]  Handling deprecation for hive.cluster.delegation.token.store.zookeeper.acl
2023-03-31 11:25:55  [ main:240 ] - [ DEBUG ]  Handling deprecation for hive.service.metrics.file.location
2023-03-31 11:25:55  [ main:240 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.use.row.serde.deserialize
2023-03-31 11:25:55  [ main:240 ] - [ DEBUG ]  Handling deprecation for hive.server2.compile.lock.timeout
2023-03-31 11:25:55  [ main:240 ] - [ DEBUG ]  Handling deprecation for hive.sample.seednumber
2023-03-31 11:25:55  [ main:240 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.client.retry.delay.seconds
2023-03-31 11:25:55  [ main:240 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.mapjoin.minmax.enabled
2023-03-31 11:25:55  [ main:240 ] - [ DEBUG ]  Handling deprecation for hive.metastore.event.clean.freq
2023-03-31 11:25:55  [ main:240 ] - [ DEBUG ]  Handling deprecation for hive.server2.session.hook
2023-03-31 11:25:55  [ main:244 ] - [ DEBUG ]  Handling deprecation for hive.mapred.reduce.tasks.speculative.execution
2023-03-31 11:25:55  [ main:245 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.sortmerge.join.bigtable.selection.policy
2023-03-31 11:25:55  [ main:245 ] - [ DEBUG ]  Handling deprecation for hive.stageid.rearrange
2023-03-31 11:25:55  [ main:245 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.groupby.flush.percent
2023-03-31 11:25:55  [ main:245 ] - [ DEBUG ]  Handling deprecation for hive.timedout.txn.reaper.interval
2023-03-31 11:25:55  [ main:245 ] - [ DEBUG ]  Handling deprecation for hive.exec.temporary.table.storage
2023-03-31 11:25:55  [ main:245 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.groupby.maxentries
2023-03-31 11:25:55  [ main:246 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.optimized.hashtable
2023-03-31 11:25:55  [ main:246 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggregate.stats.max.variance
2023-03-31 11:25:55  [ main:246 ] - [ DEBUG ]  Handling deprecation for hive.limit.optimize.fetch.max
2023-03-31 11:25:55  [ main:246 ] - [ DEBUG ]  Handling deprecation for hive.security.authenticator.manager
2023-03-31 11:25:55  [ main:246 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.num.file.cleaner.threads
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.client.stats.publishers
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.test.fail.compaction
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.exec.parallel
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.io.rcfile.record.interval
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.lrfu.lambda
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.exec.submitviachild
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.fetch.task.conversion
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.service.metrics.class
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.udtf.auto.progress
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.archive.enabled
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.server2.builtin.udf.whitelist
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.rpc.max.size
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.spnego.principal
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.test.authz.sstd.hs2.mode
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.convert.join.bucket.mapjoin.tez
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.server2.async.exec.threads
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.execution.engine
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.allocator.mmap.path
2023-03-31 11:25:55  [ main:247 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.download.permanent.fns
2023-03-31 11:25:55  [ main:248 ] - [ DEBUG ]  Handling deprecation for hive.tez.container.size
2023-03-31 11:25:55  [ main:248 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.ConnectionPassword
2023-03-31 11:25:55  [ main:248 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.max.historic.queries
2023-03-31 11:25:55  [ main:248 ] - [ DEBUG ]  Handling deprecation for hive.server2.use.SSL
2023-03-31 11:25:55  [ main:248 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.reducesink.new.enabled
2023-03-31 11:25:55  [ main:248 ] - [ DEBUG ]  Handling deprecation for hive.optimize.null.scan
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.compactor.max.num.delta
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.llap.zk.sm.keytab.file
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.compactor.history.retention.attempted
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.size
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.smalltable.filesize
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.session.silent
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.query.string
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.port
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.min.worker.threads
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.join.use.nonstaged
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.compactor.initiator.failed.compacts.threshold
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.ldap.groupClassKey
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.server2.tez.sessions.per.default.queue
2023-03-31 11:25:55  [ main:249 ] - [ DEBUG ]  Handling deprecation for hive.optimize.point.lookup
2023-03-31 11:25:55  [ main:251 ] - [ DEBUG ]  Handling deprecation for hive.server2.idle.session.check.operation
2023-03-31 11:25:55  [ main:252 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.port
2023-03-31 11:25:55  [ main:253 ] - [ DEBUG ]  Handling deprecation for hive.llap.allow.permanent.fns
2023-03-31 11:25:55  [ main:253 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.web.ssl
2023-03-31 11:25:55  [ main:253 ] - [ DEBUG ]  Handling deprecation for hive.server2.logging.operation.log.location
2023-03-31 11:25:55  [ main:253 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.hybridgrace.minnumpartitions
2023-03-31 11:25:55  [ main:253 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.ConnectionURL
2023-03-31 11:25:55  [ main:253 ] - [ DEBUG ]  Handling deprecation for hive.semantic.analyzer.hook
2023-03-31 11:25:55  [ main:253 ] - [ DEBUG ]  Handling deprecation for hive.metastore.server.tcp.keepalive
2023-03-31 11:25:55  [ main:253 ] - [ DEBUG ]  Handling deprecation for hive.service.metrics.reporter
2023-03-31 11:25:55  [ main:253 ] - [ DEBUG ]  Handling deprecation for hive.hmshandler.force.reload.conf
2023-03-31 11:25:55  [ main:253 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.rpc.threads
2023-03-31 11:25:55  [ main:254 ] - [ DEBUG ]  Handling deprecation for hive.io.rcfile.column.number.conf
2023-03-31 11:25:55  [ main:254 ] - [ DEBUG ]  Handling deprecation for hive.map.aggr.hash.min.reduction
2023-03-31 11:25:55  [ main:254 ] - [ DEBUG ]  Handling deprecation for hive.compactor.job.queue
2023-03-31 11:25:55  [ main:254 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.cpu
2023-03-31 11:25:55  [ main:254 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.clean.extra.nodes
2023-03-31 11:25:55  [ main:254 ] - [ DEBUG ]  Handling deprecation for hive.optimize.metadataonly
2023-03-31 11:25:55  [ main:254 ] - [ DEBUG ]  Handling deprecation for datanucleus.schema.validateColumns
2023-03-31 11:25:55  [ main:254 ] - [ DEBUG ]  Handling deprecation for hive.insert.into.multilevel.dirs
2023-03-31 11:25:55  [ main:254 ] - [ DEBUG ]  Handling deprecation for hive.added.archives.path
2023-03-31 11:25:55  [ main:254 ] - [ DEBUG ]  Handling deprecation for hive.hmshandler.retry.attempts
2023-03-31 11:25:55  [ main:254 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.memory.pool
2023-03-31 11:25:55  [ main:255 ] - [ DEBUG ]  Handling deprecation for hive.prewarm.numcontainers
2023-03-31 11:25:55  [ main:255 ] - [ DEBUG ]  Handling deprecation for datanucleus.identifierFactory
2023-03-31 11:25:55  [ main:255 ] - [ DEBUG ]  Handling deprecation for hive.cli.errors.ignore
2023-03-31 11:25:55  [ main:255 ] - [ DEBUG ]  Handling deprecation for hive.multigroupby.singlereducer
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.llap.execution.mode
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.txn.manager.dump.lock.state.on.acquire.timeout
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.compactor.history.retention.succeeded
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.llap.enable.grace.join.in.llap
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.conf.restricted.list
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.optimize.sampling.orderby.number
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.fetch.task.aggr
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.sortmerge.join.to.mapjoin
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.optimize.limittranspose
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.memory.mode
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.txn.timeout
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.warehouse.subdir.inherit.perms
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.stats.fetch.partition.stats
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.use.fileid.path
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.auto.progress.timeout
2023-03-31 11:25:55  [ main:256 ] - [ DEBUG ]  Handling deprecation for hive.cbo.returnpath.hiveop
2023-03-31 11:25:55  [ main:257 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.threadpool.size
2023-03-31 11:25:55  [ main:259 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.dictionary.key.size.threshold
2023-03-31 11:25:55  [ main:260 ] - [ DEBUG ]  Handling deprecation for hive.exec.scratchdir
2023-03-31 11:25:55  [ main:261 ] - [ DEBUG ]  Handling deprecation for hive.metastore.server.max.threads
2023-03-31 11:25:55  [ main:261 ] - [ DEBUG ]  Handling deprecation for hive.limit.optimize.limit.file
2023-03-31 11:25:55  [ main:261 ] - [ DEBUG ]  Handling deprecation for hive.exec.script.allow.partial.consumption
2023-03-31 11:25:55  [ main:261 ] - [ DEBUG ]  Handling deprecation for hive.metastore.try.direct.sql.ddl
2023-03-31 11:25:55  [ main:261 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.keystore.password
2023-03-31 11:25:55  [ main:261 ] - [ DEBUG ]  Handling deprecation for hive.scratchdir.lock
2023-03-31 11:25:55  [ main:261 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.namespace
2023-03-31 11:25:55  [ main:261 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.hybridgrace.minwbsize
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.server2.long.polling.timeout
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.debug.localtask
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.use.spnego
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.createtable.user.grants
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.server.tcp.keepalive
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.service.metrics.file.frequency
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.optimize.ppd
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.exec.script.maxerrsize
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.worker.keepalive.time
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.acl.blocked
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.connect.timeout
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.session.id
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.enforce.bucketmapjoin
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.server2.allow.user.substitution
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.join.noconditionaltask
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.input.format
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.optimize.index.autoupdate
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.ssl.protocol.blacklist
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.stats.fetch.column.stats
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.tez.dynamic.partition.pruning
2023-03-31 11:25:55  [ main:262 ] - [ DEBUG ]  Handling deprecation for hive.exec.max.dynamic.partitions.pernode
2023-03-31 11:25:55  [ main:263 ] - [ DEBUG ]  Handling deprecation for hive.compactor.cleaner.run.interval
2023-03-31 11:25:55  [ main:263 ] - [ DEBUG ]  Handling deprecation for hive.skewjoin.mapjoin.map.tasks
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for mapreduce.job.reduces
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.metastore.schema.verification.record.version
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.compactor.abortedtxn.threshold
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.scheduler.timeout.seconds
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.map.aggr
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.support.quoted.identifiers
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.optimize.filter.stats.reduction
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for javax.jdo.PersistenceManagerFactoryClass
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.compactor.initiator.on
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.orc.row.index.stride.dictionary.check
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.metastore.fs.handler.class
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.task.factory
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.lock.numretries
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.typecheck.on.insert
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.join
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.server2.support.dynamic.service.discovery
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.optimize.distinct.rewrite
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.metastore.authorization.storage.checks
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.skip.corrupt.data
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.base.delta.ratio
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.metastore.fastpath
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.llap.zk.sm.principal
2023-03-31 11:25:55  [ main:264 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.optimized.hashtable.probe.percent
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for datanucleus.cache.level2
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.server2.builtin.udf.blacklist
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.test.fail.heartbeater
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for datanucleus.schema.validateTables
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.metastore.kerberos.principal
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for datanucleus.rdbms.useLegacyNativeValueStrategy
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.scheduler.node.reenable.min.timeout.ms
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.llap.file.cleanup.delay.seconds
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.llap.management.rpc.port
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.optimize.ppd.storage
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.support.special.characters.tablename
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.llap.validate.acls
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.mv.files.thread
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.llap.skip.compile.udf.check
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.index.compact.binary.search
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.local.fs.read
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.hybridgrace.bloomfilter
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.max.full
2023-03-31 11:25:55  [ main:265 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.enabled
2023-03-31 11:25:55  [ main:267 ] - [ DEBUG ]  Handling deprecation for hive.optimize.correlation
2023-03-31 11:25:55  [ main:268 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.cookie.is.secure
2023-03-31 11:25:55  [ main:268 ] - [ DEBUG ]  Handling deprecation for hive.reorder.nway.joins
2023-03-31 11:25:55  [ main:268 ] - [ DEBUG ]  Handling deprecation for hive.merge.orcfile.stripe.level
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.exec.compress.output
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.user.install.directory
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.stats.list.num.entries
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.sqlstd.confwhitelist.append
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.insert.into.external.tables
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.groupby.checkinterval
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.explain.dependency.append.tasktype
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.optimize.bucketingsorting
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.login.timeout
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.scratch.dir.permission
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.cli.print.current.db
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.hashtable.key.count.adjustment
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.exec.failure.hooks
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.metastore.integral.jdo.pushdown
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.txn.retryable.sqlex.regex
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.keytab.file
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.io.exception.handlers
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.jobname.length
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.enforce.tree
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.added.jars.path
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.direct.sql.max.query.length
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.bind.host
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.server2.tez.initialize.default.sessions
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.metastore.client.socket.timeout
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.compactor.history.retention.failed
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.DetachAllOnCommit
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.txn.max.open.batch
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.compactor.check.interval
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.server2.close.session.on.disconnect
2023-03-31 11:25:55  [ main:269 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.yarn.container.mb
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.optimize.ppd.windowing
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.query.id
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.transactional.table.scan
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.compactor.delta.pct.threshold
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.reduce.enabled
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.ConnectionDriverName
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.current.database
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.metastore.orm.retrieveMapNullsAsEmptyStrings
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.stats.max.variable.length
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.llap.orc.gap.cache
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.start.cleanup.scratchdir
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.merge.tezfiles
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.exec.rcfile.use.explicit.header
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.metastore.initial.metadata.count.enabled
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.split.strategy
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.server2.async.exec.keepalive.time
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.optimize.index.filter
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.security.authorization.sqlstd.confwhitelist
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.optimize.dynamic.partition.hashjoin
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.optimize.listbucketing
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.default.serde
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.connection.basesleeptime
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.host
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.metastore.ds.connection.url.hook
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.query.result.fileformat
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.metastore.partition.name.whitelist.pattern
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.stats.map.num.entries
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.tez.dynamic.partition.pruning.max.event.size
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.cbo.enable
2023-03-31 11:25:55  [ main:270 ] - [ DEBUG ]  Handling deprecation for hive.optimize.constant.propagation
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.exec.mode.local.auto
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.optimize.reducededuplication.min.reducer
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.transform.escape.input
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.orc.splits.ms.footer.cache.enabled
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.optimize.point.lookup.min
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.server2.max.start.attempts
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.exec.dynamic.partition.mode
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.max.worker.threads
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.network
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.fpp
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.exec.driver.run.hooks
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.conf.validation
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.exec.pre.hooks
2023-03-31 11:25:55  [ main:271 ] - [ DEBUG ]  Handling deprecation for hive.unlock.numretries
2023-03-31 11:25:55  [ main:272 ] - [ DEBUG ]  Handling deprecation for hive.script.operator.id.env.var
2023-03-31 11:25:55  [ main:272 ] - [ DEBUG ]  Handling deprecation for hive.session.history.enabled
2023-03-31 11:25:55  [ main:272 ] - [ DEBUG ]  Handling deprecation for hive.added.files.path
2023-03-31 11:25:55  [ main:272 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.Multithreaded
2023-03-31 11:25:55  [ main:272 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.task.scheduler.enable.preemption
2023-03-31 11:25:55  [ main:272 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.num.executors
2023-03-31 11:25:55  [ main:272 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.file.metadata.threads
2023-03-31 11:25:55  [ main:272 ] - [ DEBUG ]  Handling deprecation for hive.rework.mapredwork
2023-03-31 11:25:55  [ main:274 ] - [ DEBUG ]  Handling deprecation for hive.metastore.client.connect.retry.delay
2023-03-31 11:25:55  [ main:275 ] - [ DEBUG ]  Handling deprecation for hive.optimize.groupby
2023-03-31 11:25:55  [ main:275 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.cache.max.full
2023-03-31 11:25:55  [ main:275 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.connection.class
2023-03-31 11:25:55  [ main:275 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.service.principal
2023-03-31 11:25:55  [ main:275 ] - [ DEBUG ]  Handling deprecation for hive.exec.check.crossproducts
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.server.read.socket.timeout
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.exec.reducers.max
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.service.refresh.interval.sec
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.fetch.task.conversion.threshold
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.exec.perf.logger
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.limit.row.max.size
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.metastore.thrift.compact.protocol.enabled
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.max.output.size
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.rpc.server.address
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for datanucleus.plugin.pluginRegistryBundleCheck
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.join.noconditionaltask.size
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.script.operator.truncate.env
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.join.cache.size
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.metastore.dbaccess.ssl.properties
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.exec.parallel.thread.number
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.client.password
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.driver.parallel.compilation
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.skewjoin.key
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.max.reader.wait
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for datanucleus.rdbms.initializeColumnInfo
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.security.metastore.authenticator.manager
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.llap.remote.token.requires.signing
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.cache.max.writer.wait
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.default.fileformat.managed
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.reloadable.aux.jars.path
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.tez.bucket.pruning
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.request.header.size
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.index.compact.file.ignore.hdfs
2023-03-31 11:25:55  [ main:276 ] - [ DEBUG ]  Handling deprecation for hive.llap.cache.allow.synthetic.fileid
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.max.threads
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.hash.table.inflation.factor
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.optimize.limittranspose.reductiontuples
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.test.rollbacktxn
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.scheduler.num.schedulable.tasks.per.node
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.acl
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.hmshandler.retry.interval
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.memory.size
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggr.stats.hbase.ttl
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.exec.local.scratchdir
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.max.message.size
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.mapred.mode
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.strict.checks.type.safety
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.default.buffer.size
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.server2.async.exec.async.compile
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.stats.gather.num.threads
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.max.input.size
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.limit.pushdown.memory.usage
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.metastore.archive.intermediate.original
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.exec.mode.local.auto.inputbytes.max
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.enforce.vectorized
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.localtask.max.memory.usage
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.tez.enable.memory.manager
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.writeset.reaper.interval
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.support.sql11.reserved.keywords
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.metastore.batch.retrieve.table.partition.max
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.use.vector.serde.deserialize
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.tez.dynamic.partition.pruning.max.data.size
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.metadata.move.exported.metadata.to.trash
2023-03-31 11:25:55  [ main:277 ] - [ DEBUG ]  Handling deprecation for hive.cli.pretty.output.num.cols
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.orc.splits.allow.synthetic.fileid
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.session.timeout
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.fetch.output.serde
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.order.columnalignment
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.hbase.snapshot.restoredir
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.log.trace.id
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.skewjoin.mapjoin.min.split
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.resultset.use.unique.column.names
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.output.service.send.buffer.size
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.zookeeper.connection.max.retries
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.server2.session.check.interval
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.compute.splits.in.am
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.max.partitions
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.compactor.worker.timeout
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.stats.filter.in.factor
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for parquet.memory.pool.ratio
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.kerberos.principal
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.new.job.grouping.set.cardinality
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.exec.schema.evolution
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.client.stats.counters
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.enforce.sortmergebucketmapjoin
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.direct.sql.max.elements.values.clause
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.tez.smb.number.waves
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.max.writer.wait
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.spnego.keytab
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.ppd.recognizetransivity
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.allow.uber
2023-03-31 11:25:55  [ main:278 ] - [ DEBUG ]  Handling deprecation for hive.server2.llap.concurrent.queries
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.io.rcfile.tolerate.corruptions
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.server2.webui.keystore.path
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.secret.bits
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for stream.stderr.reporter.enabled
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.autogen.columnalias.prefix.label
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.llap.auto.auth
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.metastore.event.listeners
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.repl.task.factory
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.int.timestamp.conversion.in.seconds
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.tez.auto.reducer.parallelism
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.metastore.rawstore.impl
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.security.metastore.authorization.manager
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.orc.splits.include.fileid
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.jar.path
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.communicator.num.threads
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.communicator.connection.sleep.between.retries.ms
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.server2.tez.session.lifetime.jitter
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.aggregate.stats.max.partitions
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.mapjoin.native.multikey.only.enabled
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.orc.compute.splits.num.threads
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.limit.query.max.table.partition
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.exec.rowoffset
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.web.port
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.stats.default.publisher
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.script.recordwriter
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.service.metrics.hadoop2.component
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.ppd.remove.duplicatefilters
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.yarn.shuffle.port
2023-03-31 11:25:55  [ main:279 ] - [ DEBUG ]  Handling deprecation for hive.server2.keystore.password
2023-03-31 11:25:55  [ main:280 ] - [ DEBUG ]  Handling deprecation for hive.strict.checks.cartesian.product
2023-03-31 11:25:55  [ main:280 ] - [ DEBUG ]  Handling deprecation for hive.server2.logging.operation.level
2023-03-31 11:25:55  [ main:280 ] - [ DEBUG ]  Handling deprecation for hive.variable.substitute
2023-03-31 11:25:55  [ main:280 ] - [ DEBUG ]  Handling deprecation for hive.txn.manager
2023-03-31 11:25:55  [ main:281 ] - [ DEBUG ]  Handling deprecation for datanucleus.cache.level2.type
2023-03-31 11:25:55  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.rpc.num.handlers
2023-03-31 11:25:55  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.metastore.stats.ndv.densityfunction
2023-03-31 11:25:55  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.direct.sql.max.elements.in.clause
2023-03-31 11:25:55  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.metastore.direct.sql.batch.size
2023-03-31 11:25:55  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.vcpus.per.instance
2023-03-31 11:25:55  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.intermediate.compression.codec
2023-03-31 11:25:55  [ main:281 ] - [ DEBUG ]  Handling deprecation for hive.metastore.server.min.threads
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.tez.exec.print.summary
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.count.open.txns.interval
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.exec.compress.intermediate
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.enabled
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.metastore.expression.proxy
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.optimize.partition.columns.separate
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.script.recordreader
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.stats.autogather
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.optimize.sort.dynamic.partition
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.metastore.init.hooks
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.metastore.dml.events
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.metastore.thrift.framed.transport.enabled
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.log.every.n.records
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.scheduler.locality.delay
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.txn.heartbeat.threadpool.size
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.index.compact.query.max.size
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.heartbeat.interval
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.vectorized.execution.reduce.groupby.enabled
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.lock.sleep.between.retries
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.test.mode.samplefreq
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.stats.dbclass
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.scheduler.node.disable.backoff.factor
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.http.response.header.size
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.conf.internal.variable.list
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.exec.concatenate.check.index
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication
2023-03-31 11:25:55  [ main:282 ] - [ DEBUG ]  Handling deprecation for hive.exec.rcfile.use.sync.cache
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for datanucleus.connectionPoolingType
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.map.aggr.hash.force.flush.memory.threshold
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.metastore.cache.pinobjtypes
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.optimize.limittranspose.reductionpercentage
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.fileformat.check
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.server2.async.exec.wait.queue.size
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.stats.default.aggregator
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.explain.user
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.server2.keystore.path
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.client.retry.limit
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.encoding.strategy
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.metastore.schema.verification
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.llap.am.liveness.connection.sleep.between.retries.ms
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.server2.thrift.resultset.serialize.in.tasks
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.metastore.connect.retries
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.cluster.delegation.token.store.zookeeper.connectString
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.exec.infer.bucket.sort
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.metastore.aggregate.stats.cache.ttl
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.index.compact.file
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.exec.submit.local.task.via.child
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.hwi.listen.port
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.metastore.token.signature
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.memory.per.instance.mb
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.metastore.archive.intermediate.extracted
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.cluster.delegation.token.store.zookeeper.znode
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.service.metrics.hadoop2.frequency
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.orc.splits.directory.batch.ms
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.metastore.hbase.cache.max.reader.wait
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.hdfs.write
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.server2.authentication.kerberos.keytab
2023-03-31 11:25:55  [ main:283 ] - [ DEBUG ]  Handling deprecation for hive.tez.cpu.vcores
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.msck.path.validation
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.followby.map.aggr.hash.percentmemory
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.tez.task.scale.memory.reserve.fraction
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.parquet.timestamp.skip.conversion
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.lock.manager
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.tez.exec.inplace.progress
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.variable.substitute.depth
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.mapper.cannot.span.multiple.partitions
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.merge.size.per.task
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.table.parameters.default
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.optimize.sampling.orderby.percent
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.ignore.mapjoin.hint
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.compactor.history.reaper.interval
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.lock.mapred.only.operation
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.tez.min.partition.factor
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.exec.orc.default.block.padding
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.spark.client.rpc.sasl.mechanisms
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.optimize.sampling.orderby
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.metastore.kerberos.keytab.file
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.groupby.mapaggr.checkinterval
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.exec.script.trust
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.mapjoin.followby.gby.localtask.max.memory.usage
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for javax.jdo.option.ConnectionUserName
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.spark.job.monitor.timeout
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.exec.show.job.failure.debug.info
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.llap.task.scheduler.node.reenable.max.timeout.ms
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.max.open.txns
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.groupby.orderby.position.alias
2023-03-31 11:25:55  [ main:284 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.extended
2023-03-31 11:25:55  [ main:285 ] - [ DEBUG ]  Handling deprecation for hive.script.operator.env.blacklist
2023-03-31 11:25:55  [ main:285 ] - [ DEBUG ]  Handling deprecation for hive.cbo.costmodel.local.fs.write
2023-03-31 11:25:55  [ main:285 ] - [ DEBUG ]  Handling deprecation for hive.in.tez.test
2023-03-31 11:25:55  [ main:285 ] - [ DEBUG ]  Handling deprecation for hive.tez.input.generate.consistent.splits
2023-03-31 11:25:55  [ main:288 ] - [ DEBUG ]  Handling deprecation for hive.optimize.bucketmapjoin.sortedmerge
2023-03-31 11:25:55  [ main:290 ] - [ DEBUG ]  Handling deprecation for hive.stats.ndv.error
2023-03-31 11:25:55  [ main:290 ] - [ DEBUG ]  Handling deprecation for hive.server2.enable.doAs
2023-03-31 11:25:55  [ main:290 ] - [ DEBUG ]  Handling deprecation for hive.server2.zookeeper.namespace
2023-03-31 11:25:55  [ main:290 ] - [ DEBUG ]  Handling deprecation for hive.stats.atomic
2023-03-31 11:25:55  [ main:290 ] - [ DEBUG ]  Handling deprecation for hive.server2.zookeeper.publish.configs
2023-03-31 11:25:55  [ main:290 ] - [ DEBUG ]  Handling deprecation for hive.llap.daemon.work.dirs
2023-03-31 11:25:55  [ main:290 ] - [ DEBUG ]  Handling deprecation for datanucleus.schema.autoCreateAll
2023-03-31 11:25:55  [ main:290 ] - [ DEBUG ]  Handling deprecation for hive.optimize.index.groupby
2023-03-31 11:25:55  [ main:290 ] - [ DEBUG ]  Handling deprecation for hive.auto.convert.sortmerge.join
2023-03-31 11:25:55  [ main:290 ] - [ DEBUG ]  Handling deprecation for hive.server2.xsrf.filter.enabled
2023-03-31 11:25:55  [ main:290 ] - [ DEBUG ]  Handling deprecation for hive.server2.idle.session.timeout
2023-03-31 11:25:55  [ main:290 ] - [ DEBUG ]  Handling deprecation for hive.llap.io.allocator.alloc.max
2023-03-31 11:25:56  [ main:465 ] - [ DEBUG ]  field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
2023-03-31 11:25:56  [ main:478 ] - [ DEBUG ]  field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
2023-03-31 11:25:56  [ main:479 ] - [ DEBUG ]  field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[GetGroups])
2023-03-31 11:25:56  [ main:479 ] - [ DEBUG ]  field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
2023-03-31 11:25:56  [ main:480 ] - [ DEBUG ]  field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
2023-03-31 11:25:56  [ main:480 ] - [ DEBUG ]  UgiMetrics, User and group related metrics
2023-03-31 11:25:56  [ main:507 ] - [ DEBUG ]  Setting hadoop.security.token.service.use_ip to true
2023-03-31 11:25:56  [ main:540 ] - [ DEBUG ]   Creating new Groups object
2023-03-31 11:25:56  [ main:560 ] - [ DEBUG ]  Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping; cacheTimeout=300000; warningDeltaMs=5000
2023-03-31 11:25:56  [ main:569 ] - [ DEBUG ]  hadoop login
2023-03-31 11:25:56  [ main:570 ] - [ DEBUG ]  hadoop login commit
2023-03-31 11:25:56  [ main:573 ] - [ DEBUG ]  using local user:UnixPrincipal: renzhuo
2023-03-31 11:25:56  [ main:573 ] - [ DEBUG ]  Using user: "UnixPrincipal: renzhuo" with name renzhuo
2023-03-31 11:25:56  [ main:574 ] - [ DEBUG ]  User entry: "renzhuo"
2023-03-31 11:25:56  [ main:574 ] - [ DEBUG ]  UGI loginUser:renzhuo (auth:SIMPLE)
2023-03-31 11:25:56  [ main:621 ] - [ INFO ]  Setting hive conf dir as src/main/resources
2023-03-31 11:25:56  [ main:685 ] - [ DEBUG ]  sampler.classes = ; loaded no samplers
2023-03-31 11:25:56  [ main:689 ] - [ DEBUG ]  span.receiver.classes = ; loaded no span receivers
2023-03-31 11:25:56  [ main:690 ] - [ DEBUG ]  Loading filesystems
2023-03-31 11:25:56  [ main:700 ] - [ DEBUG ]  file:// = class org.apache.hadoop.fs.LocalFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-common/3.0.0/hadoop-common-3.0.0.jar
2023-03-31 11:25:56  [ main:705 ] - [ DEBUG ]  viewfs:// = class org.apache.hadoop.fs.viewfs.ViewFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-common/3.0.0/hadoop-common-3.0.0.jar
2023-03-31 11:25:56  [ main:707 ] - [ DEBUG ]  ftp:// = class org.apache.hadoop.fs.ftp.FTPFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-common/3.0.0/hadoop-common-3.0.0.jar
2023-03-31 11:25:56  [ main:710 ] - [ DEBUG ]  har:// = class org.apache.hadoop.fs.HarFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-common/3.0.0/hadoop-common-3.0.0.jar
2023-03-31 11:25:56  [ main:711 ] - [ DEBUG ]  http:// = class org.apache.hadoop.fs.http.HttpFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-common/3.0.0/hadoop-common-3.0.0.jar
2023-03-31 11:25:56  [ main:712 ] - [ DEBUG ]  https:// = class org.apache.hadoop.fs.http.HttpsFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-common/3.0.0/hadoop-common-3.0.0.jar
2023-03-31 11:25:56  [ main:718 ] - [ DEBUG ]  hdfs:// = class org.apache.hadoop.hdfs.DistributedFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.0.0/hadoop-hdfs-client-3.0.0.jar
2023-03-31 11:25:56  [ main:977 ] - [ DEBUG ]  webhdfs:// = class org.apache.hadoop.hdfs.web.WebHdfsFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.0.0/hadoop-hdfs-client-3.0.0.jar
2023-03-31 11:25:56  [ main:980 ] - [ DEBUG ]  swebhdfs:// = class org.apache.hadoop.hdfs.web.SWebHdfsFileSystem from /Users/renzhuo/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.0.0/hadoop-hdfs-client-3.0.0.jar
2023-03-31 11:25:56  [ main:985 ] - [ DEBUG ]  nullscan:// = class org.apache.hadoop.hive.ql.io.NullScanFileSystem from /Users/renzhuo/.m2/repository/org/apachemvn/hive/hive-exec/2.1.1/hive-exec-2.1.1.jar
2023-03-31 11:25:56  [ main:986 ] - [ DEBUG ]  file:// = class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem from /Users/renzhuo/.m2/repository/org/apachemvn/hive/hive-exec/2.1.1/hive-exec-2.1.1.jar
2023-03-31 11:25:56  [ main:986 ] - [ DEBUG ]  Looking for FS supporting file
2023-03-31 11:25:56  [ main:986 ] - [ DEBUG ]  looking for configuration option fs.file.impl
2023-03-31 11:25:56  [ main:1002 ] - [ DEBUG ]  Looking in service filesystems for implementation class
2023-03-31 11:25:56  [ main:1002 ] - [ DEBUG ]  FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
2023-03-31 11:25:56  [ main:1060 ] - [ INFO ]  Created HiveCatalog 'batch_test'
2023-03-31 11:25:56  [ main:1075 ] - [ INFO ]  Trying to connect to metastore with URI thrift://t-d-datastorage-srv02:9083
2023-03-31 11:25:56  [ main:1123 ] - [ INFO ]  Opened a connection to metastore, current connections: 1
2023-03-31 11:25:56  [ main:1192 ] - [ INFO ]  Connected to metastore.
2023-03-31 11:25:56  [ main:1268 ] - [ INFO ]  Connected to Hive metastore
2023-03-31 11:25:58  [ main:2635 ] - [ DEBUG ]  Plan after converting SqlNode to RelNode
LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
  LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3397 ] - [ DEBUG ]  iteration: 1
2023-03-31 11:25:59  [ main:3415 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3416 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3417 ] - [ DEBUG ]  For final plan, using rel#7:LogicalSink.NONE.any.[](input=HepRelVertex#6,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3417 ] - [ DEBUG ]  For final plan, using rel#5:LogicalProject.NONE.any.[](input=HepRelVertex#4,inputs=0..2)
2023-03-31 11:25:59  [ main:3417 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3476 ] - [ DEBUG ]  optimize convert table references before rewriting sub-queries to semi-join cost 22 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3476 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3477 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3477 ] - [ DEBUG ]  For final plan, using rel#12:LogicalSink.NONE.any.[](input=HepRelVertex#11,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3477 ] - [ DEBUG ]  For final plan, using rel#10:LogicalProject.NONE.any.[](input=HepRelVertex#9,inputs=0..2)
2023-03-31 11:25:59  [ main:3477 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3477 ] - [ DEBUG ]  optimize rewrite sub-queries to semi-join cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3478 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3478 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3478 ] - [ DEBUG ]  For final plan, using rel#17:LogicalSink.NONE.any.[](input=HepRelVertex#16,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3478 ] - [ DEBUG ]  For final plan, using rel#15:LogicalProject.NONE.any.[](input=HepRelVertex#14,inputs=0..2)
2023-03-31 11:25:59  [ main:3478 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3478 ] - [ DEBUG ]  optimize sub-queries remove cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3479 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3479 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3480 ] - [ DEBUG ]  For final plan, using rel#22:LogicalSink.NONE.any.[](input=HepRelVertex#21,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3480 ] - [ DEBUG ]  For final plan, using rel#20:LogicalProject.NONE.any.[](input=HepRelVertex#19,inputs=0..2)
2023-03-31 11:25:59  [ main:3480 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3480 ] - [ DEBUG ]  optimize convert table references after sub-queries removed cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3481 ] - [ DEBUG ]  optimize subquery_rewrite cost 88 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3481 ] - [ DEBUG ]  iteration: 1
2023-03-31 11:25:59  [ main:3482 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3482 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3483 ] - [ DEBUG ]  For final plan, using rel#27:LogicalSink.NONE.any.[](input=HepRelVertex#26,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3483 ] - [ DEBUG ]  For final plan, using rel#25:LogicalProject.NONE.any.[](input=HepRelVertex#24,inputs=0..2)
2023-03-31 11:25:59  [ main:3483 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3484 ] - [ DEBUG ]  optimize convert correlate to temporal table join cost 2 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3484 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3484 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3484 ] - [ DEBUG ]  For final plan, using rel#32:LogicalSink.NONE.any.[](input=HepRelVertex#31,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3485 ] - [ DEBUG ]  For final plan, using rel#30:LogicalProject.NONE.any.[](input=HepRelVertex#29,inputs=0..2)
2023-03-31 11:25:59  [ main:3485 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3485 ] - [ DEBUG ]  optimize convert enumerable table scan cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3485 ] - [ DEBUG ]  optimize temporal_join_rewrite cost 4 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3497 ] - [ DEBUG ]  optimize decorrelate cost 11 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3498 ] - [ DEBUG ]  call#1: Apply rule [ReduceExpressionsRule(Project)] to [rel#35:LogicalProject.NONE.any.[](input=HepRelVertex#34,inputs=0..2)]
2023-03-31 11:25:59  [ main:3527 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3533 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ReduceExpressionsRule(Project)                                                 1              27,777
* Total                                                                        1              27,777

2023-03-31 11:25:59  [ main:3534 ] - [ DEBUG ]  For final plan, using rel#37:LogicalSink.NONE.any.[](input=HepRelVertex#36,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3534 ] - [ DEBUG ]  For final plan, using rel#35:LogicalProject.NONE.any.[](input=HepRelVertex#34,inputs=0..2)
2023-03-31 11:25:59  [ main:3534 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3535 ] - [ DEBUG ]  optimize default_rewrite cost 37 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3535 ] - [ DEBUG ]  iteration: 1
2023-03-31 11:25:59  [ main:3535 ] - [ DEBUG ]  iteration: 1
2023-03-31 11:25:59  [ main:3536 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3536 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3536 ] - [ DEBUG ]  For final plan, using rel#42:LogicalSink.NONE.any.[](input=HepRelVertex#41,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3536 ] - [ DEBUG ]  For final plan, using rel#40:LogicalProject.NONE.any.[](input=HepRelVertex#39,inputs=0..2)
2023-03-31 11:25:59  [ main:3536 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3537 ] - [ DEBUG ]  optimize join predicate rewrite cost 2 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3537 ] - [ DEBUG ]  call#2: Apply rule [ReduceExpressionsRule(Project)] to [rel#45:LogicalProject.NONE.any.[](input=HepRelVertex#44,inputs=0..2)]
2023-03-31 11:25:59  [ main:3538 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3538 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ReduceExpressionsRule(Project)                                                 1                  52
* Total                                                                        1                  52

2023-03-31 11:25:59  [ main:3538 ] - [ DEBUG ]  For final plan, using rel#47:LogicalSink.NONE.any.[](input=HepRelVertex#46,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3538 ] - [ DEBUG ]  For final plan, using rel#45:LogicalProject.NONE.any.[](input=HepRelVertex#44,inputs=0..2)
2023-03-31 11:25:59  [ main:3538 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3539 ] - [ DEBUG ]  optimize other predicate rewrite cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3539 ] - [ DEBUG ]  iteration: 2
2023-03-31 11:25:59  [ main:3539 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3539 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3539 ] - [ DEBUG ]  For final plan, using rel#52:LogicalSink.NONE.any.[](input=HepRelVertex#51,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3539 ] - [ DEBUG ]  For final plan, using rel#50:LogicalProject.NONE.any.[](input=HepRelVertex#49,inputs=0..2)
2023-03-31 11:25:59  [ main:3539 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3540 ] - [ DEBUG ]  optimize join predicate rewrite cost 0 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3540 ] - [ DEBUG ]  call#3: Apply rule [ReduceExpressionsRule(Project)] to [rel#55:LogicalProject.NONE.any.[](input=HepRelVertex#54,inputs=0..2)]
2023-03-31 11:25:59  [ main:3541 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3541 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ReduceExpressionsRule(Project)                                                 1                  43
* Total                                                                        1                  43

2023-03-31 11:25:59  [ main:3541 ] - [ DEBUG ]  For final plan, using rel#57:LogicalSink.NONE.any.[](input=HepRelVertex#56,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3541 ] - [ DEBUG ]  For final plan, using rel#55:LogicalProject.NONE.any.[](input=HepRelVertex#54,inputs=0..2)
2023-03-31 11:25:59  [ main:3541 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3541 ] - [ DEBUG ]  optimize other predicate rewrite cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3542 ] - [ DEBUG ]  iteration: 3
2023-03-31 11:25:59  [ main:3542 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3542 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3542 ] - [ DEBUG ]  For final plan, using rel#62:LogicalSink.NONE.any.[](input=HepRelVertex#61,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3542 ] - [ DEBUG ]  For final plan, using rel#60:LogicalProject.NONE.any.[](input=HepRelVertex#59,inputs=0..2)
2023-03-31 11:25:59  [ main:3542 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3545 ] - [ DEBUG ]  optimize join predicate rewrite cost 2 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3546 ] - [ DEBUG ]  call#4: Apply rule [ReduceExpressionsRule(Project)] to [rel#65:LogicalProject.NONE.any.[](input=HepRelVertex#64,inputs=0..2)]
2023-03-31 11:25:59  [ main:3546 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3547 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ReduceExpressionsRule(Project)                                                 1                  37
* Total                                                                        1                  37

2023-03-31 11:25:59  [ main:3547 ] - [ DEBUG ]  For final plan, using rel#67:LogicalSink.NONE.any.[](input=HepRelVertex#66,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3547 ] - [ DEBUG ]  For final plan, using rel#65:LogicalProject.NONE.any.[](input=HepRelVertex#64,inputs=0..2)
2023-03-31 11:25:59  [ main:3548 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3549 ] - [ DEBUG ]  optimize other predicate rewrite cost 3 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3549 ] - [ DEBUG ]  iteration: 4
2023-03-31 11:25:59  [ main:3550 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3550 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3551 ] - [ DEBUG ]  For final plan, using rel#72:LogicalSink.NONE.any.[](input=HepRelVertex#71,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3551 ] - [ DEBUG ]  For final plan, using rel#70:LogicalProject.NONE.any.[](input=HepRelVertex#69,inputs=0..2)
2023-03-31 11:25:59  [ main:3551 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3551 ] - [ DEBUG ]  optimize join predicate rewrite cost 2 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3554 ] - [ DEBUG ]  call#5: Apply rule [ReduceExpressionsRule(Project)] to [rel#75:LogicalProject.NONE.any.[](input=HepRelVertex#74,inputs=0..2)]
2023-03-31 11:25:59  [ main:3555 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3555 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ReduceExpressionsRule(Project)                                                 1                  59
* Total                                                                        1                  59

2023-03-31 11:25:59  [ main:3555 ] - [ DEBUG ]  For final plan, using rel#77:LogicalSink.NONE.any.[](input=HepRelVertex#76,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3556 ] - [ DEBUG ]  For final plan, using rel#75:LogicalProject.NONE.any.[](input=HepRelVertex#74,inputs=0..2)
2023-03-31 11:25:59  [ main:3556 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3556 ] - [ DEBUG ]  optimize other predicate rewrite cost 5 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3556 ] - [ DEBUG ]  iteration: 5
2023-03-31 11:25:59  [ main:3557 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3557 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3557 ] - [ DEBUG ]  For final plan, using rel#82:LogicalSink.NONE.any.[](input=HepRelVertex#81,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3557 ] - [ DEBUG ]  For final plan, using rel#80:LogicalProject.NONE.any.[](input=HepRelVertex#79,inputs=0..2)
2023-03-31 11:25:59  [ main:3557 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3558 ] - [ DEBUG ]  optimize join predicate rewrite cost 0 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3558 ] - [ DEBUG ]  call#6: Apply rule [ReduceExpressionsRule(Project)] to [rel#85:LogicalProject.NONE.any.[](input=HepRelVertex#84,inputs=0..2)]
2023-03-31 11:25:59  [ main:3560 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3563 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ReduceExpressionsRule(Project)                                                 1                  70
* Total                                                                        1                  70

2023-03-31 11:25:59  [ main:3564 ] - [ DEBUG ]  For final plan, using rel#87:LogicalSink.NONE.any.[](input=HepRelVertex#86,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3564 ] - [ DEBUG ]  For final plan, using rel#85:LogicalProject.NONE.any.[](input=HepRelVertex#84,inputs=0..2)
2023-03-31 11:25:59  [ main:3564 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3565 ] - [ DEBUG ]  optimize other predicate rewrite cost 6 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3566 ] - [ DEBUG ]  optimize predicate rewrite cost 30 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3567 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3567 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3567 ] - [ DEBUG ]  For final plan, using rel#92:LogicalSink.NONE.any.[](input=HepRelVertex#91,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3567 ] - [ DEBUG ]  For final plan, using rel#90:LogicalProject.NONE.any.[](input=HepRelVertex#89,inputs=0..2)
2023-03-31 11:25:59  [ main:3567 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3568 ] - [ DEBUG ]  optimize push predicate into table scan cost 1 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3569 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3570 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3570 ] - [ DEBUG ]  For final plan, using rel#97:LogicalSink.NONE.any.[](input=HepRelVertex#96,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3570 ] - [ DEBUG ]  For final plan, using rel#95:LogicalProject.NONE.any.[](input=HepRelVertex#94,inputs=0..2)
2023-03-31 11:25:59  [ main:3570 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3571 ] - [ DEBUG ]  optimize prune empty after predicate push down cost 3 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3572 ] - [ DEBUG ]  optimize predicate_pushdown cost 36 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3572 ] - [ DEBUG ]  iteration: 1
2023-03-31 11:25:59  [ main:3572 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3573 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3573 ] - [ DEBUG ]  For final plan, using rel#102:LogicalSink.NONE.any.[](input=HepRelVertex#101,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3573 ] - [ DEBUG ]  For final plan, using rel#100:LogicalProject.NONE.any.[](input=HepRelVertex#99,inputs=0..2)
2023-03-31 11:25:59  [ main:3573 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3574 ] - [ DEBUG ]  optimize simplify and push down join predicates cost 2 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3575 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3575 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:3576 ] - [ DEBUG ]  For final plan, using rel#107:LogicalSink.NONE.any.[](input=HepRelVertex#106,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3576 ] - [ DEBUG ]  For final plan, using rel#105:LogicalProject.NONE.any.[](input=HepRelVertex#104,inputs=0..2)
2023-03-31 11:25:59  [ main:3576 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3577 ] - [ DEBUG ]  optimize deal with possible null join keys cost 3 ms.
optimize result:
 LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3577 ] - [ DEBUG ]  optimize join_rewrite cost 5 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalProject(stamp=[$0], event=[$1], credit_number=[$2])
   +- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3578 ] - [ DEBUG ]  call#7: Apply rule [ProjectRemoveRule] to [rel#110:LogicalProject.NONE.any.[](input=HepRelVertex#109,inputs=0..2)]
2023-03-31 11:25:59  [ main:3580 ] - [ DEBUG ]  call#7: Rule ProjectRemoveRule arguments [rel#110:LogicalProject.NONE.any.[](input=HepRelVertex#109,inputs=0..2)] produced rel#109:HepRelVertex(rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01]))
2023-03-31 11:25:59  [ main:3583 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:3583 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
ProjectRemoveRule                                                              1               1,545
* Total                                                                        1               1,545

2023-03-31 11:25:59  [ main:3583 ] - [ DEBUG ]  For final plan, using rel#112:LogicalSink.NONE.any.[](input=HepRelVertex#109,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:3584 ] - [ DEBUG ]  For final plan, using rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
2023-03-31 11:25:59  [ main:3585 ] - [ DEBUG ]  optimize project_rewrite cost 7 ms.
optimize result: 
LogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- LogicalTableScan(table=[[batch_test, ods, test_01]])

2023-03-31 11:25:59  [ main:3762 ] - [ DEBUG ]  PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@552cede7; COST = {inf}
2023-03-31 11:25:59  [ main:3766 ] - [ DEBUG ]  Pop match: rule [FlinkLogicalTableSourceScanConverter(in:NONE,out:LOGICAL)] rels [#1]
2023-03-31 11:25:59  [ main:3767 ] - [ DEBUG ]  call#18: Apply rule [FlinkLogicalTableSourceScanConverter(in:NONE,out:LOGICAL)] to [rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])]
2023-03-31 11:25:59  [ main:3768 ] - [ DEBUG ]  Transform to: rel#119 via FlinkLogicalTableSourceScanConverter(in:NONE,out:LOGICAL)
2023-03-31 11:25:59  [ main:3889 ] - [ DEBUG ]  call#18 generated 1 successors: [rel#119:FlinkLogicalTableSourceScan.LOGICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)]
2023-03-31 11:25:59  [ main:3890 ] - [ DEBUG ]  PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@552cede7; COST = {inf}
2023-03-31 11:25:59  [ main:3890 ] - [ DEBUG ]  Pop match: rule [FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)] rels [#115]
2023-03-31 11:25:59  [ main:3890 ] - [ DEBUG ]  call#28: Apply rule [FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)] to [rel#115:LogicalSink.NONE.any.[](input=RelSubset#114,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)]
2023-03-31 11:25:59  [ main:3891 ] - [ DEBUG ]  Transform to: rel#121 via FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)
2023-03-31 11:25:59  [ main:3967 ] - [ DEBUG ]  call#28 generated 1 successors: [rel#121:FlinkLogicalSink.LOGICAL.any.[](input=RelSubset#120,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)]
2023-03-31 11:25:59  [ main:3967 ] - [ DEBUG ]  PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@552cede7; COST = {3.1489166E7 rows, 3.1489166E7 cpu, 5.66804988E8 io, 0.0 network, 0.0 memory}
2023-03-31 11:25:59  [ main:3967 ] - [ DEBUG ]  Rule Attempts Info for VolcanoPlanner
2023-03-31 11:25:59  [ main:3969 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
FlinkLogicalTableSourceScanConverter(in:NONE,out:LOGICAL)                      1             122,378
FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)                                 1              76,976
* Total                                                                        2             199,354

2023-03-31 11:25:59  [ main:4007 ] - [ DEBUG ]  Cheapest plan:
FlinkLogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]): rowcount = 1.5744583E7, cumulative cost = {3.1489166E7 rows, 3.1489166E7 cpu, 5.66804988E8 io, 0.0 network, 0.0 memory}, id = 122
  FlinkLogicalTableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number]): rowcount = 1.5744583E7, cumulative cost = {1.5744583E7 rows, 1.5744583E7 cpu, 5.66804988E8 io, 0.0 network, 0.0 memory}, id = 119

2023-03-31 11:25:59  [ main:4010 ] - [ DEBUG ]  Provenance:
rel#122:FlinkLogicalSink.LOGICAL.any.[](input=FlinkLogicalTableSourceScan#119,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
  direct
    rel#121:FlinkLogicalSink.LOGICAL.any.[](input=RelSubset#120,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
      call#28 rule [FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)]
        rel#115:LogicalSink.NONE.any.[](input=RelSubset#114,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
          no parent
rel#119:FlinkLogicalTableSourceScan.LOGICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)
  call#18 rule [FlinkLogicalTableSourceScanConverter(in:NONE,out:LOGICAL)]
    rel#1:LogicalTableScan.NONE.any.[](table=[batch_test, ods, test_01])
      no parent

2023-03-31 11:25:59  [ main:4013 ] - [ DEBUG ]  optimize logical cost 424 ms.
optimize result: 
FlinkLogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- FlinkLogicalTableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number])

2023-03-31 11:25:59  [ main:4015 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:4015 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:4015 ] - [ DEBUG ]  For final plan, using rel#124:FlinkLogicalSink.LOGICAL.any.[](input=HepRelVertex#123,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:4015 ] - [ DEBUG ]  For final plan, using rel#119:FlinkLogicalTableSourceScan.LOGICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:4016 ] - [ DEBUG ]  optimize logical_rewrite cost 2 ms.
optimize result: 
FlinkLogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- FlinkLogicalTableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number])

2023-03-31 11:25:59  [ main:4025 ] - [ DEBUG ]  optimize time_indicator cost 9 ms.
optimize result: 
FlinkLogicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- FlinkLogicalTableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number])

2023-03-31 11:25:59  [ main:4033 ] - [ DEBUG ]  PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@552cede7; COST = {inf}
2023-03-31 11:25:59  [ main:4033 ] - [ DEBUG ]  Pop match: rule [BatchPhysicalTableSourceScanRule(in:LOGICAL,out:BATCH_PHYSICAL)] rels [#119]
2023-03-31 11:25:59  [ main:4033 ] - [ DEBUG ]  call#47: Apply rule [BatchPhysicalTableSourceScanRule(in:LOGICAL,out:BATCH_PHYSICAL)] to [rel#119:FlinkLogicalTableSourceScan.LOGICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)]
2023-03-31 11:25:59  [ main:4035 ] - [ DEBUG ]  Transform to: rel#132 via BatchPhysicalTableSourceScanRule(in:LOGICAL,out:BATCH_PHYSICAL)
2023-03-31 11:25:59  [ main:4114 ] - [ DEBUG ]  call#47 generated 1 successors: [rel#132:BatchPhysicalTableSourceScan.BATCH_PHYSICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)]
2023-03-31 11:25:59  [ main:4114 ] - [ DEBUG ]  PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@552cede7; COST = {inf}
2023-03-31 11:25:59  [ main:4114 ] - [ DEBUG ]  Pop match: rule [BatchPhysicalSinkRule(in:LOGICAL,out:BATCH_PHYSICAL)] rels [#128]
2023-03-31 11:25:59  [ main:4114 ] - [ DEBUG ]  call#72: Apply rule [BatchPhysicalSinkRule(in:LOGICAL,out:BATCH_PHYSICAL)] to [rel#128:FlinkLogicalSink.LOGICAL.any.[](input=RelSubset#127,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)]
2023-03-31 11:25:59  [ main:4115 ] - [ DEBUG ]  Transform to: rel#134 via BatchPhysicalSinkRule(in:LOGICAL,out:BATCH_PHYSICAL)
2023-03-31 11:25:59  [ main:4222 ] - [ DEBUG ]  call#72 generated 1 successors: [rel#134:BatchPhysicalSink.BATCH_PHYSICAL.any.[](input=RelSubset#133,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)]
2023-03-31 11:25:59  [ main:4223 ] - [ DEBUG ]  PLANNER = org.apache.calcite.plan.volcano.IterativeRuleDriver@552cede7; COST = {3.1489166E7 rows, 1.5744583E7 cpu, 5.66804988E8 io, 0.0 network, 0.0 memory}
2023-03-31 11:25:59  [ main:4223 ] - [ DEBUG ]  Rule Attempts Info for VolcanoPlanner
2023-03-31 11:25:59  [ main:4224 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
FlinkLogicalTableSourceScanConverter(in:NONE,out:LOGICAL)                      1             122,378
BatchPhysicalSinkRule(in:LOGICAL,out:BATCH_PHYSICAL)                           1             108,395
BatchPhysicalTableSourceScanRule(in:LOGICAL,out:BATCH_PHYSICAL)                   1              80,681
FlinkLogicalSinkConverter(in:NONE,out:LOGICAL)                                 1              76,976
* Total                                                                        4             388,430

2023-03-31 11:25:59  [ main:4266 ] - [ DEBUG ]  Cheapest plan:
BatchPhysicalSink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]): rowcount = 1.5744583E7, cumulative cost = {3.1489166E7 rows, 1.5744583E7 cpu, 5.66804988E8 io, 0.0 network, 0.0 memory}, id = 135
  BatchPhysicalTableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number]): rowcount = 1.5744583E7, cumulative cost = {1.5744583E7 rows, 0.0 cpu, 5.66804988E8 io, 0.0 network, 0.0 memory}, id = 132

2023-03-31 11:25:59  [ main:4266 ] - [ DEBUG ]  Provenance:
rel#135:BatchPhysicalSink.BATCH_PHYSICAL.any.[](input=BatchPhysicalTableSourceScan#132,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
  direct
    rel#134:BatchPhysicalSink.BATCH_PHYSICAL.any.[](input=RelSubset#133,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
      call#72 rule [BatchPhysicalSinkRule(in:LOGICAL,out:BATCH_PHYSICAL)]
        rel#128:FlinkLogicalSink.LOGICAL.any.[](input=RelSubset#127,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
          no parent
rel#132:BatchPhysicalTableSourceScan.BATCH_PHYSICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)
  call#47 rule [BatchPhysicalTableSourceScanRule(in:LOGICAL,out:BATCH_PHYSICAL)]
    rel#119:FlinkLogicalTableSourceScan.LOGICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)
      no parent

2023-03-31 11:25:59  [ main:4267 ] - [ DEBUG ]  optimize physical cost 241 ms.
optimize result: 
Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- TableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number])

2023-03-31 11:25:59  [ main:4268 ] - [ DEBUG ]  Rule Attempts Info for HepPlanner
2023-03-31 11:25:59  [ main:4269 ] - [ DEBUG ]  
Rules                                                                   Attempts           Time (us)
* Total                                                                        0                   0

2023-03-31 11:25:59  [ main:4269 ] - [ DEBUG ]  For final plan, using rel#137:BatchPhysicalSink.BATCH_PHYSICAL.any.[](input=HepRelVertex#136,table=default_catalog.default_database.print_table,fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:4269 ] - [ DEBUG ]  For final plan, using rel#132:BatchPhysicalTableSourceScan.BATCH_PHYSICAL.any.[](table=[batch_test, ods, test_01],fields=stamp, event, credit_number)
2023-03-31 11:25:59  [ main:4270 ] - [ DEBUG ]  optimize physical_rewrite cost 2 ms.
optimize result: 
Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])
+- TableSourceScan(table=[[batch_test, ods, test_01]], fields=[stamp, event, credit_number])

2023-03-31 11:26:00  [ main:4386 ] - [ WARN ]  HiveConf of name hive.vectorized.use.checked.expressions does not exist
2023-03-31 11:26:00  [ main:4387 ] - [ WARN ]  HiveConf of name hive.strict.checks.no.partition.filter does not exist
2023-03-31 11:26:00  [ main:4388 ] - [ WARN ]  HiveConf of name hive.strict.checks.orderby.no.limit does not exist
2023-03-31 11:26:00  [ main:4388 ] - [ WARN ]  HiveConf of name hive.vectorized.adaptor.usage.mode does not exist
2023-03-31 11:26:00  [ main:4388 ] - [ WARN ]  HiveConf of name hive.vectorized.input.format.excludes does not exist
2023-03-31 11:26:00  [ main:4388 ] - [ WARN ]  HiveConf of name hive.strict.checks.bucketing does not exist
2023-03-31 11:26:00  [ main:4402 ] - [ INFO ]  Trying to connect to metastore with URI thrift://t-d-datastorage-srv03:9083
2023-03-31 11:26:00  [ main:4425 ] - [ INFO ]  Opened a connection to metastore, current connections: 2
2023-03-31 11:26:00  [ main:4442 ] - [ INFO ]  Connected to metastore.
2023-03-31 11:26:00  [ main:4633 ] - [ DEBUG ]  DDL: struct test_01 { string stamp, string event, string credit_number}
2023-03-31 11:26:00  [ main:4657 ] - [ INFO ]  Closed a connection to metastore, current connections: 1
2023-03-31 11:26:00  [ main:4665 ] - [ DEBUG ]  Looking for FS supporting hdfs
2023-03-31 11:26:00  [ main:4666 ] - [ DEBUG ]  looking for configuration option fs.hdfs.impl
2023-03-31 11:26:00  [ main:4666 ] - [ DEBUG ]  Looking in service filesystems for implementation class
2023-03-31 11:26:00  [ main:4667 ] - [ DEBUG ]  FS for hdfs is class org.apache.hadoop.hdfs.DistributedFileSystem
2023-03-31 11:26:00  [ main:4685 ] - [ DEBUG ]  dfs.client.use.legacy.blockreader.local = false
2023-03-31 11:26:00  [ main:4685 ] - [ DEBUG ]  dfs.client.read.shortcircuit = false
2023-03-31 11:26:00  [ main:4685 ] - [ DEBUG ]  dfs.client.domain.socket.data.traffic = false
2023-03-31 11:26:00  [ main:4685 ] - [ DEBUG ]  dfs.domain.socket.path = /var/run/hdfs-sockets/dn
2023-03-31 11:26:00  [ main:4695 ] - [ DEBUG ]  Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
2023-03-31 11:26:00  [ main:4999 ] - [ DEBUG ]  No HA service delegation token found for logical URI hdfs://Tdsop/user/hive/warehouse/ods.db/test_01
2023-03-31 11:26:00  [ main:5000 ] - [ DEBUG ]  dfs.client.use.legacy.blockreader.local = false
2023-03-31 11:26:00  [ main:5000 ] - [ DEBUG ]  dfs.client.read.shortcircuit = false
2023-03-31 11:26:00  [ main:5001 ] - [ DEBUG ]  dfs.client.domain.socket.data.traffic = false
2023-03-31 11:26:00  [ main:5001 ] - [ DEBUG ]  dfs.domain.socket.path = /var/run/hdfs-sockets/dn
2023-03-31 11:26:00  [ main:5011 ] - [ DEBUG ]  multipleLinearRandomRetry = null
2023-03-31 11:26:00  [ main:5044 ] - [ DEBUG ]  rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@11df2829
2023-03-31 11:26:00  [ main:5051 ] - [ DEBUG ]  getting client out of cache: org.apache.hadoop.ipc.Client@5c313224
2023-03-31 11:26:01  [ main:5403 ] - [ DEBUG ]  Trying to load the custom-built native-hadoop library...
2023-03-31 11:26:01  [ main:5415 ] - [ DEBUG ]  Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
2023-03-31 11:26:01  [ main:5415 ] - [ DEBUG ]  java.library.path=/Users/renzhuo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
2023-03-31 11:26:01  [ main:5416 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-03-31 11:26:01  [ main:5426 ] - [ DEBUG ]  Both short-circuit local reads and UNIX domain socket are disabled.
2023-03-31 11:26:01  [ main:5431 ] - [ DEBUG ]  DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
2023-03-31 11:26:01  [ main:5455 ] - [ DEBUG ]  The ping interval is 60000 ms.
2023-03-31 11:26:01  [ main:5457 ] - [ DEBUG ]  Connecting to t-d-datastorage-srv01/172.22.17.20:8020
2023-03-31 11:26:01  [ IPC Client (294309744) connection to t-d-datastorage-srv01/172.22.17.20:8020 from renzhuo:5489 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv01/172.22.17.20:8020 from renzhuo: starting, having connections 1
2023-03-31 11:26:01  [ IPC Parameter Sending Thread #0:5501 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv01/172.22.17.20:8020 from renzhuo sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
2023-03-31 11:26:01  [ IPC Client (294309744) connection to t-d-datastorage-srv01/172.22.17.20:8020 from renzhuo:5526 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv01/172.22.17.20:8020 from renzhuo got value #0
2023-03-31 11:26:01  [ main:5554 ] - [ DEBUG ]  org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby. Visit https://s.apache.org/sbnn-error
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:88)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1962)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1421)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3055)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1151)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:940)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:870)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:816)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2680)
, while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over t-d-datastorage-srv01/172.22.17.20:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby. Visit https://s.apache.org/sbnn-error
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:88)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1962)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1421)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3055)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1151)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:940)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:870)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:816)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2680)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1437)
	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy175.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:874)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy176.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1697)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1491)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1488)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1503)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1668)
	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.getNumFiles(HiveSourceFileEnumerator.java:118)
	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$0(HiveTableSource.java:146)
	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:89)
	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:144)
	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:114)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:106)
	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:49)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:250)
	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:58)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:82)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:81)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:185)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1665)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:752)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:872)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:742)
	at org.example.Main.main(Main.java:16)
2023-03-31 11:26:01  [ main:5560 ] - [ DEBUG ]  multipleLinearRandomRetry = null
2023-03-31 11:26:01  [ main:5562 ] - [ DEBUG ]  getting client out of cache: org.apache.hadoop.ipc.Client@5c313224
2023-03-31 11:26:01  [ main:5563 ] - [ DEBUG ]  The ping interval is 60000 ms.
2023-03-31 11:26:01  [ main:5564 ] - [ DEBUG ]  Connecting to t-d-datastorage-srv02/172.22.17.21:8020
2023-03-31 11:26:01  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:5579 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo: starting, having connections 2
2023-03-31 11:26:01  [ IPC Parameter Sending Thread #0:5580 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
2023-03-31 11:26:01  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:5811 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #0
2023-03-31 11:26:01  [ main:5812 ] - [ DEBUG ]  Call: getFileInfo took 249ms
2023-03-31 11:26:01  [ IPC Parameter Sending Thread #0:5842 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
2023-03-31 11:26:01  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:5847 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #1
2023-03-31 11:26:01  [ main:5848 ] - [ DEBUG ]  Call: getListing took 7ms
2023-03-31 11:26:01  [ main:5858 ] - [ INFO ]  Hive source(ods.test_01}) getNumFiles use time: 1197 ms, result: 36
2023-03-31 11:26:01  [ IPC Parameter Sending Thread #0:5862 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
2023-03-31 11:26:01  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:5872 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #2
2023-03-31 11:26:01  [ main:5873 ] - [ DEBUG ]  Call: getFileInfo took 11ms
2023-03-31 11:26:01  [ IPC Parameter Sending Thread #0:5915 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
2023-03-31 11:26:01  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:5956 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #3
2023-03-31 11:26:01  [ main:5957 ] - [ DEBUG ]  Call: getFileInfo took 42ms
2023-03-31 11:26:01  [ IPC Parameter Sending Thread #0:5959 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #4 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
2023-03-31 11:26:01  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:6000 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #4
2023-03-31 11:26:01  [ main:6000 ] - [ DEBUG ]  Call: getListing took 41ms
2023-03-31 11:26:01  [ main:6039 ] - [ DEBUG ]  Time taken to get FileStatuses: 147
2023-03-31 11:26:01  [ main:6039 ] - [ INFO ]  Total input files to process : 36
2023-03-31 11:26:01  [ main:6048 ] - [ DEBUG ]  Total # of splits generated by getSplits: 36, TimeTaken: 158
2023-03-31 11:26:01  [ main:6055 ] - [ INFO ]  Hive source(ods.test_01}) createInputSplits use time: 195 ms, result: 36
2023-03-31 11:26:01  [ main:6138 ] - [ DEBUG ]  Dig to clean the org.apache.flink.table.runtime.connector.sink.DataStructureConverterWrapper
2023-03-31 11:26:01  [ main:6138 ] - [ DEBUG ]  Dig to clean the org.apache.flink.table.data.conversion.RowRowConverter
2023-03-31 11:26:01  [ main:6138 ] - [ DEBUG ]  Dig to clean the [Lorg.apache.flink.table.data.conversion.DataStructureConverter;
2023-03-31 11:26:01  [ main:6142 ] - [ DEBUG ]  Dig to clean the [Lorg.apache.flink.table.data.RowData$FieldGetter;
2023-03-31 11:26:01  [ main:6145 ] - [ DEBUG ]  Dig to clean the java.util.LinkedHashMap
2023-03-31 11:26:01  [ main:6146 ] - [ DEBUG ]  Dig to clean the java.lang.Boolean
2023-03-31 11:26:01  [ main:6154 ] - [ DEBUG ]  Dig to clean the org.apache.flink.api.common.functions.util.PrintSinkOutputWriter
2023-03-31 11:26:01  [ main:6156 ] - [ DEBUG ]  Dig to clean the java.lang.Boolean
2023-03-31 11:26:01  [ main:6156 ] - [ DEBUG ]  Dig to clean the java.lang.String
2023-03-31 11:26:01  [ main:6156 ] - [ DEBUG ]  Dig to clean the [C
2023-03-31 11:26:01  [ main:6156 ] - [ DEBUG ]  Dig to clean the java.lang.Integer
2023-03-31 11:26:01  [ main:6178 ] - [ DEBUG ]  Using BATCH execution state backend and timer service.
2023-03-31 11:26:01  [ main:6184 ] - [ DEBUG ]  Transforming LegacySinkTransformation{id=2, name='Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])', outputType=ROW<`stamp` STRING, `event` STRING, `credit_number` STRING>(org.apache.flink.table.data.RowData, org.apache.flink.table.runtime.typeutils.RowDataSerializer), parallelism=36}
2023-03-31 11:26:01  [ main:6185 ] - [ DEBUG ]  Transforming SourceTransformation{id=1, name='HiveSource-ods.test_01', outputType=ROW<`stamp` STRING, `event` STRING, `credit_number` STRING>(org.apache.flink.table.data.RowData, org.apache.flink.table.runtime.typeutils.RowDataSerializer), parallelism=36}
2023-03-31 11:26:01  [ main:6190 ] - [ DEBUG ]  Vertex: 1
2023-03-31 11:26:01  [ main:6198 ] - [ DEBUG ]  Vertex: 2
2023-03-31 11:26:01  [ main:6233 ] - [ DEBUG ]  Generated hash 'cbc357ccb763df2852fee8c4fc7d55f2' for node 'Source: HiveSource-ods.test_01-1' {id: 1, parallelism: 36, user function: }
2023-03-31 11:26:01  [ main:6234 ] - [ DEBUG ]  Generated hash '7df19f87deec5680128845fd9a6ca18d' for node 'Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])-2' {id: 2, parallelism: 36, user function: org.apache.flink.connector.print.table.PrintTableSinkFactory$RowDataPrintFunction}
2023-03-31 11:26:02  [ main:6443 ] - [ DEBUG ]  Parallelism set: 36 for 1
2023-03-31 11:26:02  [ main:6589 ] - [ INFO ]  The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
2023-03-31 11:26:02  [ main:6590 ] - [ INFO ]  The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
2023-03-31 11:26:02  [ main:6590 ] - [ INFO ]  The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
2023-03-31 11:26:02  [ main:6590 ] - [ INFO ]  The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
2023-03-31 11:26:02  [ main:6590 ] - [ INFO ]  The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
2023-03-31 11:26:02  [ main:6590 ] - [ INFO ]  The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
2023-03-31 11:26:02  [ main:6594 ] - [ INFO ]  Starting Flink Mini Cluster
2023-03-31 11:26:02  [ main:6595 ] - [ DEBUG ]  Using configuration MiniClusterConfiguration {singleRpcService=SHARED, numTaskManagers=1, commonBindAddress='null', config={taskmanager.memory.network.min=64 mb, taskmanager.cpu.cores=1000000.0, taskmanager.memory.task.off-heap.size=1099511627776 bytes, taskmanager.memory.jvm-metaspace.size=256 mb, execution.target=local, cluster.io-pool.size=4, execution.runtime-mode=BATCH, table.planner=BLINK, taskmanager.memory.jvm-overhead.min=1 gb, rest.bind-port=0, taskmanager.memory.network.max=64 mb, taskmanager.memory.framework.off-heap.size=128 mb, execution.attached=true, taskmanager.memory.managed.size=128 mb, taskmanager.memory.framework.heap.size=128 mb, parallelism.default=8, taskmanager.numberOfTaskSlots=36, taskmanager.memory.task.heap.size=1099511627776 bytes, rest.address=localhost, taskmanager.memory.jvm-overhead.max=1 gb, akka.ask.timeout=PT5M}}
2023-03-31 11:26:02  [ main:6873 ] - [ INFO ]  Starting Metrics Registry
2023-03-31 11:26:02  [ main:6910 ] - [ INFO ]  No metrics reporter configured, no metrics will be exposed/reported.
2023-03-31 11:26:02  [ main:6910 ] - [ INFO ]  Starting RPC Service(s)
2023-03-31 11:26:02  [ main:6920 ] - [ INFO ]  Trying to start local actor system
2023-03-31 11:26:02  [ main:6924 ] - [ DEBUG ]  Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"allow-java-serialization":"on","default-dispatcher":{"executor":"fork-join-executor","fork-join-executor":{"parallelism-factor":1,"parallelism-max":4,"parallelism-min":2},"throughput":15},"guardian-supervisor-strategy":"org.apache.flink.runtime.rpc.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"50s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
2023-03-31 11:26:03  [ flink-akka.actor.default-dispatcher-5:7589 ] - [ INFO ]  Slf4jLogger started
2023-03-31 11:26:03  [ flink-akka.actor.default-dispatcher-5:7647 ] - [ DEBUG ]  logger log1-Slf4jLogger started
2023-03-31 11:26:03  [ flink-akka.actor.default-dispatcher-5:7650 ] - [ DEBUG ]  Default Loggers started
2023-03-31 11:26:03  [ main:7779 ] - [ INFO ]  Actor system started at akka://flink
2023-03-31 11:26:03  [ main:7819 ] - [ INFO ]  Trying to start local actor system
2023-03-31 11:26:03  [ main:7822 ] - [ DEBUG ]  Using akka configuration
 Config(SimpleConfigObject({"akka":{"actor":{"allow-java-serialization":"on","default-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"thread-priority":1,"throughput":15,"type":"org.apache.flink.runtime.rpc.akka.PriorityThreadsDispatcher"},"guardian-supervisor-strategy":"org.apache.flink.runtime.rpc.akka.EscalatingSupervisorStrategy","supervisor-dispatcher":{"executor":"thread-pool-executor","thread-pool-executor":{"core-pool-size-max":1,"core-pool-size-min":1},"type":"Dispatcher"},"warn-about-java-serializer-usage":"off"},"daemonic":"off","jvm-exit-on-fatal-error":"on","log-config-on-start":"off","log-dead-letters":"off","log-dead-letters-during-shutdown":"off","logger-startup-timeout":"50s","loggers":["akka.event.slf4j.Slf4jLogger"],"logging-filter":"akka.event.slf4j.Slf4jLoggingFilter","loglevel":"DEBUG","serialize-messages":"off","stdout-loglevel":"OFF"}}))
2023-03-31 11:26:03  [ flink-metrics-8:7856 ] - [ INFO ]  Slf4jLogger started
2023-03-31 11:26:03  [ flink-metrics-8:7861 ] - [ DEBUG ]  logger log1-Slf4jLogger started
2023-03-31 11:26:03  [ flink-metrics-8:7862 ] - [ DEBUG ]  Default Loggers started
2023-03-31 11:26:03  [ main:7872 ] - [ INFO ]  Actor system started at akka://flink-metrics
2023-03-31 11:26:03  [ flink-metrics-akka.actor.supervisor-dispatcher-9:7899 ] - [ DEBUG ]  Starting AkkaRpcActor with name MetricQueryService.
2023-03-31 11:26:03  [ main:7904 ] - [ INFO ]  Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
2023-03-31 11:26:03  [ main:8028 ] - [ INFO ]  Starting high-availability services
2023-03-31 11:26:03  [ main:8040 ] - [ INFO ]  Created BLOB server storage directory /var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/blobStore-d6a459b6-39c6-426c-a6fc-a1440acb1d46
2023-03-31 11:26:03  [ main:8046 ] - [ DEBUG ]  Trying to open socket on port 0
2023-03-31 11:26:03  [ main:8047 ] - [ INFO ]  Started BLOB server at 0.0.0.0:64726 - max concurrent requests: 50 - max backlog: 1000
2023-03-31 11:26:03  [ main:8056 ] - [ INFO ]  Created BLOB cache storage directory /var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/blobStore-699498b8-8792-4134-b227-367860c6df13
2023-03-31 11:26:03  [ main:8058 ] - [ INFO ]  Created BLOB cache storage directory /var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/blobStore-b913d0cf-b029-410d-9469-d6bdc17ec865
2023-03-31 11:26:03  [ main:8059 ] - [ INFO ]  Starting 1 TaskManager(s)
2023-03-31 11:26:03  [ main:8064 ] - [ INFO ]  Starting TaskManager with ResourceID: 66527030-9c98-43a4-b1f5-6190c00b39ae
2023-03-31 11:26:03  [ main:8077 ] - [ INFO ]  Temporary file directory '/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T': total 460 GB, usable 349 GB (75.87% usable)
2023-03-31 11:26:03  [ main:8081 ] - [ DEBUG ]  FileChannelManager uses directory /var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/flink-io-781524aa-3237-45ec-894c-0d6f2c700e32 for spill files.
2023-03-31 11:26:03  [ main:8086 ] - [ INFO ]  Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
	/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/flink-io-781524aa-3237-45ec-894c-0d6f2c700e32
2023-03-31 11:26:03  [ main:8102 ] - [ DEBUG ]  FileChannelManager uses directory /var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/flink-netty-shuffle-f8c39543-2025-440d-bb7e-e48ce3832b09 for spill files.
2023-03-31 11:26:03  [ main:8106 ] - [ INFO ]  Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
	/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/flink-netty-shuffle-f8c39543-2025-440d-bb7e-e48ce3832b09
2023-03-31 11:26:03  [ main:8153 ] - [ INFO ]  Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
2023-03-31 11:26:03  [ main:8179 ] - [ INFO ]  Starting the network environment and its components.
2023-03-31 11:26:03  [ main:8179 ] - [ DEBUG ]  Starting network connection manager
2023-03-31 11:26:03  [ main:8182 ] - [ INFO ]  Starting the kvState service and its components.
2023-03-31 11:26:03  [ main:8195 ] - [ DEBUG ]  Messages have a max timeout of 300000 ms
2023-03-31 11:26:03  [ main:8196 ] - [ INFO ]  Config uses fallback configuration key 'akka.ask.timeout' instead of key 'taskmanager.slot.timeout'
2023-03-31 11:26:03  [ flink-akka.actor.supervisor-dispatcher-6:8206 ] - [ DEBUG ]  Starting AkkaRpcActor with name taskmanager_0.
2023-03-31 11:26:03  [ main:8209 ] - [ INFO ]  Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
2023-03-31 11:26:03  [ flink-akka.actor.default-dispatcher-5:8232 ] - [ INFO ]  Start job leader service.
2023-03-31 11:26:03  [ flink-akka.actor.default-dispatcher-5:8235 ] - [ INFO ]  User file cache uses directory /var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/flink-dist-cache-3e67afd9-4929-4121-b541-34bad06b44fe
2023-03-31 11:26:03  [ main:8286 ] - [ DEBUG ]  Starting Dispatcher REST endpoint.
2023-03-31 11:26:03  [ main:8289 ] - [ INFO ]  Starting rest endpoint.
2023-03-31 11:26:04  [ main:8436 ] - [ DEBUG ]  Using SLF4J as the default logging framework
2023-03-31 11:26:04  [ main:8437 ] - [ DEBUG ]  -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
2023-03-31 11:26:04  [ main:8437 ] - [ DEBUG ]  -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
2023-03-31 11:26:04  [ main:8502 ] - [ WARN ]  Log file environment variable 'log.file' is not set.
2023-03-31 11:26:04  [ main:8502 ] - [ WARN ]  JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
2023-03-31 11:26:04  [ main:8537 ] - [ DEBUG ]  -Dio.netty.noUnsafe: false
2023-03-31 11:26:04  [ main:8538 ] - [ DEBUG ]  Java version: 8
2023-03-31 11:26:04  [ main:8540 ] - [ DEBUG ]  sun.misc.Unsafe.theUnsafe: available
2023-03-31 11:26:04  [ main:8540 ] - [ DEBUG ]  sun.misc.Unsafe.copyMemory: available
2023-03-31 11:26:04  [ main:8540 ] - [ DEBUG ]  java.nio.Buffer.address: available
2023-03-31 11:26:04  [ main:8541 ] - [ DEBUG ]  direct buffer constructor: available
2023-03-31 11:26:04  [ main:8542 ] - [ DEBUG ]  java.nio.Bits.unaligned: available, true
2023-03-31 11:26:04  [ main:8542 ] - [ DEBUG ]  jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
2023-03-31 11:26:04  [ main:8542 ] - [ DEBUG ]  java.nio.DirectByteBuffer.<init>(long, int): available
2023-03-31 11:26:04  [ main:8542 ] - [ DEBUG ]  sun.misc.Unsafe: available
2023-03-31 11:26:04  [ main:8542 ] - [ DEBUG ]  -Dio.netty.tmpdir: /var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T (java.io.tmpdir)
2023-03-31 11:26:04  [ main:8542 ] - [ DEBUG ]  -Dio.netty.bitMode: 64 (sun.arch.data.model)
2023-03-31 11:26:04  [ main:8544 ] - [ DEBUG ]  Platform: MacOS
2023-03-31 11:26:04  [ main:8545 ] - [ DEBUG ]  -Dio.netty.maxDirectMemory: 3817865216 bytes
2023-03-31 11:26:04  [ main:8545 ] - [ DEBUG ]  -Dio.netty.uninitializedArrayAllocationThreshold: -1
2023-03-31 11:26:04  [ main:8546 ] - [ DEBUG ]  java.nio.ByteBuffer.cleaner(): available
2023-03-31 11:26:04  [ main:8547 ] - [ DEBUG ]  -Dio.netty.noPreferDirect: false
2023-03-31 11:26:04  [ main:8551 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@24731caf under DELETE@/v1/cluster.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.ShutdownHandler@24731caf under DELETE@/cluster.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@62f6185a under GET@/v1/config.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.DashboardConfigHandler@62f6185a under GET@/config.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@34b462e0 under GET@/v1/datasets.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetListHandler@34b462e0 under GET@/datasets.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@446f3a53 under GET@/v1/datasets/delete/:triggerid.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteStatusHandler@446f3a53 under GET@/datasets/delete/:triggerid.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@4c3fcbe7 under DELETE@/v1/datasets/:datasetid.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.dataset.ClusterDataSetDeleteHandlers$ClusterDataSetDeleteTriggerHandler@4c3fcbe7 under DELETE@/datasets/:datasetid.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@1e592ef2 under GET@/v1/jars.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.webmonitor.handlers.JarListHandler@1e592ef2 under GET@/jars.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@96dfcbb under POST@/v1/jars/upload.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler@96dfcbb under POST@/jars/upload.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@34ede267 under DELETE@/v1/jars/:jarid.
2023-03-31 11:26:04  [ main:8552 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.webmonitor.handlers.JarDeleteHandler@34ede267 under DELETE@/jars/:jarid.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6bf77ee under GET@/v1/jars/:jarid/plan.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@6bf77ee under GET@/jars/:jarid/plan.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@51c6e775 under POST@/v1/jars/:jarid/plan.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler@51c6e775 under POST@/jars/:jarid/plan.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@372841d2 under POST@/v1/jars/:jarid/run.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.webmonitor.handlers.JarRunHandler@372841d2 under POST@/jars/:jarid/run.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@6c8d638a under GET@/v1/jobmanager/config.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterConfigHandler@6c8d638a under GET@/jobmanager/config.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@117069f2 under GET@/v1/jobmanager/log.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@117069f2 under GET@/jobmanager/log.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@77ebc9e6 under GET@/v1/jobmanager/logs.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogListHandler@77ebc9e6 under GET@/jobmanager/logs.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@2b82018 under GET@/v1/jobmanager/logs/:filename.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerCustomLogHandler@2b82018 under GET@/jobmanager/logs/:filename.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@52e92f6 under GET@/v1/jobmanager/metrics.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobManagerMetricsHandler@52e92f6 under GET@/jobmanager/metrics.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@17034458 under GET@/v1/jobmanager/stdout.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.JobManagerLogFileHandler@17034458 under GET@/jobmanager/stdout.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@3e0e0ba7 under GET@/v1/jobs.
2023-03-31 11:26:04  [ main:8553 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobIdsHandler@3e0e0ba7 under GET@/jobs.
2023-03-31 11:26:04  [ main:8554 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@7df5549e under POST@/v1/jobs.
2023-03-31 11:26:04  [ main:8554 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobSubmitHandler@7df5549e under POST@/jobs.
2023-03-31 11:26:04  [ main:8554 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@cbdc0f4 under GET@/v1/jobs/metrics.
2023-03-31 11:26:04  [ main:8554 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingJobsMetricsHandler@cbdc0f4 under GET@/jobs/metrics.
2023-03-31 11:26:04  [ main:8554 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@11174bf under GET@/v1/jobs/overview.
2023-03-31 11:26:04  [ main:8554 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobsOverviewHandler@11174bf under GET@/jobs/overview.
2023-03-31 11:26:04  [ main:8554 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@4f0c1409 under GET@/v1/jobs/:jobid.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobDetailsHandler@4f0c1409 under GET@/jobs/:jobid.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@188ae8d2 under PATCH@/v1/jobs/:jobid.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@188ae8d2 under PATCH@/jobs/:jobid.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@7a522157 under GET@/v1/jobs/:jobid/accumulators.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobAccumulatorsHandler@7a522157 under GET@/jobs/:jobid/accumulators.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@706c062e under GET@/v1/jobs/:jobid/checkpoints.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler@706c062e under GET@/jobs/:jobid/checkpoints.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@5feff876 under GET@/v1/jobs/:jobid/checkpoints/config.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointConfigHandler@5feff876 under GET@/jobs/:jobid/checkpoints/config.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@10ec4721 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointStatisticDetailsHandler@10ec4721 under GET@/jobs/:jobid/checkpoints/details/:checkpointid.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@7bdf94f2 under GET@/v1/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.checkpoints.TaskCheckpointStatisticDetailsHandler@7bdf94f2 under GET@/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@6b92a0d1 under GET@/v1/jobs/:jobid/config.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobConfigHandler@6b92a0d1 under GET@/jobs/:jobid/config.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@4b9ed99d under POST@/v1/jobs/:jobid/coordinators/:operatorid.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler@4b9ed99d under POST@/jobs/:jobid/coordinators/:operatorid.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@39b95a80 under GET@/v1/jobs/:jobid/exceptions.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler@39b95a80 under GET@/jobs/:jobid/exceptions.
2023-03-31 11:26:04  [ main:8555 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@3ec62141 under GET@/v1/jobs/:jobid/execution-result.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler@3ec62141 under GET@/jobs/:jobid/execution-result.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@402c9a60 under GET@/v1/jobs/:jobid/metrics.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobMetricsHandler@402c9a60 under GET@/jobs/:jobid/metrics.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@7d43f1c9 under GET@/v1/jobs/:jobid/plan.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobPlanHandler@7d43f1c9 under GET@/jobs/:jobid/plan.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@eddc9bb under PATCH@/v1/jobs/:jobid/rescaling.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingTriggerHandler@eddc9bb under PATCH@/jobs/:jobid/rescaling.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@11e3d08 under GET@/v1/jobs/:jobid/rescaling/:triggerid.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.rescaling.RescalingHandlers$RescalingStatusHandler@11e3d08 under GET@/jobs/:jobid/rescaling/:triggerid.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@25c548d1 under POST@/v1/jobs/:jobid/savepoints.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointTriggerHandler@25c548d1 under POST@/jobs/:jobid/savepoints.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@476a2819 under GET@/v1/jobs/:jobid/savepoints/:triggerid.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler@476a2819 under GET@/jobs/:jobid/savepoints/:triggerid.
2023-03-31 11:26:04  [ main:8556 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@279ab15e under POST@/v1/jobs/:jobid/stop.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$StopWithSavepointHandler@279ab15e under POST@/jobs/:jobid/stop.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@571c2ed8 under GET@/v1/jobs/:jobid/vertices/:vertexid.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobVertexDetailsHandler@571c2ed8 under GET@/jobs/:jobid/vertices/:vertexid.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@5810772a under GET@/v1/jobs/:jobid/vertices/:vertexid/accumulators.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobVertexAccumulatorsHandler@5810772a under GET@/jobs/:jobid/vertices/:vertexid/accumulators.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@4eacb6d1 under GET@/v1/jobs/:jobid/vertices/:vertexid/backpressure.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler@4eacb6d1 under GET@/jobs/:jobid/vertices/:vertexid/backpressure.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@3ccefe1b under GET@/v1/jobs/:jobid/vertices/:vertexid/flamegraph.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobVertexFlameGraphHandler$DisabledJobVertexFlameGraphHandler@3ccefe1b under GET@/jobs/:jobid/vertices/:vertexid/flamegraph.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@a926db4 under GET@/v1/jobs/:jobid/vertices/:vertexid/metrics.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexMetricsHandler@a926db4 under GET@/jobs/:jobid/vertices/:vertexid/metrics.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@51e0f2eb under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.SubtasksAllAccumulatorsHandler@51e0f2eb under GET@/jobs/:jobid/vertices/:vertexid/subtasks/accumulators.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@7a51dc38 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingSubtasksMetricsHandler@7a51dc38 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/metrics.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@31a4db4f under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.SubtaskCurrentAttemptDetailsHandler@31a4db4f under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@2842ef02 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptDetailsHandler@2842ef02 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt.
2023-03-31 11:26:04  [ main:8557 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@7e63374b under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.SubtaskExecutionAttemptAccumulatorsHandler@7e63374b under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@aa794a3 under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.SubtaskMetricsHandler@aa794a3 under GET@/jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@1dce481b under GET@/v1/jobs/:jobid/vertices/:vertexid/subtasktimes.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.SubtasksTimesHandler@1dce481b under GET@/jobs/:jobid/vertices/:vertexid/subtasktimes.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@22cb8e5f under GET@/v1/jobs/:jobid/vertices/:vertexid/taskmanagers.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobVertexTaskManagersHandler@22cb8e5f under GET@/jobs/:jobid/vertices/:vertexid/taskmanagers.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@45753c22 under GET@/v1/jobs/:jobid/vertices/:vertexid/watermarks.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.JobVertexWatermarksHandler@45753c22 under GET@/jobs/:jobid/vertices/:vertexid/watermarks.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@7341baa3 under GET@/v1/jobs/:jobid/yarn-cancel.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@7341baa3 under GET@/jobs/:jobid/yarn-cancel.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@1ca0aa40 under GET@/v1/jobs/:jobid/yarn-stop.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.JobCancellationHandler@1ca0aa40 under GET@/jobs/:jobid/yarn-stop.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@626766fd under GET@/v1/overview.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.cluster.ClusterOverviewHandler@626766fd under GET@/overview.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@e1fd2bf under POST@/v1/savepoint-disposal.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalTriggerHandler@e1fd2bf under POST@/savepoint-disposal.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@70f5f57d under GET@/v1/savepoint-disposal/:triggerid.
2023-03-31 11:26:04  [ main:8558 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.savepoints.SavepointDisposalHandlers$SavepointDisposalStatusHandler@70f5f57d under GET@/savepoint-disposal/:triggerid.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@691eb389 under GET@/v1/taskmanagers.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagersHandler@691eb389 under GET@/taskmanagers.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@301770d9 under GET@/v1/taskmanagers/metrics.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.AggregatingTaskManagersMetricsHandler@301770d9 under GET@/taskmanagers/metrics.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@40edc64e under GET@/v1/taskmanagers/:taskmanagerid.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler@40edc64e under GET@/taskmanagers/:taskmanagerid.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@1fec9d33 under GET@/v1/taskmanagers/:taskmanagerid/log.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogFileHandler@1fec9d33 under GET@/taskmanagers/:taskmanagerid/log.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@372f7bc under GET@/v1/taskmanagers/:taskmanagerid/logs.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerLogListHandler@372f7bc under GET@/taskmanagers/:taskmanagerid/logs.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@8e8ceb3 under GET@/v1/taskmanagers/:taskmanagerid/logs/:filename.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerCustomLogHandler@8e8ceb3 under GET@/taskmanagers/:taskmanagerid/logs/:filename.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@2699b656 under GET@/v1/taskmanagers/:taskmanagerid/metrics.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.job.metrics.TaskManagerMetricsHandler@2699b656 under GET@/taskmanagers/:taskmanagerid/metrics.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@5d67b2b3 under GET@/v1/taskmanagers/:taskmanagerid/stdout.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerStdoutFileHandler@5d67b2b3 under GET@/taskmanagers/:taskmanagerid/stdout.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@49bc71b4 under GET@/v1/taskmanagers/:taskmanagerid/thread-dump.
2023-03-31 11:26:04  [ main:8559 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerThreadDumpHandler@49bc71b4 under GET@/taskmanagers/:taskmanagerid/thread-dump.
2023-03-31 11:26:04  [ main:8560 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@7efa366 under GET@/v1/:*.
2023-03-31 11:26:04  [ main:8560 ] - [ DEBUG ]  Register handler org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler@7efa366 under GET@/:*.
2023-03-31 11:26:04  [ main:8568 ] - [ DEBUG ]  -Dio.netty.eventLoopThreads: 16
2023-03-31 11:26:04  [ main:8588 ] - [ DEBUG ]  -Dio.netty.noKeySetOptimization: false
2023-03-31 11:26:04  [ main:8589 ] - [ DEBUG ]  -Dio.netty.selectorAutoRebuildThreshold: 512
2023-03-31 11:26:04  [ main:8594 ] - [ DEBUG ]  org.jctools-core.MpscChunkedArrayQueue: available
2023-03-31 11:26:04  [ main:8616 ] - [ DEBUG ]  -Dio.netty.processId: 38678 (auto-detected)
2023-03-31 11:26:04  [ main:8619 ] - [ DEBUG ]  -Djava.net.preferIPv4Stack: false
2023-03-31 11:26:04  [ main:8619 ] - [ DEBUG ]  -Djava.net.preferIPv6Addresses: false
2023-03-31 11:26:04  [ main:8621 ] - [ DEBUG ]  Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
2023-03-31 11:26:04  [ main:8622 ] - [ DEBUG ]  Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
2023-03-31 11:26:04  [ main:8624 ] - [ DEBUG ]  -Dio.netty.machineId: 6c:b1:33:ff:fe:9b:24:36 (auto-detected)
2023-03-31 11:26:04  [ main:8634 ] - [ DEBUG ]  -Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.level: simple
2023-03-31 11:26:04  [ main:8634 ] - [ DEBUG ]  -Dorg.apache.flink.shaded.netty4.io.netty.leakDetection.targetRecords: 4
2023-03-31 11:26:04  [ main:8653 ] - [ DEBUG ]  -Dio.netty.allocator.numHeapArenas: 16
2023-03-31 11:26:04  [ main:8653 ] - [ DEBUG ]  -Dio.netty.allocator.numDirectArenas: 16
2023-03-31 11:26:04  [ main:8653 ] - [ DEBUG ]  -Dio.netty.allocator.pageSize: 8192
2023-03-31 11:26:04  [ main:8653 ] - [ DEBUG ]  -Dio.netty.allocator.maxOrder: 11
2023-03-31 11:26:04  [ main:8653 ] - [ DEBUG ]  -Dio.netty.allocator.chunkSize: 16777216
2023-03-31 11:26:04  [ main:8654 ] - [ DEBUG ]  -Dio.netty.allocator.smallCacheSize: 256
2023-03-31 11:26:04  [ main:8654 ] - [ DEBUG ]  -Dio.netty.allocator.normalCacheSize: 64
2023-03-31 11:26:04  [ main:8654 ] - [ DEBUG ]  -Dio.netty.allocator.maxCachedBufferCapacity: 32768
2023-03-31 11:26:04  [ main:8654 ] - [ DEBUG ]  -Dio.netty.allocator.cacheTrimInterval: 8192
2023-03-31 11:26:04  [ main:8654 ] - [ DEBUG ]  -Dio.netty.allocator.cacheTrimIntervalMillis: 0
2023-03-31 11:26:04  [ main:8654 ] - [ DEBUG ]  -Dio.netty.allocator.useCacheForAllThreads: true
2023-03-31 11:26:04  [ main:8654 ] - [ DEBUG ]  -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
2023-03-31 11:26:04  [ main:8663 ] - [ DEBUG ]  -Dio.netty.allocator.type: pooled
2023-03-31 11:26:04  [ main:8663 ] - [ DEBUG ]  -Dio.netty.threadLocalDirectBufferSize: 0
2023-03-31 11:26:04  [ main:8663 ] - [ DEBUG ]  -Dio.netty.maxThreadLocalCharBufferSize: 16384
2023-03-31 11:26:04  [ main:8683 ] - [ DEBUG ]  Binding rest endpoint to null:0.
2023-03-31 11:26:04  [ main:8684 ] - [ INFO ]  Rest endpoint listening at localhost:64730
2023-03-31 11:26:04  [ main:8688 ] - [ INFO ]  Proposing leadership to contender http://localhost:64730
2023-03-31 11:26:04  [ main:8695 ] - [ INFO ]  Web frontend listening at http://localhost:64730.
2023-03-31 11:26:04  [ mini-cluster-io-thread-1:8696 ] - [ INFO ]  http://localhost:64730 was granted leadership with leaderSessionID=3cc2ece4-c654-4205-bbde-160606bd274b
2023-03-31 11:26:04  [ mini-cluster-io-thread-1:8699 ] - [ INFO ]  Received confirmation of leadership for leader http://localhost:64730 , session=3cc2ece4-c654-4205-bbde-160606bd274b
2023-03-31 11:26:04  [ main:8726 ] - [ DEBUG ]  Starting Dispatcher.
2023-03-31 11:26:04  [ main:8730 ] - [ INFO ]  Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
2023-03-31 11:26:04  [ main:8730 ] - [ DEBUG ]  Starting ResourceManagerService.
2023-03-31 11:26:04  [ main:8730 ] - [ INFO ]  Starting resource manager service.
2023-03-31 11:26:04  [ main:8731 ] - [ INFO ]  Proposing leadership to contender LeaderContender: ResourceManagerServiceImpl
2023-03-31 11:26:04  [ mini-cluster-io-thread-2:8733 ] - [ INFO ]  DefaultDispatcherRunner was granted leadership with leader id 4bddecbe-97e7-4cb7-abf8-d295eacb9b02. Creating new DispatcherLeaderProcess.
2023-03-31 11:26:04  [ pool-6-thread-1:8736 ] - [ INFO ]  Resource manager service is granted leadership with session id 703586c5-a39d-4485-8e2f-79850281f03f.
2023-03-31 11:26:04  [ mini-cluster-io-thread-2:8760 ] - [ INFO ]  Start SessionDispatcherLeaderProcess.
2023-03-31 11:26:04  [ main:8761 ] - [ INFO ]  Flink Mini Cluster started successfully
2023-03-31 11:26:04  [ mini-cluster-io-thread-1:8767 ] - [ INFO ]  Recover all persisted job graphs.
2023-03-31 11:26:04  [ mini-cluster-io-thread-1:8768 ] - [ INFO ]  Successfully recovered 0 persisted job graphs.
2023-03-31 11:26:04  [ flink-akka.actor.supervisor-dispatcher-6:8786 ] - [ DEBUG ]  Starting FencedAkkaRpcActor with name dispatcher_1.
2023-03-31 11:26:04  [ mini-cluster-io-thread-1:8792 ] - [ INFO ]  Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .
2023-03-31 11:26:04  [ flink-akka.actor.supervisor-dispatcher-6:8809 ] - [ DEBUG ]  Starting FencedAkkaRpcActor with name resourcemanager_2.
2023-03-31 11:26:04  [ pool-6-thread-1:8809 ] - [ INFO ]  Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_2 .
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-5:8842 ] - [ INFO ]  Starting the resource manager.
2023-03-31 11:26:04  [ mini-cluster-io-thread-1:8853 ] - [ INFO ]  Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_1 , session=4bddecbe-97e7-4cb7-abf8-d295eacb9b02
2023-03-31 11:26:04  [ mini-cluster-io-thread-4:8857 ] - [ DEBUG ]  Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_1. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
2023-03-31 11:26:04  [ mini-cluster-io-thread-3:8858 ] - [ DEBUG ]  Try to connect to remote RPC endpoint with address akka://flink/user/rpc/dispatcher_1. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-5:8864 ] - [ DEBUG ]  Starting the slot manager.
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-5:8878 ] - [ DEBUG ]  Trigger heartbeat request.
2023-03-31 11:26:04  [ mini-cluster-io-thread-2:8878 ] - [ INFO ]  Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_2 , session=703586c5-a39d-4485-8e2f-79850281f03f
2023-03-31 11:26:04  [ mini-cluster-io-thread-2:8879 ] - [ DEBUG ]  Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_2. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-5:8884 ] - [ DEBUG ]  Trigger heartbeat request.
2023-03-31 11:26:04  [ mini-cluster-io-thread-1:8885 ] - [ DEBUG ]  Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_2. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-7:8888 ] - [ INFO ]  Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(8e2f79850281f03f703586c5a39d4485).
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-7:8902 ] - [ DEBUG ]  Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_2. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-7:8941 ] - [ INFO ]  Resolved ResourceManager address, beginning registration
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-7:8942 ] - [ DEBUG ]  Registration at ResourceManager attempt 1 (timeout=100ms)
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-5:8967 ] - [ DEBUG ]  Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-9:8980 ] - [ INFO ]  Registering TaskManager with ResourceID 66527030-9c98-43a4-b1f5-6190c00b39ae (akka://flink/user/rpc/taskmanager_0) at ResourceManager
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-9:9000 ] - [ DEBUG ]  Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_2 was successful.
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-9:9002 ] - [ INFO ]  Successful registration at resource manager akka://flink/user/rpc/resourcemanager_2 under registration id 36c46d75f0d6709eed7e91e0e296f01a.
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-5:9004 ] - [ DEBUG ]  Registering task executor 66527030-9c98-43a4-b1f5-6190c00b39ae under 36c46d75f0d6709eed7e91e0e296f01a at the slot manager.
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-7:9005 ] - [ INFO ]  Received JobGraph submission 'insert-into_default_catalog.default_database.print_table' (ba766f85be2130d6661f17297a4a9f8b).
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-7:9006 ] - [ INFO ]  Submitting job 'insert-into_default_catalog.default_database.print_table' (ba766f85be2130d6661f17297a4a9f8b).
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-7:9024 ] - [ DEBUG ]  Start leadership runner for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:04  [ flink-akka.actor.default-dispatcher-7:9025 ] - [ INFO ]  Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
2023-03-31 11:26:04  [ mini-cluster-io-thread-3:9026 ] - [ DEBUG ]  Create new JobMasterServiceProcess because we were granted leadership under 5bffa200-30a3-4658-9109-06eb82fbf448.
2023-03-31 11:26:04  [ ForkJoinPool.commonPool-worker-1:9034 ] - [ DEBUG ]  Wait until job initialization is finished
2023-03-31 11:26:04  [ flink-akka.actor.supervisor-dispatcher-6:9046 ] - [ DEBUG ]  Starting FencedAkkaRpcActor with name jobmanager_3.
2023-03-31 11:26:04  [ jobmanager-io-thread-1:9049 ] - [ INFO ]  Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
2023-03-31 11:26:04  [ jobmanager-io-thread-1:9071 ] - [ INFO ]  Initializing job 'insert-into_default_catalog.default_database.print_table' (ba766f85be2130d6661f17297a4a9f8b).
2023-03-31 11:26:04  [ jobmanager-io-thread-1:9108 ] - [ INFO ]  Using restart back off time strategy NoRestartBackoffTimeStrategy for insert-into_default_catalog.default_database.print_table (ba766f85be2130d6661f17297a4a9f8b).
2023-03-31 11:26:04  [ jobmanager-io-thread-1:9160 ] - [ INFO ]  Running initialization on master for job insert-into_default_catalog.default_database.print_table (ba766f85be2130d6661f17297a4a9f8b).
2023-03-31 11:26:04  [ jobmanager-io-thread-1:9163 ] - [ INFO ]  Successfully ran initialization on master in 3 ms.
2023-03-31 11:26:04  [ jobmanager-io-thread-1:9164 ] - [ DEBUG ]  Adding 1 vertices from job graph insert-into_default_catalog.default_database.print_table (ba766f85be2130d6661f17297a4a9f8b).
2023-03-31 11:26:04  [ jobmanager-io-thread-1:9164 ] - [ DEBUG ]  Attaching 1 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.
2023-03-31 11:26:05  [ jobmanager-io-thread-1:9419 ] - [ DEBUG ]  Connecting ExecutionJobVertex cbc357ccb763df2852fee8c4fc7d55f2 (Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number])) to 0 predecessors.
2023-03-31 11:26:05  [ jobmanager-io-thread-1:9437 ] - [ INFO ]  Built 36 pipelined regions in 3 ms
2023-03-31 11:26:05  [ jobmanager-io-thread-1:9440 ] - [ DEBUG ]  Successfully created execution graph from job graph insert-into_default_catalog.default_database.print_table (ba766f85be2130d6661f17297a4a9f8b).
2023-03-31 11:26:05  [ jobmanager-io-thread-1:9461 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@32a58dfe
2023-03-31 11:26:05  [ jobmanager-io-thread-1:9462 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ jobmanager-io-thread-1:9463 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@54e9fd22
2023-03-31 11:26:05  [ jobmanager-io-thread-1:9482 ] - [ DEBUG ]  Status of the shared state registry of job ba766f85be2130d6661f17297a4a9f8b after restore: SharedStateRegistry{registeredStates={}}.
2023-03-31 11:26:05  [ jobmanager-io-thread-1:9482 ] - [ INFO ]  No checkpoint found during restore.
2023-03-31 11:26:05  [ jobmanager-io-thread-1:9482 ] - [ DEBUG ]  Resetting the master hooks.
2023-03-31 11:26:05  [ jobmanager-io-thread-1:9487 ] - [ INFO ]  Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@44f1ef26 for insert-into_default_catalog.default_database.print_table (ba766f85be2130d6661f17297a4a9f8b).
2023-03-31 11:26:05  [ jobmanager-io-thread-1:9495 ] - [ DEBUG ]  Successfully created the JobMasterService for job ba766f85be2130d6661f17297a4a9f8b under leader id 5bffa200-30a3-4658-9109-06eb82fbf448.
2023-03-31 11:26:05  [ jobmanager-io-thread-1:9495 ] - [ DEBUG ]  Confirm leadership 5bffa200-30a3-4658-9109-06eb82fbf448.
2023-03-31 11:26:05  [ jobmanager-io-thread-1:9495 ] - [ INFO ]  Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=5bffa200-30a3-4658-9109-06eb82fbf448
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9498 ] - [ INFO ]  Starting execution of job 'insert-into_default_catalog.default_database.print_table' (ba766f85be2130d6661f17297a4a9f8b) under job master id 910906eb82fbf4485bffa20030a34658.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9500 ] - [ INFO ]  Starting split enumerator for source Source: HiveSource-ods.test_01.
2023-03-31 11:26:05  [ IPC Parameter Sending Thread #0:9508 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
2023-03-31 11:26:05  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:9528 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #5
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9529 ] - [ DEBUG ]  Call: getFileInfo took 23ms
2023-03-31 11:26:05  [ IPC Parameter Sending Thread #0:9536 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #6 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
2023-03-31 11:26:05  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:9540 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #6
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9540 ] - [ DEBUG ]  Call: getFileInfo took 5ms
2023-03-31 11:26:05  [ IPC Parameter Sending Thread #0:9542 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
2023-03-31 11:26:05  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:9565 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #7
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9567 ] - [ DEBUG ]  Call: getListing took 25ms
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9580 ] - [ DEBUG ]  Time taken to get FileStatuses: 46
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9582 ] - [ INFO ]  Total input files to process : 36
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9586 ] - [ DEBUG ]  Total # of splits generated by getSplits: 36, TimeTaken: 52
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9605 ] - [ INFO ]  Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9606 ] - [ INFO ]  Job insert-into_default_catalog.default_database.print_table (ba766f85be2130d6661f17297a4a9f8b) switched from state CREATED to RUNNING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9620 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36) (a804fa908af17cf17211c1700cf07411) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9627 ] - [ DEBUG ]  Received slot request [SlotRequestId{5adfd4e53b79c2f2bb0a371295717cc2}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9630 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{5adfd4e53b79c2f2bb0a371295717cc2} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9631 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9634 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{a1b011e220d017ae09f8162db569f90e}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{5adfd4e53b79c2f2bb0a371295717cc2})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9638 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36) (4fcda378a5d3eb0b9a9f36477c5ea9bc) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9638 ] - [ DEBUG ]  Received slot request [SlotRequestId{10fa1ddd50cd0a45caf9b78b201855cb}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9639 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{10fa1ddd50cd0a45caf9b78b201855cb} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9640 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9640 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{68ea29629da9d0637f9c176c92bdd919}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_1) from the physical slot (SlotRequestId{10fa1ddd50cd0a45caf9b78b201855cb})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9640 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36) (93d69acaf74237fbf2c6e56c068e29bc) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9640 ] - [ DEBUG ]  Received slot request [SlotRequestId{4dc1c6d88ab6df02acfba70daf35b9db}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9640 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{4dc1c6d88ab6df02acfba70daf35b9db} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9640 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9641 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{29e32a8b11282aec1a625cd3afc0bde1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_2) from the physical slot (SlotRequestId{4dc1c6d88ab6df02acfba70daf35b9db})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9641 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36) (9f4f3e1ecc6ba91aa045b2a561e97ef6) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9641 ] - [ DEBUG ]  Received slot request [SlotRequestId{acc7d804f0466d7e96ba13c2d16a1649}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9641 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{acc7d804f0466d7e96ba13c2d16a1649} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9641 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9641 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{9a2450a136e971aeda80ee332fb31e22}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_3) from the physical slot (SlotRequestId{acc7d804f0466d7e96ba13c2d16a1649})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9643 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36) (3a4a32942dddaf0385d40bde5fc34a86) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9644 ] - [ DEBUG ]  Received slot request [SlotRequestId{967f9760dfdbad4edc94afc216e9df51}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9644 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{967f9760dfdbad4edc94afc216e9df51} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9644 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=5}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9644 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{2b9e9dfe1e07aa91cc05428b6a6881b8}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_4) from the physical slot (SlotRequestId{967f9760dfdbad4edc94afc216e9df51})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9644 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36) (eb234fe2b78cf50d28a21c5e8428c8e2) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9644 ] - [ DEBUG ]  Received slot request [SlotRequestId{fc90ef3fc45405ced76ddb96cf3a0894}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9644 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{fc90ef3fc45405ced76ddb96cf3a0894} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9644 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=6}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9644 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{402d2a8f881b0384ae577292ac830226}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_5) from the physical slot (SlotRequestId{fc90ef3fc45405ced76ddb96cf3a0894})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9645 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36) (b3f0a8b054e16515fd877f40d8750540) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9645 ] - [ DEBUG ]  Received slot request [SlotRequestId{1ea124cb7077029de21e80c377d30d95}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9645 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{1ea124cb7077029de21e80c377d30d95} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9645 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=7}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9645 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{94a2c257d86d4ef6d5013b36066e5b12}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_6) from the physical slot (SlotRequestId{1ea124cb7077029de21e80c377d30d95})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9645 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36) (138b4422f4982380d457251dd8595f35) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9645 ] - [ DEBUG ]  Received slot request [SlotRequestId{720bde7c0295181f0e9abc3afbec8a3d}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9645 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{720bde7c0295181f0e9abc3afbec8a3d} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9645 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=8}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9645 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{eb7cd3ec6270905f9862d012cc7c8ac6}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_7) from the physical slot (SlotRequestId{720bde7c0295181f0e9abc3afbec8a3d})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9645 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36) (05715cb683fac77f419f61068d36469e) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9645 ] - [ DEBUG ]  Received slot request [SlotRequestId{dc467aa2be8a77fa6a1158e6ebf36357}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9646 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{dc467aa2be8a77fa6a1158e6ebf36357} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9646 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=9}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9646 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{2895aef5f6ca98b5183f7dfb94c602b1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_8) from the physical slot (SlotRequestId{dc467aa2be8a77fa6a1158e6ebf36357})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9646 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36) (4d8d3851631b53958f2950b993e951c3) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9646 ] - [ DEBUG ]  Received slot request [SlotRequestId{ece7b5e42097f59986901a24616c7f15}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9646 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{ece7b5e42097f59986901a24616c7f15} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9646 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=10}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9646 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{207012b26687b2f7a54c1d76b1af67e4}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_9) from the physical slot (SlotRequestId{ece7b5e42097f59986901a24616c7f15})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9647 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36) (ba08ed9470b8b256110a141383e411b8) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9647 ] - [ DEBUG ]  Received slot request [SlotRequestId{a98da1b687c3faac87f6d6435a460d89}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9647 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{a98da1b687c3faac87f6d6435a460d89} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9647 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=11}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9649 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{56f60c74544660a6e22955406236e396}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_10) from the physical slot (SlotRequestId{a98da1b687c3faac87f6d6435a460d89})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9650 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36) (0f08aa732c67bb70a34b87c43db14906) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9650 ] - [ DEBUG ]  Received slot request [SlotRequestId{cb1beae73ab58ba621436107f4d9612d}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9650 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{cb1beae73ab58ba621436107f4d9612d} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9651 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=12}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9651 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{cac52620136e26478f73acc5336b03a1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_11) from the physical slot (SlotRequestId{cb1beae73ab58ba621436107f4d9612d})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9651 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36) (bbaa5006ecd904ef625977413144688b) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9651 ] - [ DEBUG ]  Received slot request [SlotRequestId{010c7014363b5c3ec680462b1c1f575c}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9651 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{010c7014363b5c3ec680462b1c1f575c} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9651 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=13}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9651 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{b361e4af85c367e201d36ea958ad2b02}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_12) from the physical slot (SlotRequestId{010c7014363b5c3ec680462b1c1f575c})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9651 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36) (0e4277a1f6d404bc71bb79fcfbc9769e) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9651 ] - [ DEBUG ]  Received slot request [SlotRequestId{eccc283128f0f3eed6e053d9e1d950d6}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9651 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{eccc283128f0f3eed6e053d9e1d950d6} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9652 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=14}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9652 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{c9ec96b1dfea558674365b4112a1925c}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_13) from the physical slot (SlotRequestId{eccc283128f0f3eed6e053d9e1d950d6})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9652 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36) (7cbc3523540446665872efda7f69c476) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9652 ] - [ DEBUG ]  Received slot request [SlotRequestId{0c5e59063bb3e277a9d0f534fab0ba40}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9652 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{0c5e59063bb3e277a9d0f534fab0ba40} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9652 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=15}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9652 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{4d3172c6fdaae7c3c39d2b1996eed876}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_14) from the physical slot (SlotRequestId{0c5e59063bb3e277a9d0f534fab0ba40})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9653 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36) (6e822a8a973db92453d7d0dd1fdc15cc) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9653 ] - [ DEBUG ]  Received slot request [SlotRequestId{b8c01daecadb045b390d5cd08e8f24a1}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9653 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{b8c01daecadb045b390d5cd08e8f24a1} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9653 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=16}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9654 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{732e06267707ec2ea526e1714d2f22b9}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_15) from the physical slot (SlotRequestId{b8c01daecadb045b390d5cd08e8f24a1})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9654 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36) (4cacc0cf82a95a1a2846442152dd1ebd) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9655 ] - [ DEBUG ]  Received slot request [SlotRequestId{b1c63d1b0061d76298fc7f35d5b0ede7}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9655 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{b1c63d1b0061d76298fc7f35d5b0ede7} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9655 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=17}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9655 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{6365f315f77b21e01243eeb89fb4cdb3}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_16) from the physical slot (SlotRequestId{b1c63d1b0061d76298fc7f35d5b0ede7})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9655 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36) (0c9a99f0839bbedbc59a320c8e177bf5) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9655 ] - [ DEBUG ]  Received slot request [SlotRequestId{6e6f508e5379df168fe3dd60c9347960}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9655 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{6e6f508e5379df168fe3dd60c9347960} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9655 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=18}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9656 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{26a0ff7770e865f77a3b23191104ad96}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_17) from the physical slot (SlotRequestId{6e6f508e5379df168fe3dd60c9347960})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9656 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36) (ddd59049b3eb442e5b9545c88ad0890a) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9656 ] - [ DEBUG ]  Received slot request [SlotRequestId{8de43a1f4e34194663b91f428427f841}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9656 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{8de43a1f4e34194663b91f428427f841} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9656 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=19}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9657 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{31ec7339a419f014436a70eac127deb9}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_18) from the physical slot (SlotRequestId{8de43a1f4e34194663b91f428427f841})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9657 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36) (9f77d72911b3e515bb1f11ed3d21b925) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9657 ] - [ DEBUG ]  Received slot request [SlotRequestId{52a0eb3a9e348287659c7e3ed057c223}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9657 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{52a0eb3a9e348287659c7e3ed057c223} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9657 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=20}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9657 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{53f9c6e420d79edc49ed7f3f10372616}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_19) from the physical slot (SlotRequestId{52a0eb3a9e348287659c7e3ed057c223})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9657 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36) (986a109fb0d2fe0a94164c7afedc55e7) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9658 ] - [ DEBUG ]  Received slot request [SlotRequestId{067f7b2061a1aaaed945ffafc05f3d80}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9658 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{067f7b2061a1aaaed945ffafc05f3d80} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9658 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=21}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9658 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{d534a2104feb547754940c5495c1e1cb}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_20) from the physical slot (SlotRequestId{067f7b2061a1aaaed945ffafc05f3d80})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9658 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36) (0a0c6c9c1d45808bef65560eb7afc889) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9658 ] - [ DEBUG ]  Received slot request [SlotRequestId{839f8d62d5c2595795ef7f6126be583b}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9658 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{839f8d62d5c2595795ef7f6126be583b} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9658 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=22}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9658 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{8c2a29ea51a46314ec52202736f28999}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_21) from the physical slot (SlotRequestId{839f8d62d5c2595795ef7f6126be583b})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9659 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36) (536a25360f375ad9517b80540b08d145) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9660 ] - [ DEBUG ]  Received slot request [SlotRequestId{e364086a08fb426225858a132ff59015}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9660 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{e364086a08fb426225858a132ff59015} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9660 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=23}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9660 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{811b9fb47bac09639d940edcbf92bf68}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_22) from the physical slot (SlotRequestId{e364086a08fb426225858a132ff59015})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9660 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36) (cd37ec48093b17fb0d6fc7ba3aa250f4) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9660 ] - [ DEBUG ]  Received slot request [SlotRequestId{82011bf2f61eca70362907fa63ea8b39}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9660 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{82011bf2f61eca70362907fa63ea8b39} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9660 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=24}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9661 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{7cd64f44093cff196e204b560450efc1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_23) from the physical slot (SlotRequestId{82011bf2f61eca70362907fa63ea8b39})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9661 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36) (71d31e42db0cfc094428f464d12688f8) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9661 ] - [ DEBUG ]  Received slot request [SlotRequestId{1a54978c7b40db6ae94bb73fd91740b1}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9661 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{1a54978c7b40db6ae94bb73fd91740b1} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9661 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=25}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9661 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{882acd84211011703d7109a63ee98252}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_24) from the physical slot (SlotRequestId{1a54978c7b40db6ae94bb73fd91740b1})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9661 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36) (0180ce402fb520a6d0325667c4c9f55b) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9661 ] - [ DEBUG ]  Received slot request [SlotRequestId{bb029117f280c8fbeaf176817245b0ee}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9661 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{bb029117f280c8fbeaf176817245b0ee} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9661 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=26}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9661 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{f29d86f0ac8fcfa4d8afb19207159a6e}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_25) from the physical slot (SlotRequestId{bb029117f280c8fbeaf176817245b0ee})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9662 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36) (2f88ff53e91055cd65a3913274b72538) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9662 ] - [ DEBUG ]  Received slot request [SlotRequestId{bb105b4fcf7b451142e05f4d95da2e44}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9662 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{bb105b4fcf7b451142e05f4d95da2e44} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9662 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=27}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9662 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{f02a9e1c9c254777c6bc21160561678a}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_26) from the physical slot (SlotRequestId{bb105b4fcf7b451142e05f4d95da2e44})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9663 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36) (a1bb9f26bcffcad198f52a482fb53559) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9663 ] - [ DEBUG ]  Received slot request [SlotRequestId{a177cc3e8856646080c9b0a9ebee356b}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9663 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{a177cc3e8856646080c9b0a9ebee356b} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9663 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=28}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9663 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{3f371a2e1cafd1ad33e9bedf3f4c05c1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_27) from the physical slot (SlotRequestId{a177cc3e8856646080c9b0a9ebee356b})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9663 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36) (1eb215c73dfbc39e6702cf343ce8d121) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9663 ] - [ DEBUG ]  Received slot request [SlotRequestId{286cd3c53d99b2de3bb5899056bdd5ff}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9663 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{286cd3c53d99b2de3bb5899056bdd5ff} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9663 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=29}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9663 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{9109655bf279ee91f05c5cd1a652ee51}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_28) from the physical slot (SlotRequestId{286cd3c53d99b2de3bb5899056bdd5ff})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9663 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36) (38f97eb8a1c5de44871786bc3329ddac) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9663 ] - [ DEBUG ]  Received slot request [SlotRequestId{224f621608445848dc3b736a8729ab01}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9664 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{224f621608445848dc3b736a8729ab01} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9664 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=30}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9664 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{a0631b75a63d326f272eea8365caf81c}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_29) from the physical slot (SlotRequestId{224f621608445848dc3b736a8729ab01})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9664 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36) (737edf5169bbacb604ef4156b6663334) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9664 ] - [ DEBUG ]  Received slot request [SlotRequestId{0b839e3791d34c9c5051bc9b454f6355}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9664 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{0b839e3791d34c9c5051bc9b454f6355} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9664 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=31}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9664 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{fb16285e710742184fcdf7bf45a8e828}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_30) from the physical slot (SlotRequestId{0b839e3791d34c9c5051bc9b454f6355})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9664 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36) (3757ee42c112a6cecf5c7cdc072c08b6) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9664 ] - [ DEBUG ]  Received slot request [SlotRequestId{81aa6c879e299c9d96bda2f6c37ada42}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9664 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{81aa6c879e299c9d96bda2f6c37ada42} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9664 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=32}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9665 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{84af3687dc11df61de3007025a9b921f}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_31) from the physical slot (SlotRequestId{81aa6c879e299c9d96bda2f6c37ada42})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9665 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36) (c19840797252b89377f7ef7ba4fcf5d4) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9665 ] - [ DEBUG ]  Received slot request [SlotRequestId{adf445367b8cbe53076e4e4617f1bff5}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9665 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{adf445367b8cbe53076e4e4617f1bff5} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9665 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=33}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9665 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{a18f6d7fd9e005a361d5b001261988d0}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_32) from the physical slot (SlotRequestId{adf445367b8cbe53076e4e4617f1bff5})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9665 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36) (4a8bef25bef4f0beec2b6df232b4aefc) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9666 ] - [ DEBUG ]  Received slot request [SlotRequestId{ed885ac3e126f588dd4c9ac5485bd1fe}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9666 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{ed885ac3e126f588dd4c9ac5485bd1fe} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9666 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=34}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9666 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{cb0f8060b632163d16bb92223b445eac}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_33) from the physical slot (SlotRequestId{ed885ac3e126f588dd4c9ac5485bd1fe})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9667 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36) (3bae0b19ee60137b874880f7d35cdfca) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9668 ] - [ DEBUG ]  Received slot request [SlotRequestId{a83a108aa0c1c84bcd49b3a427c9a3d0}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9669 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{a83a108aa0c1c84bcd49b3a427c9a3d0} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9669 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=35}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9669 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{06860c5d9aa3150e66a816ff77c3764a}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_34) from the physical slot (SlotRequestId{a83a108aa0c1c84bcd49b3a427c9a3d0})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9669 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36) (18427157dcd5156c436e5dfbf779c7ab) switched from CREATED to SCHEDULED.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9670 ] - [ DEBUG ]  Received slot request [SlotRequestId{5306c134b4aa837165b67d87a69bf745}] with resource requirements: ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9670 ] - [ DEBUG ]  Request new allocated batch slot with slot request id SlotRequestId{5306c134b4aa837165b67d87a69bf745} and resource profile ResourceProfile{UNKNOWN}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9670 ] - [ DEBUG ]  Declare new resource requirements for job ba766f85be2130d6661f17297a4a9f8b.
	required resources: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=36}]
	acquired resources: ResourceCounter{resources={}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9670 ] - [ DEBUG ]  Request a logical slot (SlotRequestId{5a4b473b8e00f4fb7c7f04e6e1f862e3}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_35) from the physical slot (SlotRequestId{5306c134b4aa837165b67d87a69bf745})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9670 ] - [ DEBUG ]  Trigger heartbeat request.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9671 ] - [ INFO ]  Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_2(8e2f79850281f03f703586c5a39d4485)
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9672 ] - [ DEBUG ]  Try to connect to remote RPC endpoint with address akka://flink/user/rpc/resourcemanager_2. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9676 ] - [ INFO ]  Resolved ResourceManager address, beginning registration
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9678 ] - [ DEBUG ]  Registration at ResourceManager attempt 1 (timeout=100ms)
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9683 ] - [ DEBUG ]  Add job ba766f85be2130d6661f17297a4a9f8b to job leader id monitoring.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9685 ] - [ INFO ]  Registering job manager 910906eb82fbf4485bffa20030a34658@akka://flink/user/rpc/jobmanager_3 for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ mini-cluster-io-thread-1:9685 ] - [ DEBUG ]  Job ba766f85be2130d6661f17297a4a9f8b has a new job leader 5bffa200-30a3-4658-9109-06eb82fbf448@akka://flink/user/rpc/jobmanager_3.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9687 ] - [ DEBUG ]  Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9698 ] - [ INFO ]  Registered job manager 910906eb82fbf4485bffa20030a34658@akka://flink/user/rpc/jobmanager_3 for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9699 ] - [ DEBUG ]  Registration with ResourceManager at akka://flink/user/rpc/resourcemanager_2 was successful.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9700 ] - [ INFO ]  JobManager successfully registered at ResourceManager, leader id: 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9702 ] - [ INFO ]  Received resource requirements from job ba766f85be2130d6661f17297a4a9f8b: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=36}]
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9705 ] - [ DEBUG ]  Initiating tracking of resources for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9707 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_0 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9710 ] - [ INFO ]  Receive slot request 1115d5439ef2cf0f882dd10b19cda519 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9710 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_1 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9710 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_2 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9711 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_3 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9711 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_4 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9711 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_5 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9714 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_6 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9714 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_7 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9715 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_8 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9715 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_9 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9715 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_10 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9715 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_11 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9716 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_12 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9716 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_13 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9716 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_14 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9717 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_15 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9717 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9717 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_16 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9718 ] - [ INFO ]  Allocated slot for 1115d5439ef2cf0f882dd10b19cda519.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9720 ] - [ INFO ]  Add job ba766f85be2130d6661f17297a4a9f8b for job leader monitoring.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9722 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_17 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ mini-cluster-io-thread-3:9722 ] - [ DEBUG ]  New leader information for job ba766f85be2130d6661f17297a4a9f8b. Address: akka://flink/user/rpc/jobmanager_3, leader id: 910906eb82fbf4485bffa20030a34658.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9722 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_18 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9723 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_19 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ mini-cluster-io-thread-3:9723 ] - [ INFO ]  Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 5bffa200-30a3-4658-9109-06eb82fbf448.
2023-03-31 11:26:05  [ mini-cluster-io-thread-3:9723 ] - [ DEBUG ]  Try to connect to remote RPC endpoint with address akka://flink/user/rpc/jobmanager_3. Returning a org.apache.flink.runtime.jobmaster.JobMasterGateway gateway.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9723 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_20 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9723 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_21 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9724 ] - [ INFO ]  Receive slot request e513fdd5c4b1f46fd684e15d482ed8d6 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9724 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9724 ] - [ INFO ]  Resolved JobManager address, beginning registration
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9724 ] - [ DEBUG ]  Registration at JobManager attempt 1 (timeout=100ms)
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9724 ] - [ INFO ]  Allocated slot for e513fdd5c4b1f46fd684e15d482ed8d6.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9725 ] - [ INFO ]  Receive slot request 43352989c37f53018f9b85386d62156a for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9725 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9725 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_22 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9725 ] - [ INFO ]  Allocated slot for 43352989c37f53018f9b85386d62156a.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9725 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_23 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9725 ] - [ INFO ]  Receive slot request ad3de297d0b909d6b7bc2396ceaa2e26 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9726 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9726 ] - [ INFO ]  Allocated slot for ad3de297d0b909d6b7bc2396ceaa2e26.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9726 ] - [ INFO ]  Receive slot request 9b0bc5af62966b679b2a4a0f2bfa483f for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9726 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9726 ] - [ INFO ]  Allocated slot for 9b0bc5af62966b679b2a4a0f2bfa483f.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9727 ] - [ INFO ]  Receive slot request 96e72c04c02b602173c636ffb4b26af7 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9727 ] - [ DEBUG ]  Try to connect to remote RPC endpoint with address akka://flink/user/rpc/taskmanager_0. Returning a org.apache.flink.runtime.taskexecutor.TaskExecutorGateway gateway.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9727 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9727 ] - [ INFO ]  Allocated slot for 96e72c04c02b602173c636ffb4b26af7.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9727 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_24 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9728 ] - [ INFO ]  Receive slot request f47ce2af1de0ed2306225306c3d26dfe for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9728 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9728 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_25 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9728 ] - [ INFO ]  Allocated slot for f47ce2af1de0ed2306225306c3d26dfe.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9728 ] - [ INFO ]  Receive slot request 30eb83318128e345d3f24db5bb02ea14 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9728 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9729 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_26 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9729 ] - [ INFO ]  Allocated slot for 30eb83318128e345d3f24db5bb02ea14.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9729 ] - [ INFO ]  Receive slot request 4cf541597e0efa5c4c02d18777aa0ff3 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9730 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9732 ] - [ INFO ]  Allocated slot for 4cf541597e0efa5c4c02d18777aa0ff3.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9732 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_27 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9732 ] - [ INFO ]  Receive slot request 32204247f2a9dcbb3f64030bc03c896c for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9732 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9733 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_28 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9733 ] - [ INFO ]  Allocated slot for 32204247f2a9dcbb3f64030bc03c896c.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9733 ] - [ INFO ]  Receive slot request ad9cd0d0c392e6fd77603f6af858c8d0 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9733 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9733 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_29 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9733 ] - [ INFO ]  Allocated slot for ad9cd0d0c392e6fd77603f6af858c8d0.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9733 ] - [ INFO ]  Receive slot request 916a0304e1b3a002e37bce0a5a3d6a91 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9733 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_30 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9733 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9733 ] - [ INFO ]  Allocated slot for 916a0304e1b3a002e37bce0a5a3d6a91.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9734 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_31 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9734 ] - [ INFO ]  Receive slot request c2a6a50e69242d36c1517984ba118d27 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9734 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9734 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_32 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9734 ] - [ INFO ]  Allocated slot for c2a6a50e69242d36c1517984ba118d27.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9734 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_33 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9734 ] - [ INFO ]  Receive slot request 1c857d3bd2c64719d4eb43e86d804337 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9734 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9734 ] - [ INFO ]  Allocated slot for 1c857d3bd2c64719d4eb43e86d804337.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9734 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_34 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9734 ] - [ INFO ]  Receive slot request 27393d5d22fc71fa3675aa19b69683f2 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9734 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9734 ] - [ INFO ]  Allocated slot for 27393d5d22fc71fa3675aa19b69683f2.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:9734 ] - [ DEBUG ]  Starting allocation of slot 66527030-9c98-43a4-b1f5-6190c00b39ae_35 for job ba766f85be2130d6661f17297a4a9f8b with resource profile ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9735 ] - [ INFO ]  Receive slot request 529db54fdf957f05051f2a9705e53dbb for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9735 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9737 ] - [ INFO ]  Allocated slot for 529db54fdf957f05051f2a9705e53dbb.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9738 ] - [ INFO ]  Receive slot request 22a91adbd20099800ddd036173df6f0c for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9739 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9739 ] - [ INFO ]  Allocated slot for 22a91adbd20099800ddd036173df6f0c.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9739 ] - [ INFO ]  Receive slot request 11097abd215e6451f2e95b37fe86699a for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9739 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9739 ] - [ INFO ]  Allocated slot for 11097abd215e6451f2e95b37fe86699a.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9740 ] - [ INFO ]  Receive slot request b0ffa6ffd647ebc3dd5e4a07d4f4a3b6 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9740 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9740 ] - [ INFO ]  Allocated slot for b0ffa6ffd647ebc3dd5e4a07d4f4a3b6.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9740 ] - [ INFO ]  Receive slot request 51ca3690186c6f89309474a25879bbf5 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9740 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9740 ] - [ INFO ]  Allocated slot for 51ca3690186c6f89309474a25879bbf5.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9740 ] - [ INFO ]  Receive slot request 36086a0f68b34902fe232fae71009313 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9740 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9741 ] - [ INFO ]  Allocated slot for 36086a0f68b34902fe232fae71009313.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9741 ] - [ INFO ]  Receive slot request d4a609ea419f71be34cb5de07aa2a014 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9741 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9741 ] - [ INFO ]  Allocated slot for d4a609ea419f71be34cb5de07aa2a014.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9741 ] - [ INFO ]  Receive slot request 275e91a727866e4043757d69ead9e2ab for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9741 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9742 ] - [ INFO ]  Allocated slot for 275e91a727866e4043757d69ead9e2ab.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9742 ] - [ INFO ]  Receive slot request 0cf73c7f9304a1056efe98b0f6f517df for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9742 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9742 ] - [ INFO ]  Allocated slot for 0cf73c7f9304a1056efe98b0f6f517df.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9742 ] - [ INFO ]  Receive slot request 4f34c734ce3549c59d170e43098814ec for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9742 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9742 ] - [ INFO ]  Allocated slot for 4f34c734ce3549c59d170e43098814ec.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9743 ] - [ INFO ]  Receive slot request 178afb1de8d4b4f843316406dbcc6c67 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9743 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9743 ] - [ INFO ]  Allocated slot for 178afb1de8d4b4f843316406dbcc6c67.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9743 ] - [ INFO ]  Receive slot request 06d1567f24d75f9b5dae99b52084c1a4 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9743 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9743 ] - [ INFO ]  Allocated slot for 06d1567f24d75f9b5dae99b52084c1a4.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9744 ] - [ INFO ]  Receive slot request cd29661e846913336458ae880a376e5b for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9744 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9744 ] - [ INFO ]  Allocated slot for cd29661e846913336458ae880a376e5b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9745 ] - [ INFO ]  Receive slot request ecf8a41a8ff33e11a5134e31acbb5e89 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9745 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9745 ] - [ INFO ]  Allocated slot for ecf8a41a8ff33e11a5134e31acbb5e89.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9745 ] - [ INFO ]  Receive slot request 3fa0eff69efa0bd8ffb5d90f845c4693 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9746 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9747 ] - [ INFO ]  Allocated slot for 3fa0eff69efa0bd8ffb5d90f845c4693.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9748 ] - [ INFO ]  Receive slot request 29bff3f54d94023cdd813b9ee98997b1 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9749 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9749 ] - [ INFO ]  Allocated slot for 29bff3f54d94023cdd813b9ee98997b1.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9749 ] - [ INFO ]  Receive slot request 106f0ba9ba38260c4ed7492d2955d689 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9750 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9750 ] - [ INFO ]  Allocated slot for 106f0ba9ba38260c4ed7492d2955d689.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9750 ] - [ INFO ]  Receive slot request 46827fcd58daec7152008dd28742381a for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9750 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9750 ] - [ INFO ]  Allocated slot for 46827fcd58daec7152008dd28742381a.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9750 ] - [ INFO ]  Receive slot request 19602b51c9127c777128cba1de2ca8db for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9751 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9751 ] - [ INFO ]  Allocated slot for 19602b51c9127c777128cba1de2ca8db.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9751 ] - [ INFO ]  Receive slot request fdf2ddb6217f04cda389971a9af4a223 for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9751 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9751 ] - [ INFO ]  Allocated slot for fdf2ddb6217f04cda389971a9af4a223.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9751 ] - [ INFO ]  Receive slot request d0c10e9b833c29c8e13dc6e3b5cd71cb for job ba766f85be2130d6661f17297a4a9f8b from resource manager with leader id 8e2f79850281f03f703586c5a39d4485.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9751 ] - [ DEBUG ]  Initialized MemoryManager with total memory size 3728270 and page size 32768.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9751 ] - [ INFO ]  Allocated slot for d0c10e9b833c29c8e13dc6e3b5cd71cb.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9752 ] - [ DEBUG ]  Register new TaskExecutor 66527030-9c98-43a4-b1f5-6190c00b39ae.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:9754 ] - [ DEBUG ]  Registration with JobManager at akka://flink/user/rpc/jobmanager_3 was successful.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9756 ] - [ INFO ]  Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9757 ] - [ INFO ]  Establish JobManager connection for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9760 ] - [ INFO ]  Offer reserved slots to the leader of job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9768 ] - [ DEBUG ]  Received 36 slot offers from TaskExecutor 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1).
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9769 ] - [ DEBUG ]  Matched slot offer 4f34c734ce3549c59d170e43098814ec to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9770 ] - [ DEBUG ]  Matched slot offer 3fa0eff69efa0bd8ffb5d90f845c4693 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9771 ] - [ DEBUG ]  Matched slot offer 916a0304e1b3a002e37bce0a5a3d6a91 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9771 ] - [ DEBUG ]  Matched slot offer 51ca3690186c6f89309474a25879bbf5 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9771 ] - [ DEBUG ]  Matched slot offer 29bff3f54d94023cdd813b9ee98997b1 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9771 ] - [ DEBUG ]  Matched slot offer 32204247f2a9dcbb3f64030bc03c896c to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9771 ] - [ DEBUG ]  Matched slot offer ecf8a41a8ff33e11a5134e31acbb5e89 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9771 ] - [ DEBUG ]  Matched slot offer 275e91a727866e4043757d69ead9e2ab to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9771 ] - [ DEBUG ]  Matched slot offer cd29661e846913336458ae880a376e5b to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9771 ] - [ DEBUG ]  Matched slot offer 529db54fdf957f05051f2a9705e53dbb to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9771 ] - [ DEBUG ]  Matched slot offer 106f0ba9ba38260c4ed7492d2955d689 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9772 ] - [ DEBUG ]  Matched slot offer 96e72c04c02b602173c636ffb4b26af7 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer 1c857d3bd2c64719d4eb43e86d804337 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer f47ce2af1de0ed2306225306c3d26dfe to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer 1115d5439ef2cf0f882dd10b19cda519 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer c2a6a50e69242d36c1517984ba118d27 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer 06d1567f24d75f9b5dae99b52084c1a4 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer 22a91adbd20099800ddd036173df6f0c to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer b0ffa6ffd647ebc3dd5e4a07d4f4a3b6 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer ad9cd0d0c392e6fd77603f6af858c8d0 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer 4cf541597e0efa5c4c02d18777aa0ff3 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer ad3de297d0b909d6b7bc2396ceaa2e26 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer 30eb83318128e345d3f24db5bb02ea14 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer 178afb1de8d4b4f843316406dbcc6c67 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer 36086a0f68b34902fe232fae71009313 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9773 ] - [ DEBUG ]  Matched slot offer 43352989c37f53018f9b85386d62156a to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9774 ] - [ DEBUG ]  Matched slot offer 27393d5d22fc71fa3675aa19b69683f2 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9774 ] - [ DEBUG ]  Matched slot offer 19602b51c9127c777128cba1de2ca8db to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9774 ] - [ DEBUG ]  Matched slot offer 0cf73c7f9304a1056efe98b0f6f517df to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9774 ] - [ DEBUG ]  Matched slot offer 46827fcd58daec7152008dd28742381a to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9774 ] - [ DEBUG ]  Matched slot offer d4a609ea419f71be34cb5de07aa2a014 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9774 ] - [ DEBUG ]  Matched slot offer fdf2ddb6217f04cda389971a9af4a223 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9774 ] - [ DEBUG ]  Matched slot offer 9b0bc5af62966b679b2a4a0f2bfa483f to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9774 ] - [ DEBUG ]  Matched slot offer d0c10e9b833c29c8e13dc6e3b5cd71cb to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9774 ] - [ DEBUG ]  Matched slot offer e513fdd5c4b1f46fd684e15d482ed8d6 to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9775 ] - [ DEBUG ]  Matched slot offer 11097abd215e6451f2e95b37fe86699a to requirement ResourceProfile{UNKNOWN}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9776 ] - [ DEBUG ]  Acquired new resources; new total acquired resources: ResourceCounter{resources={ResourceProfile{UNKNOWN}=36}}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9776 ] - [ DEBUG ]  Matched slot AllocatedSlot 4f34c734ce3549c59d170e43098814ec @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 24 to pending request PendingRequest{slotRequestId=SlotRequestId{5adfd4e53b79c2f2bb0a371295717cc2}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9776 ] - [ DEBUG ]  Reserve slot 4f34c734ce3549c59d170e43098814ec for slot request id SlotRequestId{5adfd4e53b79c2f2bb0a371295717cc2}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9776 ] - [ DEBUG ]  Reserve free slot with allocation id 4f34c734ce3549c59d170e43098814ec.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Matched slot AllocatedSlot 3fa0eff69efa0bd8ffb5d90f845c4693 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 29 to pending request PendingRequest{slotRequestId=SlotRequestId{10fa1ddd50cd0a45caf9b78b201855cb}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Reserve slot 3fa0eff69efa0bd8ffb5d90f845c4693 for slot request id SlotRequestId{10fa1ddd50cd0a45caf9b78b201855cb}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Reserve free slot with allocation id 3fa0eff69efa0bd8ffb5d90f845c4693.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Matched slot AllocatedSlot 916a0304e1b3a002e37bce0a5a3d6a91 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 11 to pending request PendingRequest{slotRequestId=SlotRequestId{4dc1c6d88ab6df02acfba70daf35b9db}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Reserve slot 916a0304e1b3a002e37bce0a5a3d6a91 for slot request id SlotRequestId{4dc1c6d88ab6df02acfba70daf35b9db}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Reserve free slot with allocation id 916a0304e1b3a002e37bce0a5a3d6a91.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Matched slot AllocatedSlot 51ca3690186c6f89309474a25879bbf5 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 19 to pending request PendingRequest{slotRequestId=SlotRequestId{acc7d804f0466d7e96ba13c2d16a1649}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Reserve slot 51ca3690186c6f89309474a25879bbf5 for slot request id SlotRequestId{acc7d804f0466d7e96ba13c2d16a1649}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Reserve free slot with allocation id 51ca3690186c6f89309474a25879bbf5.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Matched slot AllocatedSlot 29bff3f54d94023cdd813b9ee98997b1 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 30 to pending request PendingRequest{slotRequestId=SlotRequestId{967f9760dfdbad4edc94afc216e9df51}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Reserve slot 29bff3f54d94023cdd813b9ee98997b1 for slot request id SlotRequestId{967f9760dfdbad4edc94afc216e9df51}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Reserve free slot with allocation id 29bff3f54d94023cdd813b9ee98997b1.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Matched slot AllocatedSlot 32204247f2a9dcbb3f64030bc03c896c @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 9 to pending request PendingRequest{slotRequestId=SlotRequestId{fc90ef3fc45405ced76ddb96cf3a0894}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Reserve slot 32204247f2a9dcbb3f64030bc03c896c for slot request id SlotRequestId{fc90ef3fc45405ced76ddb96cf3a0894}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9777 ] - [ DEBUG ]  Reserve free slot with allocation id 32204247f2a9dcbb3f64030bc03c896c.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Matched slot AllocatedSlot ecf8a41a8ff33e11a5134e31acbb5e89 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 28 to pending request PendingRequest{slotRequestId=SlotRequestId{1ea124cb7077029de21e80c377d30d95}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve slot ecf8a41a8ff33e11a5134e31acbb5e89 for slot request id SlotRequestId{1ea124cb7077029de21e80c377d30d95}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve free slot with allocation id ecf8a41a8ff33e11a5134e31acbb5e89.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Matched slot AllocatedSlot 275e91a727866e4043757d69ead9e2ab @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 22 to pending request PendingRequest{slotRequestId=SlotRequestId{720bde7c0295181f0e9abc3afbec8a3d}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve slot 275e91a727866e4043757d69ead9e2ab for slot request id SlotRequestId{720bde7c0295181f0e9abc3afbec8a3d}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve free slot with allocation id 275e91a727866e4043757d69ead9e2ab.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Matched slot AllocatedSlot cd29661e846913336458ae880a376e5b @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 27 to pending request PendingRequest{slotRequestId=SlotRequestId{dc467aa2be8a77fa6a1158e6ebf36357}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve slot cd29661e846913336458ae880a376e5b for slot request id SlotRequestId{dc467aa2be8a77fa6a1158e6ebf36357}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve free slot with allocation id cd29661e846913336458ae880a376e5b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Matched slot AllocatedSlot 529db54fdf957f05051f2a9705e53dbb @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 15 to pending request PendingRequest{slotRequestId=SlotRequestId{ece7b5e42097f59986901a24616c7f15}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve slot 529db54fdf957f05051f2a9705e53dbb for slot request id SlotRequestId{ece7b5e42097f59986901a24616c7f15}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve free slot with allocation id 529db54fdf957f05051f2a9705e53dbb.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Matched slot AllocatedSlot 106f0ba9ba38260c4ed7492d2955d689 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 31 to pending request PendingRequest{slotRequestId=SlotRequestId{a98da1b687c3faac87f6d6435a460d89}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve slot 106f0ba9ba38260c4ed7492d2955d689 for slot request id SlotRequestId{a98da1b687c3faac87f6d6435a460d89}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve free slot with allocation id 106f0ba9ba38260c4ed7492d2955d689.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Matched slot AllocatedSlot 96e72c04c02b602173c636ffb4b26af7 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 5 to pending request PendingRequest{slotRequestId=SlotRequestId{cb1beae73ab58ba621436107f4d9612d}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve slot 96e72c04c02b602173c636ffb4b26af7 for slot request id SlotRequestId{cb1beae73ab58ba621436107f4d9612d}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve free slot with allocation id 96e72c04c02b602173c636ffb4b26af7.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Matched slot AllocatedSlot 1c857d3bd2c64719d4eb43e86d804337 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 13 to pending request PendingRequest{slotRequestId=SlotRequestId{010c7014363b5c3ec680462b1c1f575c}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve slot 1c857d3bd2c64719d4eb43e86d804337 for slot request id SlotRequestId{010c7014363b5c3ec680462b1c1f575c}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9778 ] - [ DEBUG ]  Reserve free slot with allocation id 1c857d3bd2c64719d4eb43e86d804337.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Matched slot AllocatedSlot f47ce2af1de0ed2306225306c3d26dfe @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 6 to pending request PendingRequest{slotRequestId=SlotRequestId{eccc283128f0f3eed6e053d9e1d950d6}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Reserve slot f47ce2af1de0ed2306225306c3d26dfe for slot request id SlotRequestId{eccc283128f0f3eed6e053d9e1d950d6}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Reserve free slot with allocation id f47ce2af1de0ed2306225306c3d26dfe.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Matched slot AllocatedSlot 1115d5439ef2cf0f882dd10b19cda519 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 0 to pending request PendingRequest{slotRequestId=SlotRequestId{0c5e59063bb3e277a9d0f534fab0ba40}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Reserve slot 1115d5439ef2cf0f882dd10b19cda519 for slot request id SlotRequestId{0c5e59063bb3e277a9d0f534fab0ba40}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Reserve free slot with allocation id 1115d5439ef2cf0f882dd10b19cda519.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Matched slot AllocatedSlot c2a6a50e69242d36c1517984ba118d27 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 12 to pending request PendingRequest{slotRequestId=SlotRequestId{b8c01daecadb045b390d5cd08e8f24a1}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Reserve slot c2a6a50e69242d36c1517984ba118d27 for slot request id SlotRequestId{b8c01daecadb045b390d5cd08e8f24a1}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Reserve free slot with allocation id c2a6a50e69242d36c1517984ba118d27.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Matched slot AllocatedSlot 06d1567f24d75f9b5dae99b52084c1a4 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 26 to pending request PendingRequest{slotRequestId=SlotRequestId{b1c63d1b0061d76298fc7f35d5b0ede7}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Reserve slot 06d1567f24d75f9b5dae99b52084c1a4 for slot request id SlotRequestId{b1c63d1b0061d76298fc7f35d5b0ede7}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Reserve free slot with allocation id 06d1567f24d75f9b5dae99b52084c1a4.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Matched slot AllocatedSlot 22a91adbd20099800ddd036173df6f0c @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 16 to pending request PendingRequest{slotRequestId=SlotRequestId{6e6f508e5379df168fe3dd60c9347960}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Reserve slot 22a91adbd20099800ddd036173df6f0c for slot request id SlotRequestId{6e6f508e5379df168fe3dd60c9347960}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Reserve free slot with allocation id 22a91adbd20099800ddd036173df6f0c.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Matched slot AllocatedSlot b0ffa6ffd647ebc3dd5e4a07d4f4a3b6 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 18 to pending request PendingRequest{slotRequestId=SlotRequestId{8de43a1f4e34194663b91f428427f841}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9779 ] - [ DEBUG ]  Reserve slot b0ffa6ffd647ebc3dd5e4a07d4f4a3b6 for slot request id SlotRequestId{8de43a1f4e34194663b91f428427f841}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Reserve free slot with allocation id b0ffa6ffd647ebc3dd5e4a07d4f4a3b6.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Matched slot AllocatedSlot ad9cd0d0c392e6fd77603f6af858c8d0 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 10 to pending request PendingRequest{slotRequestId=SlotRequestId{52a0eb3a9e348287659c7e3ed057c223}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Reserve slot ad9cd0d0c392e6fd77603f6af858c8d0 for slot request id SlotRequestId{52a0eb3a9e348287659c7e3ed057c223}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Reserve free slot with allocation id ad9cd0d0c392e6fd77603f6af858c8d0.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Matched slot AllocatedSlot 4cf541597e0efa5c4c02d18777aa0ff3 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 8 to pending request PendingRequest{slotRequestId=SlotRequestId{067f7b2061a1aaaed945ffafc05f3d80}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Reserve slot 4cf541597e0efa5c4c02d18777aa0ff3 for slot request id SlotRequestId{067f7b2061a1aaaed945ffafc05f3d80}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Reserve free slot with allocation id 4cf541597e0efa5c4c02d18777aa0ff3.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Matched slot AllocatedSlot ad3de297d0b909d6b7bc2396ceaa2e26 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 3 to pending request PendingRequest{slotRequestId=SlotRequestId{839f8d62d5c2595795ef7f6126be583b}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Reserve slot ad3de297d0b909d6b7bc2396ceaa2e26 for slot request id SlotRequestId{839f8d62d5c2595795ef7f6126be583b}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Reserve free slot with allocation id ad3de297d0b909d6b7bc2396ceaa2e26.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Matched slot AllocatedSlot 30eb83318128e345d3f24db5bb02ea14 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 7 to pending request PendingRequest{slotRequestId=SlotRequestId{e364086a08fb426225858a132ff59015}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Reserve slot 30eb83318128e345d3f24db5bb02ea14 for slot request id SlotRequestId{e364086a08fb426225858a132ff59015}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Reserve free slot with allocation id 30eb83318128e345d3f24db5bb02ea14.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Matched slot AllocatedSlot 178afb1de8d4b4f843316406dbcc6c67 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 25 to pending request PendingRequest{slotRequestId=SlotRequestId{82011bf2f61eca70362907fa63ea8b39}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9780 ] - [ DEBUG ]  Reserve slot 178afb1de8d4b4f843316406dbcc6c67 for slot request id SlotRequestId{82011bf2f61eca70362907fa63ea8b39}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9781 ] - [ DEBUG ]  Reserve free slot with allocation id 178afb1de8d4b4f843316406dbcc6c67.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9781 ] - [ DEBUG ]  Matched slot AllocatedSlot 36086a0f68b34902fe232fae71009313 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 20 to pending request PendingRequest{slotRequestId=SlotRequestId{1a54978c7b40db6ae94bb73fd91740b1}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9781 ] - [ DEBUG ]  Reserve slot 36086a0f68b34902fe232fae71009313 for slot request id SlotRequestId{1a54978c7b40db6ae94bb73fd91740b1}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9781 ] - [ DEBUG ]  Reserve free slot with allocation id 36086a0f68b34902fe232fae71009313.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9781 ] - [ DEBUG ]  Matched slot AllocatedSlot 43352989c37f53018f9b85386d62156a @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 2 to pending request PendingRequest{slotRequestId=SlotRequestId{bb029117f280c8fbeaf176817245b0ee}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Reserve slot 43352989c37f53018f9b85386d62156a for slot request id SlotRequestId{bb029117f280c8fbeaf176817245b0ee}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Reserve free slot with allocation id 43352989c37f53018f9b85386d62156a.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Matched slot AllocatedSlot 27393d5d22fc71fa3675aa19b69683f2 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 14 to pending request PendingRequest{slotRequestId=SlotRequestId{bb105b4fcf7b451142e05f4d95da2e44}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Reserve slot 27393d5d22fc71fa3675aa19b69683f2 for slot request id SlotRequestId{bb105b4fcf7b451142e05f4d95da2e44}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Reserve free slot with allocation id 27393d5d22fc71fa3675aa19b69683f2.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Matched slot AllocatedSlot 19602b51c9127c777128cba1de2ca8db @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 33 to pending request PendingRequest{slotRequestId=SlotRequestId{a177cc3e8856646080c9b0a9ebee356b}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Reserve slot 19602b51c9127c777128cba1de2ca8db for slot request id SlotRequestId{a177cc3e8856646080c9b0a9ebee356b}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Reserve free slot with allocation id 19602b51c9127c777128cba1de2ca8db.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Matched slot AllocatedSlot 0cf73c7f9304a1056efe98b0f6f517df @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 23 to pending request PendingRequest{slotRequestId=SlotRequestId{286cd3c53d99b2de3bb5899056bdd5ff}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Reserve slot 0cf73c7f9304a1056efe98b0f6f517df for slot request id SlotRequestId{286cd3c53d99b2de3bb5899056bdd5ff}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Reserve free slot with allocation id 0cf73c7f9304a1056efe98b0f6f517df.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Matched slot AllocatedSlot 46827fcd58daec7152008dd28742381a @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 32 to pending request PendingRequest{slotRequestId=SlotRequestId{224f621608445848dc3b736a8729ab01}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Reserve slot 46827fcd58daec7152008dd28742381a for slot request id SlotRequestId{224f621608445848dc3b736a8729ab01}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Reserve free slot with allocation id 46827fcd58daec7152008dd28742381a.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Matched slot AllocatedSlot d4a609ea419f71be34cb5de07aa2a014 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 21 to pending request PendingRequest{slotRequestId=SlotRequestId{0b839e3791d34c9c5051bc9b454f6355}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Reserve slot d4a609ea419f71be34cb5de07aa2a014 for slot request id SlotRequestId{0b839e3791d34c9c5051bc9b454f6355}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9782 ] - [ DEBUG ]  Reserve free slot with allocation id d4a609ea419f71be34cb5de07aa2a014.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9783 ] - [ DEBUG ]  Matched slot AllocatedSlot fdf2ddb6217f04cda389971a9af4a223 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 34 to pending request PendingRequest{slotRequestId=SlotRequestId{81aa6c879e299c9d96bda2f6c37ada42}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9783 ] - [ DEBUG ]  Reserve slot fdf2ddb6217f04cda389971a9af4a223 for slot request id SlotRequestId{81aa6c879e299c9d96bda2f6c37ada42}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9783 ] - [ DEBUG ]  Reserve free slot with allocation id fdf2ddb6217f04cda389971a9af4a223.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9784 ] - [ DEBUG ]  Matched slot AllocatedSlot 9b0bc5af62966b679b2a4a0f2bfa483f @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 4 to pending request PendingRequest{slotRequestId=SlotRequestId{adf445367b8cbe53076e4e4617f1bff5}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9784 ] - [ DEBUG ]  Reserve slot 9b0bc5af62966b679b2a4a0f2bfa483f for slot request id SlotRequestId{adf445367b8cbe53076e4e4617f1bff5}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9784 ] - [ DEBUG ]  Reserve free slot with allocation id 9b0bc5af62966b679b2a4a0f2bfa483f.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9784 ] - [ DEBUG ]  Matched slot AllocatedSlot d0c10e9b833c29c8e13dc6e3b5cd71cb @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 35 to pending request PendingRequest{slotRequestId=SlotRequestId{ed885ac3e126f588dd4c9ac5485bd1fe}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9784 ] - [ DEBUG ]  Reserve slot d0c10e9b833c29c8e13dc6e3b5cd71cb for slot request id SlotRequestId{ed885ac3e126f588dd4c9ac5485bd1fe}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9784 ] - [ DEBUG ]  Reserve free slot with allocation id d0c10e9b833c29c8e13dc6e3b5cd71cb.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9784 ] - [ DEBUG ]  Matched slot AllocatedSlot e513fdd5c4b1f46fd684e15d482ed8d6 @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 1 to pending request PendingRequest{slotRequestId=SlotRequestId{a83a108aa0c1c84bcd49b3a427c9a3d0}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9784 ] - [ DEBUG ]  Reserve slot e513fdd5c4b1f46fd684e15d482ed8d6 for slot request id SlotRequestId{a83a108aa0c1c84bcd49b3a427c9a3d0}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9784 ] - [ DEBUG ]  Reserve free slot with allocation id e513fdd5c4b1f46fd684e15d482ed8d6.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9784 ] - [ DEBUG ]  Matched slot AllocatedSlot 11097abd215e6451f2e95b37fe86699a @ 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) - 17 to pending request PendingRequest{slotRequestId=SlotRequestId{5306c134b4aa837165b67d87a69bf745}, resourceProfile=ResourceProfile{UNKNOWN}, isBatchRequest=true, unfulfillableSince=9223372036854775807}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9784 ] - [ DEBUG ]  Reserve slot 11097abd215e6451f2e95b37fe86699a for slot request id SlotRequestId{5306c134b4aa837165b67d87a69bf745}
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9784 ] - [ DEBUG ]  Reserve free slot with allocation id 11097abd215e6451f2e95b37fe86699a.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9785 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{a1b011e220d017ae09f8162db569f90e}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{5adfd4e53b79c2f2bb0a371295717cc2})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9788 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36) (a804fa908af17cf17211c1700cf07411) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9788 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36) (attempt #0) with attempt id a804fa908af17cf17211c1700cf07411 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 4f34c734ce3549c59d170e43098814ec
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9800 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{68ea29629da9d0637f9c176c92bdd919}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_1) from the physical slot (SlotRequestId{10fa1ddd50cd0a45caf9b78b201855cb})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9802 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36) (4fcda378a5d3eb0b9a9f36477c5ea9bc) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9802 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36) (attempt #0) with attempt id 4fcda378a5d3eb0b9a9f36477c5ea9bc to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 3fa0eff69efa0bd8ffb5d90f845c4693
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9803 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{29e32a8b11282aec1a625cd3afc0bde1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_2) from the physical slot (SlotRequestId{4dc1c6d88ab6df02acfba70daf35b9db})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9803 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36) (93d69acaf74237fbf2c6e56c068e29bc) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9803 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36) (attempt #0) with attempt id 93d69acaf74237fbf2c6e56c068e29bc to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 916a0304e1b3a002e37bce0a5a3d6a91
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9804 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{9a2450a136e971aeda80ee332fb31e22}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_3) from the physical slot (SlotRequestId{acc7d804f0466d7e96ba13c2d16a1649})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9804 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36) (9f4f3e1ecc6ba91aa045b2a561e97ef6) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9804 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36) (attempt #0) with attempt id 9f4f3e1ecc6ba91aa045b2a561e97ef6 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 51ca3690186c6f89309474a25879bbf5
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9806 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{2b9e9dfe1e07aa91cc05428b6a6881b8}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_4) from the physical slot (SlotRequestId{967f9760dfdbad4edc94afc216e9df51})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9806 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36) (3a4a32942dddaf0385d40bde5fc34a86) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9806 ] - [ INFO ]  Activate slot 4f34c734ce3549c59d170e43098814ec.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9808 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36) (attempt #0) with attempt id 3a4a32942dddaf0385d40bde5fc34a86 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 29bff3f54d94023cdd813b9ee98997b1
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9811 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{402d2a8f881b0384ae577292ac830226}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_5) from the physical slot (SlotRequestId{fc90ef3fc45405ced76ddb96cf3a0894})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9812 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36) (eb234fe2b78cf50d28a21c5e8428c8e2) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9812 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36) (attempt #0) with attempt id eb234fe2b78cf50d28a21c5e8428c8e2 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 32204247f2a9dcbb3f64030bc03c896c
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9814 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{94a2c257d86d4ef6d5013b36066e5b12}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_6) from the physical slot (SlotRequestId{1ea124cb7077029de21e80c377d30d95})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9814 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36) (b3f0a8b054e16515fd877f40d8750540) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9814 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36) (attempt #0) with attempt id b3f0a8b054e16515fd877f40d8750540 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id ecf8a41a8ff33e11a5134e31acbb5e89
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9815 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{eb7cd3ec6270905f9862d012cc7c8ac6}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_7) from the physical slot (SlotRequestId{720bde7c0295181f0e9abc3afbec8a3d})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9815 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36) (138b4422f4982380d457251dd8595f35) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9818 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36) (attempt #0) with attempt id 138b4422f4982380d457251dd8595f35 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 275e91a727866e4043757d69ead9e2ab
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9819 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{2895aef5f6ca98b5183f7dfb94c602b1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_8) from the physical slot (SlotRequestId{dc467aa2be8a77fa6a1158e6ebf36357})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9819 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36) (05715cb683fac77f419f61068d36469e) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9819 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36) (attempt #0) with attempt id 05715cb683fac77f419f61068d36469e to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id cd29661e846913336458ae880a376e5b
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9819 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{207012b26687b2f7a54c1d76b1af67e4}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_9) from the physical slot (SlotRequestId{ece7b5e42097f59986901a24616c7f15})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9819 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36) (4d8d3851631b53958f2950b993e951c3) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9819 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36) (attempt #0) with attempt id 4d8d3851631b53958f2950b993e951c3 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 529db54fdf957f05051f2a9705e53dbb
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9819 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{56f60c74544660a6e22955406236e396}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_10) from the physical slot (SlotRequestId{a98da1b687c3faac87f6d6435a460d89})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9820 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36) (ba08ed9470b8b256110a141383e411b8) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9821 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36) (attempt #0) with attempt id ba08ed9470b8b256110a141383e411b8 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 106f0ba9ba38260c4ed7492d2955d689
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9821 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{cac52620136e26478f73acc5336b03a1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_11) from the physical slot (SlotRequestId{cb1beae73ab58ba621436107f4d9612d})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9821 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36) (0f08aa732c67bb70a34b87c43db14906) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9821 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36) (attempt #0) with attempt id 0f08aa732c67bb70a34b87c43db14906 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 96e72c04c02b602173c636ffb4b26af7
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9821 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{b361e4af85c367e201d36ea958ad2b02}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_12) from the physical slot (SlotRequestId{010c7014363b5c3ec680462b1c1f575c})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9821 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36) (bbaa5006ecd904ef625977413144688b) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9821 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36) (attempt #0) with attempt id bbaa5006ecd904ef625977413144688b to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 1c857d3bd2c64719d4eb43e86d804337
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9822 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{c9ec96b1dfea558674365b4112a1925c}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_13) from the physical slot (SlotRequestId{eccc283128f0f3eed6e053d9e1d950d6})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9822 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36) (0e4277a1f6d404bc71bb79fcfbc9769e) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9822 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36) (attempt #0) with attempt id 0e4277a1f6d404bc71bb79fcfbc9769e to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id f47ce2af1de0ed2306225306c3d26dfe
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9822 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{4d3172c6fdaae7c3c39d2b1996eed876}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_14) from the physical slot (SlotRequestId{0c5e59063bb3e277a9d0f534fab0ba40})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9822 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36) (7cbc3523540446665872efda7f69c476) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9822 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36) (attempt #0) with attempt id 7cbc3523540446665872efda7f69c476 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 1115d5439ef2cf0f882dd10b19cda519
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9822 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{732e06267707ec2ea526e1714d2f22b9}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_15) from the physical slot (SlotRequestId{b8c01daecadb045b390d5cd08e8f24a1})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9822 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36) (6e822a8a973db92453d7d0dd1fdc15cc) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9822 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36) (attempt #0) with attempt id 6e822a8a973db92453d7d0dd1fdc15cc to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id c2a6a50e69242d36c1517984ba118d27
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9823 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{6365f315f77b21e01243eeb89fb4cdb3}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_16) from the physical slot (SlotRequestId{b1c63d1b0061d76298fc7f35d5b0ede7})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9823 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36) (4cacc0cf82a95a1a2846442152dd1ebd) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9823 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36) (attempt #0) with attempt id 4cacc0cf82a95a1a2846442152dd1ebd to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 06d1567f24d75f9b5dae99b52084c1a4
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9824 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{26a0ff7770e865f77a3b23191104ad96}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_17) from the physical slot (SlotRequestId{6e6f508e5379df168fe3dd60c9347960})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9824 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36) (0c9a99f0839bbedbc59a320c8e177bf5) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9824 ] - [ DEBUG ]  Registered new allocation id 4f34c734ce3549c59d170e43098814ec for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9824 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36) (attempt #0) with attempt id 0c9a99f0839bbedbc59a320c8e177bf5 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 22a91adbd20099800ddd036173df6f0c
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9825 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{31ec7339a419f014436a70eac127deb9}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_18) from the physical slot (SlotRequestId{8de43a1f4e34194663b91f428427f841})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9825 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36) (ddd59049b3eb442e5b9545c88ad0890a) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9825 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36) (attempt #0) with attempt id ddd59049b3eb442e5b9545c88ad0890a to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id b0ffa6ffd647ebc3dd5e4a07d4f4a3b6
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9825 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{53f9c6e420d79edc49ed7f3f10372616}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_19) from the physical slot (SlotRequestId{52a0eb3a9e348287659c7e3ed057c223})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9825 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36) (9f77d72911b3e515bb1f11ed3d21b925) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9825 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36) (attempt #0) with attempt id 9f77d72911b3e515bb1f11ed3d21b925 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id ad9cd0d0c392e6fd77603f6af858c8d0
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9826 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{d534a2104feb547754940c5495c1e1cb}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_20) from the physical slot (SlotRequestId{067f7b2061a1aaaed945ffafc05f3d80})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9826 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36) (986a109fb0d2fe0a94164c7afedc55e7) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9826 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36) (attempt #0) with attempt id 986a109fb0d2fe0a94164c7afedc55e7 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 4cf541597e0efa5c4c02d18777aa0ff3
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9826 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{8c2a29ea51a46314ec52202736f28999}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_21) from the physical slot (SlotRequestId{839f8d62d5c2595795ef7f6126be583b})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9826 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_4f34c734ce3549c59d170e43098814ec], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=0}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 0 under allocation id 4f34c734ce3549c59d170e43098814ec.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9827 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36) (0a0c6c9c1d45808bef65560eb7afc889) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9827 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36) (attempt #0) with attempt id 0a0c6c9c1d45808bef65560eb7afc889 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id ad3de297d0b909d6b7bc2396ceaa2e26
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9827 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{811b9fb47bac09639d940edcbf92bf68}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_22) from the physical slot (SlotRequestId{e364086a08fb426225858a132ff59015})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9827 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36) (536a25360f375ad9517b80540b08d145) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9827 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36) (attempt #0) with attempt id 536a25360f375ad9517b80540b08d145 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 30eb83318128e345d3f24db5bb02ea14
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9827 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{7cd64f44093cff196e204b560450efc1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_23) from the physical slot (SlotRequestId{82011bf2f61eca70362907fa63ea8b39})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9827 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36) (cd37ec48093b17fb0d6fc7ba3aa250f4) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9827 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36) (attempt #0) with attempt id cd37ec48093b17fb0d6fc7ba3aa250f4 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 178afb1de8d4b4f843316406dbcc6c67
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9827 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{882acd84211011703d7109a63ee98252}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_24) from the physical slot (SlotRequestId{1a54978c7b40db6ae94bb73fd91740b1})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9828 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36) (71d31e42db0cfc094428f464d12688f8) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9829 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36) (attempt #0) with attempt id 71d31e42db0cfc094428f464d12688f8 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 36086a0f68b34902fe232fae71009313
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9829 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{f29d86f0ac8fcfa4d8afb19207159a6e}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_25) from the physical slot (SlotRequestId{bb029117f280c8fbeaf176817245b0ee})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9838 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36) (0180ce402fb520a6d0325667c4c9f55b) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9841 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36) (attempt #0) with attempt id 0180ce402fb520a6d0325667c4c9f55b to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 43352989c37f53018f9b85386d62156a
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9841 ] - [ INFO ]  StateChangelogStorageLoader initialized with shortcut names {memory}.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9864 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{f02a9e1c9c254777c6bc21160561678a}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_26) from the physical slot (SlotRequestId{bb105b4fcf7b451142e05f4d95da2e44})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9880 ] - [ INFO ]  Creating a changelog storage with name 'memory'.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9881 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36) (2f88ff53e91055cd65a3913274b72538) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9881 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36) (attempt #0) with attempt id 2f88ff53e91055cd65a3913274b72538 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 27393d5d22fc71fa3675aa19b69683f2
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9881 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{3f371a2e1cafd1ad33e9bedf3f4c05c1}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_27) from the physical slot (SlotRequestId{a177cc3e8856646080c9b0a9ebee356b})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9882 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36) (a1bb9f26bcffcad198f52a482fb53559) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9882 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36) (attempt #0) with attempt id a1bb9f26bcffcad198f52a482fb53559 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 19602b51c9127c777128cba1de2ca8db
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9882 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{9109655bf279ee91f05c5cd1a652ee51}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_28) from the physical slot (SlotRequestId{286cd3c53d99b2de3bb5899056bdd5ff})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9882 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36) (1eb215c73dfbc39e6702cf343ce8d121) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9882 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36) (attempt #0) with attempt id 1eb215c73dfbc39e6702cf343ce8d121 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 0cf73c7f9304a1056efe98b0f6f517df
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9882 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{a0631b75a63d326f272eea8365caf81c}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_29) from the physical slot (SlotRequestId{224f621608445848dc3b736a8729ab01})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9883 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36) (38f97eb8a1c5de44871786bc3329ddac) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9883 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36) (attempt #0) with attempt id 38f97eb8a1c5de44871786bc3329ddac to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 46827fcd58daec7152008dd28742381a
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9883 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{fb16285e710742184fcdf7bf45a8e828}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_30) from the physical slot (SlotRequestId{0b839e3791d34c9c5051bc9b454f6355})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9883 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36) (737edf5169bbacb604ef4156b6663334) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9883 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36) (attempt #0) with attempt id 737edf5169bbacb604ef4156b6663334 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id d4a609ea419f71be34cb5de07aa2a014
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9883 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{84af3687dc11df61de3007025a9b921f}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_31) from the physical slot (SlotRequestId{81aa6c879e299c9d96bda2f6c37ada42})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9883 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36) (3757ee42c112a6cecf5c7cdc072c08b6) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9883 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36) (attempt #0) with attempt id 3757ee42c112a6cecf5c7cdc072c08b6 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id fdf2ddb6217f04cda389971a9af4a223
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9883 ] - [ DEBUG ]  Registered new state changelog storage for job ba766f85be2130d6661f17297a4a9f8b : org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9883 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{a18f6d7fd9e005a361d5b001261988d0}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_32) from the physical slot (SlotRequestId{adf445367b8cbe53076e4e4617f1bff5})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9883 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36) (c19840797252b89377f7ef7ba4fcf5d4) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9884 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36) (attempt #0) with attempt id c19840797252b89377f7ef7ba4fcf5d4 to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 9b0bc5af62966b679b2a4a0f2bfa483f
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9884 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{cb0f8060b632163d16bb92223b445eac}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_33) from the physical slot (SlotRequestId{ed885ac3e126f588dd4c9ac5485bd1fe})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9884 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36) (4a8bef25bef4f0beec2b6df232b4aefc) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9890 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36) (attempt #0) with attempt id 4a8bef25bef4f0beec2b6df232b4aefc to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id d0c10e9b833c29c8e13dc6e3b5cd71cb
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9892 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{06860c5d9aa3150e66a816ff77c3764a}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_34) from the physical slot (SlotRequestId{a83a108aa0c1c84bcd49b3a427c9a3d0})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9892 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36) (3bae0b19ee60137b874880f7d35cdfca) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9892 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36) (attempt #0) with attempt id 3bae0b19ee60137b874880f7d35cdfca to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id e513fdd5c4b1f46fd684e15d482ed8d6
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9892 ] - [ DEBUG ]  Allocated logical slot (SlotRequestId{5a4b473b8e00f4fb7c7f04e6e1f862e3}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_35) from the physical slot (SlotRequestId{5306c134b4aa837165b67d87a69bf745})
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9892 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36) (18427157dcd5156c436e5dfbf779c7ab) switched from SCHEDULED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:9892 ] - [ INFO ]  Deploying Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36) (attempt #0) with attempt id 18427157dcd5156c436e5dfbf779c7ab to 66527030-9c98-43a4-b1f5-6190c00b39ae @ localhost (dataPort=-1) with allocation id 11097abd215e6451f2e95b37fe86699a
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9917 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0 (a804fa908af17cf17211c1700cf07411), deploy into slot with allocation id 4f34c734ce3549c59d170e43098814ec.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:9918 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0 (a804fa908af17cf17211c1700cf07411) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:9924 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0 (a804fa908af17cf17211c1700cf07411) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:9934 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0 (a804fa908af17cf17211c1700cf07411) [DEPLOYING].
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9935 ] - [ INFO ]  Activate slot 29bff3f54d94023cdd813b9ee98997b1.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:9936 ] - [ DEBUG ]  Getting user code class loader for task a804fa908af17cf17211c1700cf07411 at library cache manager took 1 milliseconds
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:9978 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0 (a804fa908af17cf17211c1700cf07411) [DEPLOYING].
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9980 ] - [ DEBUG ]  Registered new allocation id 29bff3f54d94023cdd813b9ee98997b1 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9981 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_29bff3f54d94023cdd813b9ee98997b1], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=4}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 4 under allocation id 29bff3f54d94023cdd813b9ee98997b1.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9981 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9982 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0 (3a4a32942dddaf0385d40bde5fc34a86), deploy into slot with allocation id 29bff3f54d94023cdd813b9ee98997b1.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:9983 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0 (3a4a32942dddaf0385d40bde5fc34a86) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9988 ] - [ INFO ]  Activate slot 3fa0eff69efa0bd8ffb5d90f845c4693.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:9988 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0 (3a4a32942dddaf0385d40bde5fc34a86) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:9990 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0 (3a4a32942dddaf0385d40bde5fc34a86) [DEPLOYING].
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9993 ] - [ DEBUG ]  Registered new allocation id 3fa0eff69efa0bd8ffb5d90f845c4693 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9994 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_3fa0eff69efa0bd8ffb5d90f845c4693], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=1}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 1 under allocation id 3fa0eff69efa0bd8ffb5d90f845c4693.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9994 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:9996 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0 (4fcda378a5d3eb0b9a9f36477c5ea9bc), deploy into slot with allocation id 3fa0eff69efa0bd8ffb5d90f845c4693.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:9996 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0 (4fcda378a5d3eb0b9a9f36477c5ea9bc) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:9999 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0 (4fcda378a5d3eb0b9a9f36477c5ea9bc) [DEPLOYING]
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10001 ] - [ INFO ]  Activate slot 51ca3690186c6f89309474a25879bbf5.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:10001 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0 (4fcda378a5d3eb0b9a9f36477c5ea9bc) [DEPLOYING].
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10008 ] - [ DEBUG ]  Registered new allocation id 51ca3690186c6f89309474a25879bbf5 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:10008 ] - [ DEBUG ]  Getting user code class loader for task 4fcda378a5d3eb0b9a9f36477c5ea9bc at library cache manager took 5 milliseconds
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:10008 ] - [ DEBUG ]  Getting user code class loader for task 3a4a32942dddaf0385d40bde5fc34a86 at library cache manager took 16 milliseconds
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10009 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_51ca3690186c6f89309474a25879bbf5], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=3}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 3 under allocation id 51ca3690186c6f89309474a25879bbf5.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:10010 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0 (4fcda378a5d3eb0b9a9f36477c5ea9bc) [DEPLOYING].
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10010 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:10014 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0 (3a4a32942dddaf0385d40bde5fc34a86) [DEPLOYING].
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10016 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0 (9f4f3e1ecc6ba91aa045b2a561e97ef6), deploy into slot with allocation id 51ca3690186c6f89309474a25879bbf5.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:10017 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0 (9f4f3e1ecc6ba91aa045b2a561e97ef6) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10021 ] - [ INFO ]  Activate slot 916a0304e1b3a002e37bce0a5a3d6a91.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:10021 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0 (9f4f3e1ecc6ba91aa045b2a561e97ef6) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:10022 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0 (9f4f3e1ecc6ba91aa045b2a561e97ef6) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:10023 ] - [ DEBUG ]  Getting user code class loader for task 9f4f3e1ecc6ba91aa045b2a561e97ef6 at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:10029 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0 (9f4f3e1ecc6ba91aa045b2a561e97ef6) [DEPLOYING].
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10035 ] - [ DEBUG ]  Registered new allocation id 916a0304e1b3a002e37bce0a5a3d6a91 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10035 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_916a0304e1b3a002e37bce0a5a3d6a91], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=2}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 2 under allocation id 916a0304e1b3a002e37bce0a5a3d6a91.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10035 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10036 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0 (93d69acaf74237fbf2c6e56c068e29bc), deploy into slot with allocation id 916a0304e1b3a002e37bce0a5a3d6a91.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:10037 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0 (93d69acaf74237fbf2c6e56c068e29bc) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:10039 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0 (93d69acaf74237fbf2c6e56c068e29bc) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:10040 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0 (93d69acaf74237fbf2c6e56c068e29bc) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:10040 ] - [ DEBUG ]  Getting user code class loader for task 93d69acaf74237fbf2c6e56c068e29bc at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10041 ] - [ INFO ]  Activate slot 32204247f2a9dcbb3f64030bc03c896c.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:10047 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@52b4904d
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:10049 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:10047 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@5aead479
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:10049 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@5a7d4582
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:10049 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@7e3f5ea5
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:10049 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:10050 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:10050 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0 (93d69acaf74237fbf2c6e56c068e29bc) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:10050 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10050 ] - [ DEBUG ]  Registered new allocation id 32204247f2a9dcbb3f64030bc03c896c for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10051 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_32204247f2a9dcbb3f64030bc03c896c], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=5}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 5 under allocation id 32204247f2a9dcbb3f64030bc03c896c.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10051 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:10051 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@f0b983c
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10053 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0 (eb234fe2b78cf50d28a21c5e8428c8e2), deploy into slot with allocation id 32204247f2a9dcbb3f64030bc03c896c.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:10053 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@27fe9332
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:10053 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@51198680
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:10053 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@3b3ddf06
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:10052 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@18652252
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:10055 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:10055 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0 (eb234fe2b78cf50d28a21c5e8428c8e2) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:10058 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0 (eb234fe2b78cf50d28a21c5e8428c8e2) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:10058 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0 (eb234fe2b78cf50d28a21c5e8428c8e2) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:10058 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@23c10315
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:10058 ] - [ DEBUG ]  Getting user code class loader for task eb234fe2b78cf50d28a21c5e8428c8e2 at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10059 ] - [ INFO ]  Activate slot ecf8a41a8ff33e11a5134e31acbb5e89.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:10060 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0 (eb234fe2b78cf50d28a21c5e8428c8e2) [DEPLOYING].
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10061 ] - [ DEBUG ]  Registered new allocation id ecf8a41a8ff33e11a5134e31acbb5e89 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:10061 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@5fe75675
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:10061 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10061 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_ecf8a41a8ff33e11a5134e31acbb5e89], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=6}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 6 under allocation id ecf8a41a8ff33e11a5134e31acbb5e89.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:10061 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@1ae5c3ed
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10061 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10068 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0 (b3f0a8b054e16515fd877f40d8750540), deploy into slot with allocation id ecf8a41a8ff33e11a5134e31acbb5e89.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:10069 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0 (b3f0a8b054e16515fd877f40d8750540) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:10071 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0 (b3f0a8b054e16515fd877f40d8750540) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:10072 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0 (b3f0a8b054e16515fd877f40d8750540) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:10073 ] - [ DEBUG ]  Getting user code class loader for task b3f0a8b054e16515fd877f40d8750540 at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10076 ] - [ INFO ]  Activate slot 275e91a727866e4043757d69ead9e2ab.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:10077 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0 (b3f0a8b054e16515fd877f40d8750540) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:10077 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@a1866dc
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:10077 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:10077 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@e27e347
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10078 ] - [ DEBUG ]  Registered new allocation id 275e91a727866e4043757d69ead9e2ab for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10079 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_275e91a727866e4043757d69ead9e2ab], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=7}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 7 under allocation id 275e91a727866e4043757d69ead9e2ab.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10079 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10080 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0 (138b4422f4982380d457251dd8595f35), deploy into slot with allocation id 275e91a727866e4043757d69ead9e2ab.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:10080 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0 (138b4422f4982380d457251dd8595f35) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:10087 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0 (138b4422f4982380d457251dd8595f35) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:10087 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0 (138b4422f4982380d457251dd8595f35) [DEPLOYING].
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10088 ] - [ INFO ]  Activate slot cd29661e846913336458ae880a376e5b.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:10090 ] - [ DEBUG ]  Getting user code class loader for task 138b4422f4982380d457251dd8595f35 at library cache manager took 3 milliseconds
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:10092 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0 (138b4422f4982380d457251dd8595f35) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:10094 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0 (b3f0a8b054e16515fd877f40d8750540) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:10094 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0 (9f4f3e1ecc6ba91aa045b2a561e97ef6) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:10094 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0 (93d69acaf74237fbf2c6e56c068e29bc) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:10094 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0 (eb234fe2b78cf50d28a21c5e8428c8e2) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:10094 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0 (3a4a32942dddaf0385d40bde5fc34a86) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:10094 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0 (a804fa908af17cf17211c1700cf07411) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:10095 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0 (4fcda378a5d3eb0b9a9f36477c5ea9bc) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:10095 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@2157692e
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:10096 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:10096 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@2568f9dc
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:10097 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0 (138b4422f4982380d457251dd8595f35) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:10099 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:10100 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:10100 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:10100 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:10100 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:10100 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10101 ] - [ DEBUG ]  Registered new allocation id cd29661e846913336458ae880a376e5b for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10102 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_cd29661e846913336458ae880a376e5b], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=8}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 8 under allocation id cd29661e846913336458ae880a376e5b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10102 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:10102 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:10103 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10102 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36) (9f4f3e1ecc6ba91aa045b2a561e97ef6) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10107 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0 (05715cb683fac77f419f61068d36469e), deploy into slot with allocation id cd29661e846913336458ae880a376e5b.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:10109 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0 (05715cb683fac77f419f61068d36469e) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:10111 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0 (05715cb683fac77f419f61068d36469e) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:10112 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0 (05715cb683fac77f419f61068d36469e) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:10112 ] - [ DEBUG ]  Getting user code class loader for task 05715cb683fac77f419f61068d36469e at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:10112 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0 (05715cb683fac77f419f61068d36469e) [DEPLOYING].
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10113 ] - [ INFO ]  Activate slot 529db54fdf957f05051f2a9705e53dbb.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10117 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36) (4fcda378a5d3eb0b9a9f36477c5ea9bc) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10118 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36) (eb234fe2b78cf50d28a21c5e8428c8e2) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10118 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36) (93d69acaf74237fbf2c6e56c068e29bc) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10118 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36) (3a4a32942dddaf0385d40bde5fc34a86) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10118 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36) (a804fa908af17cf17211c1700cf07411) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10118 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36) (b3f0a8b054e16515fd877f40d8750540) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10119 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36) (138b4422f4982380d457251dd8595f35) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:10127 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@2dc7f6a8
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:10127 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:10127 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@29f867c8
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:10132 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0 (05715cb683fac77f419f61068d36469e) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10134 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36) (05715cb683fac77f419f61068d36469e) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:10134 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10145 ] - [ DEBUG ]  Registered new allocation id 529db54fdf957f05051f2a9705e53dbb for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10145 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_529db54fdf957f05051f2a9705e53dbb], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=9}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 9 under allocation id 529db54fdf957f05051f2a9705e53dbb.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10145 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10146 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0 (4d8d3851631b53958f2950b993e951c3), deploy into slot with allocation id 529db54fdf957f05051f2a9705e53dbb.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:10146 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0 (4d8d3851631b53958f2950b993e951c3) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10154 ] - [ INFO ]  Activate slot 106f0ba9ba38260c4ed7492d2955d689.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:10155 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0 (4d8d3851631b53958f2950b993e951c3) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:10155 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0 (4d8d3851631b53958f2950b993e951c3) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:10155 ] - [ DEBUG ]  Getting user code class loader for task 4d8d3851631b53958f2950b993e951c3 at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10160 ] - [ DEBUG ]  Registered new allocation id 106f0ba9ba38260c4ed7492d2955d689 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10160 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_106f0ba9ba38260c4ed7492d2955d689], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=10}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 10 under allocation id 106f0ba9ba38260c4ed7492d2955d689.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10160 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10160 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0 (ba08ed9470b8b256110a141383e411b8), deploy into slot with allocation id 106f0ba9ba38260c4ed7492d2955d689.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10161 ] - [ INFO ]  Activate slot 96e72c04c02b602173c636ffb4b26af7.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:10161 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0 (ba08ed9470b8b256110a141383e411b8) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:10162 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0 (4d8d3851631b53958f2950b993e951c3) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:10177 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0 (ba08ed9470b8b256110a141383e411b8) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:10177 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0 (ba08ed9470b8b256110a141383e411b8) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:10177 ] - [ DEBUG ]  Getting user code class loader for task ba08ed9470b8b256110a141383e411b8 at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10182 ] - [ DEBUG ]  Registered new allocation id 96e72c04c02b602173c636ffb4b26af7 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10182 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_96e72c04c02b602173c636ffb4b26af7], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=11}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 11 under allocation id 96e72c04c02b602173c636ffb4b26af7.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10183 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10183 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0 (0f08aa732c67bb70a34b87c43db14906), deploy into slot with allocation id 96e72c04c02b602173c636ffb4b26af7.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:10183 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0 (0f08aa732c67bb70a34b87c43db14906) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:10193 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0 (0f08aa732c67bb70a34b87c43db14906) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:10194 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0 (0f08aa732c67bb70a34b87c43db14906) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:10194 ] - [ DEBUG ]  Getting user code class loader for task 0f08aa732c67bb70a34b87c43db14906 at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10196 ] - [ INFO ]  Activate slot 1c857d3bd2c64719d4eb43e86d804337.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:10196 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0 (ba08ed9470b8b256110a141383e411b8) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:10196 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0 (0f08aa732c67bb70a34b87c43db14906) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:10201 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@35dc3cd5
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:10202 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:10202 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@31e3beb6
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10205 ] - [ DEBUG ]  Registered new allocation id 1c857d3bd2c64719d4eb43e86d804337 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10205 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_1c857d3bd2c64719d4eb43e86d804337], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=12}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 12 under allocation id 1c857d3bd2c64719d4eb43e86d804337.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10205 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10205 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0 (bbaa5006ecd904ef625977413144688b), deploy into slot with allocation id 1c857d3bd2c64719d4eb43e86d804337.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:10206 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0 (ba08ed9470b8b256110a141383e411b8) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:10216 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0 (bbaa5006ecd904ef625977413144688b) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:10221 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:10222 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0 (bbaa5006ecd904ef625977413144688b) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:10222 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@3ea88432
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10223 ] - [ INFO ]  Activate slot f47ce2af1de0ed2306225306c3d26dfe.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10223 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36) (ba08ed9470b8b256110a141383e411b8) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:10224 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0 (bbaa5006ecd904ef625977413144688b) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:10224 ] - [ DEBUG ]  Getting user code class loader for task bbaa5006ecd904ef625977413144688b at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:10224 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:10225 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@230d64a7
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:10225 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:10225 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@3a97073f
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:10226 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@2732e41e
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:10226 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0 (0f08aa732c67bb70a34b87c43db14906) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:10227 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0 (bbaa5006ecd904ef625977413144688b) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:10227 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:10227 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0 (4d8d3851631b53958f2950b993e951c3) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10228 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36) (0f08aa732c67bb70a34b87c43db14906) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10228 ] - [ DEBUG ]  Registered new allocation id f47ce2af1de0ed2306225306c3d26dfe for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:10228 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:10228 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@66107ed3
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:10228 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10228 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_f47ce2af1de0ed2306225306c3d26dfe], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=13}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 13 under allocation id f47ce2af1de0ed2306225306c3d26dfe.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10229 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10229 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0 (0e4277a1f6d404bc71bb79fcfbc9769e), deploy into slot with allocation id f47ce2af1de0ed2306225306c3d26dfe.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:10229 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0 (0e4277a1f6d404bc71bb79fcfbc9769e) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10249 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36) (4d8d3851631b53958f2950b993e951c3) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10247 ] - [ INFO ]  Activate slot 1115d5439ef2cf0f882dd10b19cda519.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:10237 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@622ff6c9
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:10252 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0 (bbaa5006ecd904ef625977413144688b) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:10253 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0 (0e4277a1f6d404bc71bb79fcfbc9769e) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:10253 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0 (0e4277a1f6d404bc71bb79fcfbc9769e) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:10253 ] - [ DEBUG ]  Getting user code class loader for task 0e4277a1f6d404bc71bb79fcfbc9769e at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10253 ] - [ DEBUG ]  Registered new allocation id 1115d5439ef2cf0f882dd10b19cda519 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10253 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_1115d5439ef2cf0f882dd10b19cda519], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=14}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 14 under allocation id 1115d5439ef2cf0f882dd10b19cda519.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10253 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:10253 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0 (0e4277a1f6d404bc71bb79fcfbc9769e) [DEPLOYING].
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-9:10253 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0 (7cbc3523540446665872efda7f69c476), deploy into slot with allocation id 1115d5439ef2cf0f882dd10b19cda519.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:10254 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@28c36a79
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:10254 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:10254 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0 (7cbc3523540446665872efda7f69c476) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:10254 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@2cfcfbb9
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:10261 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0 (7cbc3523540446665872efda7f69c476) [DEPLOYING]
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-5:10261 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36) (bbaa5006ecd904ef625977413144688b) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:10261 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:10262 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0 (7cbc3523540446665872efda7f69c476) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:10262 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0 (0e4277a1f6d404bc71bb79fcfbc9769e) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:10262 ] - [ DEBUG ]  Getting user code class loader for task 7cbc3523540446665872efda7f69c476 at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:10266 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0 (7cbc3523540446665872efda7f69c476) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:10267 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:10267 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@6691c657
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:10267 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:10267 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@62ae4e5d
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:10267 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0 (7cbc3523540446665872efda7f69c476) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:10267 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10269 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36) (0e4277a1f6d404bc71bb79fcfbc9769e) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10269 ] - [ INFO ]  Activate slot c2a6a50e69242d36c1517984ba118d27.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10281 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36) (7cbc3523540446665872efda7f69c476) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10290 ] - [ DEBUG ]  Registered new allocation id c2a6a50e69242d36c1517984ba118d27 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10295 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_c2a6a50e69242d36c1517984ba118d27], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=15}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 15 under allocation id c2a6a50e69242d36c1517984ba118d27.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10295 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10302 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0 (6e822a8a973db92453d7d0dd1fdc15cc), deploy into slot with allocation id c2a6a50e69242d36c1517984ba118d27.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:10302 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0 (6e822a8a973db92453d7d0dd1fdc15cc) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:10331 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0 (6e822a8a973db92453d7d0dd1fdc15cc) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:10332 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0 (6e822a8a973db92453d7d0dd1fdc15cc) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:10332 ] - [ DEBUG ]  Getting user code class loader for task 6e822a8a973db92453d7d0dd1fdc15cc at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10315 ] - [ INFO ]  Activate slot 06d1567f24d75f9b5dae99b52084c1a4.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:10333 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0 (6e822a8a973db92453d7d0dd1fdc15cc) [DEPLOYING].
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10335 ] - [ DEBUG ]  Registered new allocation id 06d1567f24d75f9b5dae99b52084c1a4 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10335 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_06d1567f24d75f9b5dae99b52084c1a4], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=16}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 16 under allocation id 06d1567f24d75f9b5dae99b52084c1a4.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10335 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10335 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0 (4cacc0cf82a95a1a2846442152dd1ebd), deploy into slot with allocation id 06d1567f24d75f9b5dae99b52084c1a4.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:10336 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0 (4cacc0cf82a95a1a2846442152dd1ebd) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10336 ] - [ INFO ]  Activate slot 22a91adbd20099800ddd036173df6f0c.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:10348 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0 (4cacc0cf82a95a1a2846442152dd1ebd) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:10349 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0 (4cacc0cf82a95a1a2846442152dd1ebd) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:10349 ] - [ DEBUG ]  Getting user code class loader for task 4cacc0cf82a95a1a2846442152dd1ebd at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:10349 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@2db294cd
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:10349 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:10349 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@68b00f1f
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:10350 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0 (6e822a8a973db92453d7d0dd1fdc15cc) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:10350 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0 (4cacc0cf82a95a1a2846442152dd1ebd) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:10350 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:10352 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@3f9b1c50
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10352 ] - [ DEBUG ]  Registered new allocation id 22a91adbd20099800ddd036173df6f0c for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10352 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_22a91adbd20099800ddd036173df6f0c], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=17}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 17 under allocation id 22a91adbd20099800ddd036173df6f0c.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10352 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:10353 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:10353 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@3ba49f7b
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:10353 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0 (4cacc0cf82a95a1a2846442152dd1ebd) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:10353 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10355 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0 (0c9a99f0839bbedbc59a320c8e177bf5), deploy into slot with allocation id 22a91adbd20099800ddd036173df6f0c.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:10356 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0 (0c9a99f0839bbedbc59a320c8e177bf5) switched from CREATED to DEPLOYING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10358 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36) (6e822a8a973db92453d7d0dd1fdc15cc) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10358 ] - [ INFO ]  Activate slot b0ffa6ffd647ebc3dd5e4a07d4f4a3b6.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:10359 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0 (0c9a99f0839bbedbc59a320c8e177bf5) [DEPLOYING]
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:10362 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0 (0c9a99f0839bbedbc59a320c8e177bf5) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:10362 ] - [ DEBUG ]  Getting user code class loader for task 0c9a99f0839bbedbc59a320c8e177bf5 at library cache manager took 0 milliseconds
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-7:10362 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36) (4cacc0cf82a95a1a2846442152dd1ebd) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:10365 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0 (0c9a99f0839bbedbc59a320c8e177bf5) [DEPLOYING].
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:10365 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@56e93bee
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:10366 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:10366 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@52d6a5eb
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:10366 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0 (0c9a99f0839bbedbc59a320c8e177bf5) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10367 ] - [ DEBUG ]  Registered new allocation id b0ffa6ffd647ebc3dd5e4a07d4f4a3b6 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10367 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_b0ffa6ffd647ebc3dd5e4a07d4f4a3b6], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=18}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 18 under allocation id b0ffa6ffd647ebc3dd5e4a07d4f4a3b6.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10367 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:05  [ flink-akka.actor.default-dispatcher-8:10370 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0 (ddd59049b3eb442e5b9545c88ad0890a), deploy into slot with allocation id b0ffa6ffd647ebc3dd5e4a07d4f4a3b6.
2023-03-31 11:26:05  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:10370 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0 (ddd59049b3eb442e5b9545c88ad0890a) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:10377 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0 (ddd59049b3eb442e5b9545c88ad0890a) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:10378 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0 (ddd59049b3eb442e5b9545c88ad0890a) [DEPLOYING].
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10378 ] - [ INFO ]  Activate slot ad9cd0d0c392e6fd77603f6af858c8d0.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:10378 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:10378 ] - [ DEBUG ]  Getting user code class loader for task ddd59049b3eb442e5b9545c88ad0890a at library cache manager took 0 milliseconds
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-7:10378 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36) (0c9a99f0839bbedbc59a320c8e177bf5) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:10384 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0 (ddd59049b3eb442e5b9545c88ad0890a) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:10392 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@bea6221
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:10392 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:10392 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@fd5e784
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:10392 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0 (ddd59049b3eb442e5b9545c88ad0890a) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10393 ] - [ DEBUG ]  Registered new allocation id ad9cd0d0c392e6fd77603f6af858c8d0 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10393 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_ad9cd0d0c392e6fd77603f6af858c8d0], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=19}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 19 under allocation id ad9cd0d0c392e6fd77603f6af858c8d0.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10393 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10393 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0 (9f77d72911b3e515bb1f11ed3d21b925), deploy into slot with allocation id ad9cd0d0c392e6fd77603f6af858c8d0.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:10394 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-5:10394 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36) (ddd59049b3eb442e5b9545c88ad0890a) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10395 ] - [ INFO ]  Activate slot 4cf541597e0efa5c4c02d18777aa0ff3.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:10395 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0 (9f77d72911b3e515bb1f11ed3d21b925) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:10401 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0 (9f77d72911b3e515bb1f11ed3d21b925) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:10401 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0 (9f77d72911b3e515bb1f11ed3d21b925) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:10402 ] - [ DEBUG ]  Getting user code class loader for task 9f77d72911b3e515bb1f11ed3d21b925 at library cache manager took 0 milliseconds
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10402 ] - [ DEBUG ]  Registered new allocation id 4cf541597e0efa5c4c02d18777aa0ff3 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10402 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_4cf541597e0efa5c4c02d18777aa0ff3], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=20}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 20 under allocation id 4cf541597e0efa5c4c02d18777aa0ff3.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10402 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10402 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0 (986a109fb0d2fe0a94164c7afedc55e7), deploy into slot with allocation id 4cf541597e0efa5c4c02d18777aa0ff3.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:10403 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0 (986a109fb0d2fe0a94164c7afedc55e7) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:10410 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0 (986a109fb0d2fe0a94164c7afedc55e7) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:10410 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0 (9f77d72911b3e515bb1f11ed3d21b925) [DEPLOYING].
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10410 ] - [ INFO ]  Activate slot ad3de297d0b909d6b7bc2396ceaa2e26.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:10410 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0 (986a109fb0d2fe0a94164c7afedc55e7) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:10411 ] - [ DEBUG ]  Getting user code class loader for task 986a109fb0d2fe0a94164c7afedc55e7 at library cache manager took 0 milliseconds
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:10411 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@7f50b20d
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:10411 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:10411 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@7b6f393f
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:10411 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0 (9f77d72911b3e515bb1f11ed3d21b925) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:10411 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0 (986a109fb0d2fe0a94164c7afedc55e7) [DEPLOYING].
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10411 ] - [ DEBUG ]  Registered new allocation id ad3de297d0b909d6b7bc2396ceaa2e26 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-7:10411 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36) (9f77d72911b3e515bb1f11ed3d21b925) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:10411 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:10412 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@30ccde16
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:10412 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10412 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_ad3de297d0b909d6b7bc2396ceaa2e26], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=21}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 21 under allocation id ad3de297d0b909d6b7bc2396ceaa2e26.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10412 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:10412 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@34f35cd7
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:10412 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0 (986a109fb0d2fe0a94164c7afedc55e7) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:10412 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-7:10412 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36) (986a109fb0d2fe0a94164c7afedc55e7) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10413 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0 (0a0c6c9c1d45808bef65560eb7afc889), deploy into slot with allocation id ad3de297d0b909d6b7bc2396ceaa2e26.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:10413 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0 (0a0c6c9c1d45808bef65560eb7afc889) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10420 ] - [ INFO ]  Activate slot 30eb83318128e345d3f24db5bb02ea14.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:10422 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0 (0a0c6c9c1d45808bef65560eb7afc889) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:10422 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0 (0a0c6c9c1d45808bef65560eb7afc889) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:10422 ] - [ DEBUG ]  Getting user code class loader for task 0a0c6c9c1d45808bef65560eb7afc889 at library cache manager took 0 milliseconds
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10424 ] - [ DEBUG ]  Registered new allocation id 30eb83318128e345d3f24db5bb02ea14 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10424 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_30eb83318128e345d3f24db5bb02ea14], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=22}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 22 under allocation id 30eb83318128e345d3f24db5bb02ea14.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10424 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:10424 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0 (0a0c6c9c1d45808bef65560eb7afc889) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:10425 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@10415071
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:10425 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:10425 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@147ec6a1
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:10425 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0 (0a0c6c9c1d45808bef65560eb7afc889) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10425 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0 (536a25360f375ad9517b80540b08d145), deploy into slot with allocation id 30eb83318128e345d3f24db5bb02ea14.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:10425 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0 (536a25360f375ad9517b80540b08d145) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-7:10431 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36) (0a0c6c9c1d45808bef65560eb7afc889) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:10431 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:10434 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0 (536a25360f375ad9517b80540b08d145) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:10434 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0 (536a25360f375ad9517b80540b08d145) [DEPLOYING].
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10434 ] - [ INFO ]  Activate slot 178afb1de8d4b4f843316406dbcc6c67.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:10434 ] - [ DEBUG ]  Getting user code class loader for task 536a25360f375ad9517b80540b08d145 at library cache manager took 0 milliseconds
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:10435 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0 (536a25360f375ad9517b80540b08d145) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:10435 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@6f923fba
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:10435 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:10435 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@6bada690
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:10435 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0 (536a25360f375ad9517b80540b08d145) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-7:10435 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36) (536a25360f375ad9517b80540b08d145) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10435 ] - [ DEBUG ]  Registered new allocation id 178afb1de8d4b4f843316406dbcc6c67 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10436 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_178afb1de8d4b4f843316406dbcc6c67], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=23}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 23 under allocation id 178afb1de8d4b4f843316406dbcc6c67.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10436 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:10436 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10437 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0 (cd37ec48093b17fb0d6fc7ba3aa250f4), deploy into slot with allocation id 178afb1de8d4b4f843316406dbcc6c67.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10437 ] - [ INFO ]  Activate slot 36086a0f68b34902fe232fae71009313.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:10437 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0 (cd37ec48093b17fb0d6fc7ba3aa250f4) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:10446 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0 (cd37ec48093b17fb0d6fc7ba3aa250f4) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:10447 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0 (cd37ec48093b17fb0d6fc7ba3aa250f4) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:10448 ] - [ DEBUG ]  Getting user code class loader for task cd37ec48093b17fb0d6fc7ba3aa250f4 at library cache manager took 1 milliseconds
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:10526 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0 (cd37ec48093b17fb0d6fc7ba3aa250f4) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:10528 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@7832c2b0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:10528 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:10528 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@7c0d496e
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:10529 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0 (cd37ec48093b17fb0d6fc7ba3aa250f4) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10530 ] - [ DEBUG ]  Registered new allocation id 36086a0f68b34902fe232fae71009313 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-9:10530 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36) (cd37ec48093b17fb0d6fc7ba3aa250f4) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:10530 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10530 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_36086a0f68b34902fe232fae71009313], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=24}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 24 under allocation id 36086a0f68b34902fe232fae71009313.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10530 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10531 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0 (71d31e42db0cfc094428f464d12688f8), deploy into slot with allocation id 36086a0f68b34902fe232fae71009313.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:10532 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0 (71d31e42db0cfc094428f464d12688f8) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:10537 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0 (71d31e42db0cfc094428f464d12688f8) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:10539 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0 (71d31e42db0cfc094428f464d12688f8) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:10543 ] - [ DEBUG ]  Getting user code class loader for task 71d31e42db0cfc094428f464d12688f8 at library cache manager took 4 milliseconds
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:10544 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0 (71d31e42db0cfc094428f464d12688f8) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:10544 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@23d86a5e
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:10544 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:10544 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@27e7eafd
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:10544 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0 (71d31e42db0cfc094428f464d12688f8) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:10544 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-9:10556 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36) (71d31e42db0cfc094428f464d12688f8) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10571 ] - [ INFO ]  Activate slot 43352989c37f53018f9b85386d62156a.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10721 ] - [ DEBUG ]  Registered new allocation id 43352989c37f53018f9b85386d62156a for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10749 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_43352989c37f53018f9b85386d62156a], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=25}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 25 under allocation id 43352989c37f53018f9b85386d62156a.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10775 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10777 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0 (0180ce402fb520a6d0325667c4c9f55b), deploy into slot with allocation id 43352989c37f53018f9b85386d62156a.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:10778 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0 (0180ce402fb520a6d0325667c4c9f55b) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:10781 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0 (0180ce402fb520a6d0325667c4c9f55b) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:10781 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0 (0180ce402fb520a6d0325667c4c9f55b) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:10781 ] - [ DEBUG ]  Getting user code class loader for task 0180ce402fb520a6d0325667c4c9f55b at library cache manager took 0 milliseconds
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:10809 ] - [ INFO ]  Activate slot 27393d5d22fc71fa3675aa19b69683f2.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:10943 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0 (0180ce402fb520a6d0325667c4c9f55b) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:10946 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@38020d83
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:10946 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:10946 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@3c3d8e5b
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:10947 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0 (0180ce402fb520a6d0325667c4c9f55b) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:10949 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-7:10950 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36) (0180ce402fb520a6d0325667c4c9f55b) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:11018 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:11024 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:11027 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:11026 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:11024 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:11022 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:11018 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:11018 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:11051 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:11051 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11056 ] - [ DEBUG ]  Registered new allocation id 27393d5d22fc71fa3675aa19b69683f2 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11067 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_27393d5d22fc71fa3675aa19b69683f2], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=26}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 26 under allocation id 27393d5d22fc71fa3675aa19b69683f2.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11070 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11072 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0 (2f88ff53e91055cd65a3913274b72538), deploy into slot with allocation id 27393d5d22fc71fa3675aa19b69683f2.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11073 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0 (2f88ff53e91055cd65a3913274b72538) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11079 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0 (2f88ff53e91055cd65a3913274b72538) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11079 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0 (2f88ff53e91055cd65a3913274b72538) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11079 ] - [ DEBUG ]  Getting user code class loader for task 2f88ff53e91055cd65a3913274b72538 at library cache manager took 0 milliseconds
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11080 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0 (2f88ff53e91055cd65a3913274b72538) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:11080 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:11083 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:11087 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:11087 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:11085 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:11084 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11084 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@30bba675
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:11083 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11131 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11131 ] - [ INFO ]  Activate slot 19602b51c9127c777128cba1de2ca8db.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:11131 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:11130 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:11109 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:11101 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:11094 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:11148 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11137 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@348424ba
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:11155 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11155 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0 (2f88ff53e91055cd65a3913274b72538) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11156 ] - [ DEBUG ]  Registered new allocation id 19602b51c9127c777128cba1de2ca8db for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-9:11157 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36) (2f88ff53e91055cd65a3913274b72538) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11157 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_19602b51c9127c777128cba1de2ca8db], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=27}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 27 under allocation id 19602b51c9127c777128cba1de2ca8db.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11157 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11162 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0 (a1bb9f26bcffcad198f52a482fb53559), deploy into slot with allocation id 19602b51c9127c777128cba1de2ca8db.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11157 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11162 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0 (a1bb9f26bcffcad198f52a482fb53559) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11173 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0 (a1bb9f26bcffcad198f52a482fb53559) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11174 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0 (a1bb9f26bcffcad198f52a482fb53559) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11175 ] - [ DEBUG ]  Getting user code class loader for task a1bb9f26bcffcad198f52a482fb53559 at library cache manager took 0 milliseconds
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11177 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0 (a1bb9f26bcffcad198f52a482fb53559) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11177 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@226e9238
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11178 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11178 ] - [ INFO ]  Activate slot 0cf73c7f9304a1056efe98b0f6f517df.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11204 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@265112ad
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11205 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0 (a1bb9f26bcffcad198f52a482fb53559) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11205 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-7:11205 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36) (a1bb9f26bcffcad198f52a482fb53559) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:11211 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11212 ] - [ DEBUG ]  Registered new allocation id 0cf73c7f9304a1056efe98b0f6f517df for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11212 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_0cf73c7f9304a1056efe98b0f6f517df], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=28}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 28 under allocation id 0cf73c7f9304a1056efe98b0f6f517df.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11212 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11212 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0 (1eb215c73dfbc39e6702cf343ce8d121), deploy into slot with allocation id 0cf73c7f9304a1056efe98b0f6f517df.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11213 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0 (1eb215c73dfbc39e6702cf343ce8d121) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:11219 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11220 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0 (1eb215c73dfbc39e6702cf343ce8d121) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11220 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0 (1eb215c73dfbc39e6702cf343ce8d121) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11220 ] - [ DEBUG ]  Getting user code class loader for task 1eb215c73dfbc39e6702cf343ce8d121 at library cache manager took 0 milliseconds
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11220 ] - [ INFO ]  Activate slot 46827fcd58daec7152008dd28742381a.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11221 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0 (1eb215c73dfbc39e6702cf343ce8d121) [DEPLOYING].
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11222 ] - [ DEBUG ]  Registered new allocation id 46827fcd58daec7152008dd28742381a for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11222 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@3317089b
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11222 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_46827fcd58daec7152008dd28742381a], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=29}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 29 under allocation id 46827fcd58daec7152008dd28742381a.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11223 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11223 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@37288a21
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11223 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0 (1eb215c73dfbc39e6702cf343ce8d121) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11223 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11224 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0 (38f97eb8a1c5de44871786bc3329ddac), deploy into slot with allocation id 46827fcd58daec7152008dd28742381a.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11224 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0 (38f97eb8a1c5de44871786bc3329ddac) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11232 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11233 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0 (38f97eb8a1c5de44871786bc3329ddac) [DEPLOYING]
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-7:11233 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36) (1eb215c73dfbc39e6702cf343ce8d121) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11234 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0 (38f97eb8a1c5de44871786bc3329ddac) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11235 ] - [ DEBUG ]  Getting user code class loader for task 38f97eb8a1c5de44871786bc3329ddac at library cache manager took 1 milliseconds
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11236 ] - [ INFO ]  Activate slot d4a609ea419f71be34cb5de07aa2a014.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11237 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0 (38f97eb8a1c5de44871786bc3329ddac) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11237 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@20b733ea
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11237 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11238 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@2488b324
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11243 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0 (38f97eb8a1c5de44871786bc3329ddac) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11246 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-7:11246 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36) (38f97eb8a1c5de44871786bc3329ddac) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11254 ] - [ DEBUG ]  Registered new allocation id d4a609ea419f71be34cb5de07aa2a014 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11254 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_d4a609ea419f71be34cb5de07aa2a014], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=30}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 30 under allocation id d4a609ea419f71be34cb5de07aa2a014.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11254 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11254 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0 (737edf5169bbacb604ef4156b6663334), deploy into slot with allocation id d4a609ea419f71be34cb5de07aa2a014.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11255 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0 (737edf5169bbacb604ef4156b6663334) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11255 ] - [ INFO ]  Activate slot fdf2ddb6217f04cda389971a9af4a223.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11256 ] - [ DEBUG ]  Registered new allocation id fdf2ddb6217f04cda389971a9af4a223 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11256 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_fdf2ddb6217f04cda389971a9af4a223], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=31}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 31 under allocation id fdf2ddb6217f04cda389971a9af4a223.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11256 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11256 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0 (3757ee42c112a6cecf5c7cdc072c08b6), deploy into slot with allocation id fdf2ddb6217f04cda389971a9af4a223.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11257 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0 (3757ee42c112a6cecf5c7cdc072c08b6) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11257 ] - [ INFO ]  Activate slot 9b0bc5af62966b679b2a4a0f2bfa483f.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11258 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0 (3757ee42c112a6cecf5c7cdc072c08b6) [DEPLOYING]
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11258 ] - [ DEBUG ]  Registered new allocation id 9b0bc5af62966b679b2a4a0f2bfa483f for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11258 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0 (3757ee42c112a6cecf5c7cdc072c08b6) [DEPLOYING].
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11258 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_9b0bc5af62966b679b2a4a0f2bfa483f], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=32}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 32 under allocation id 9b0bc5af62966b679b2a4a0f2bfa483f.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11259 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11259 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0 (737edf5169bbacb604ef4156b6663334) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11259 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0 (737edf5169bbacb604ef4156b6663334) [DEPLOYING].
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11259 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0 (c19840797252b89377f7ef7ba4fcf5d4), deploy into slot with allocation id 9b0bc5af62966b679b2a4a0f2bfa483f.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11259 ] - [ DEBUG ]  Getting user code class loader for task 3757ee42c112a6cecf5c7cdc072c08b6 at library cache manager took 0 milliseconds
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11259 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0 (c19840797252b89377f7ef7ba4fcf5d4) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11259 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0 (c19840797252b89377f7ef7ba4fcf5d4) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11259 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0 (c19840797252b89377f7ef7ba4fcf5d4) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11260 ] - [ DEBUG ]  Getting user code class loader for task c19840797252b89377f7ef7ba4fcf5d4 at library cache manager took 0 milliseconds
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11259 ] - [ INFO ]  Activate slot d0c10e9b833c29c8e13dc6e3b5cd71cb.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11259 ] - [ DEBUG ]  Getting user code class loader for task 737edf5169bbacb604ef4156b6663334 at library cache manager took 0 milliseconds
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11259 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0 (3757ee42c112a6cecf5c7cdc072c08b6) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11261 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0 (c19840797252b89377f7ef7ba4fcf5d4) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11266 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@471056f7
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11267 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11266 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@226d4106
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11267 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@432fdacd
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11267 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0 (c19840797252b89377f7ef7ba4fcf5d4) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11267 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-9:11268 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36) (c19840797252b89377f7ef7ba4fcf5d4) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11268 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11268 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0 (737edf5169bbacb604ef4156b6663334) [DEPLOYING].
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11270 ] - [ DEBUG ]  Registered new allocation id d0c10e9b833c29c8e13dc6e3b5cd71cb for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11270 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@2652c193
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11272 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@4206dc27
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11272 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11272 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_d0c10e9b833c29c8e13dc6e3b5cd71cb], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=33}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 33 under allocation id d0c10e9b833c29c8e13dc6e3b5cd71cb.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11272 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0 (3757ee42c112a6cecf5c7cdc072c08b6) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11272 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11272 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11273 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0 (4a8bef25bef4f0beec2b6df232b4aefc), deploy into slot with allocation id d0c10e9b833c29c8e13dc6e3b5cd71cb.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-9:11273 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36) (3757ee42c112a6cecf5c7cdc072c08b6) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11272 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@463940bf
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11273 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0 (737edf5169bbacb604ef4156b6663334) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11273 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-7:11273 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36) (737edf5169bbacb604ef4156b6663334) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11274 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0 (4a8bef25bef4f0beec2b6df232b4aefc) switched from CREATED to DEPLOYING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11274 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0 (4a8bef25bef4f0beec2b6df232b4aefc) [DEPLOYING]
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11275 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0 (4a8bef25bef4f0beec2b6df232b4aefc) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11277 ] - [ DEBUG ]  Getting user code class loader for task 4a8bef25bef4f0beec2b6df232b4aefc at library cache manager took 2 milliseconds
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11284 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0 (4a8bef25bef4f0beec2b6df232b4aefc) [DEPLOYING].
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11284 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@63cc23d6
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11284 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11284 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@67f3bb77
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11284 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0 (4a8bef25bef4f0beec2b6df232b4aefc) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11284 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-9:11285 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36) (4a8bef25bef4f0beec2b6df232b4aefc) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11310 ] - [ INFO ]  Activate slot 4f34c734ce3549c59d170e43098814ec.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11324 ] - [ INFO ]  Activate slot 3fa0eff69efa0bd8ffb5d90f845c4693.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11325 ] - [ INFO ]  Activate slot 916a0304e1b3a002e37bce0a5a3d6a91.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11325 ] - [ INFO ]  Activate slot 51ca3690186c6f89309474a25879bbf5.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11325 ] - [ INFO ]  Activate slot 29bff3f54d94023cdd813b9ee98997b1.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11325 ] - [ INFO ]  Activate slot 32204247f2a9dcbb3f64030bc03c896c.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11325 ] - [ INFO ]  Activate slot ecf8a41a8ff33e11a5134e31acbb5e89.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11329 ] - [ INFO ]  Activate slot 275e91a727866e4043757d69ead9e2ab.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11329 ] - [ INFO ]  Activate slot cd29661e846913336458ae880a376e5b.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11329 ] - [ INFO ]  Activate slot 529db54fdf957f05051f2a9705e53dbb.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11329 ] - [ INFO ]  Activate slot 106f0ba9ba38260c4ed7492d2955d689.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11329 ] - [ INFO ]  Activate slot 96e72c04c02b602173c636ffb4b26af7.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11329 ] - [ INFO ]  Activate slot 1c857d3bd2c64719d4eb43e86d804337.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11329 ] - [ INFO ]  Activate slot f47ce2af1de0ed2306225306c3d26dfe.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11329 ] - [ INFO ]  Activate slot 1115d5439ef2cf0f882dd10b19cda519.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11329 ] - [ INFO ]  Activate slot c2a6a50e69242d36c1517984ba118d27.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11329 ] - [ INFO ]  Activate slot 06d1567f24d75f9b5dae99b52084c1a4.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11330 ] - [ INFO ]  Activate slot 22a91adbd20099800ddd036173df6f0c.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11330 ] - [ INFO ]  Activate slot b0ffa6ffd647ebc3dd5e4a07d4f4a3b6.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11339 ] - [ INFO ]  Activate slot ad9cd0d0c392e6fd77603f6af858c8d0.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11339 ] - [ INFO ]  Activate slot 4cf541597e0efa5c4c02d18777aa0ff3.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11339 ] - [ INFO ]  Activate slot ad3de297d0b909d6b7bc2396ceaa2e26.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11339 ] - [ INFO ]  Activate slot 30eb83318128e345d3f24db5bb02ea14.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11339 ] - [ INFO ]  Activate slot 178afb1de8d4b4f843316406dbcc6c67.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11339 ] - [ INFO ]  Activate slot 36086a0f68b34902fe232fae71009313.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11339 ] - [ INFO ]  Activate slot 43352989c37f53018f9b85386d62156a.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11340 ] - [ INFO ]  Activate slot 27393d5d22fc71fa3675aa19b69683f2.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11340 ] - [ INFO ]  Activate slot 19602b51c9127c777128cba1de2ca8db.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11340 ] - [ INFO ]  Activate slot 0cf73c7f9304a1056efe98b0f6f517df.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11340 ] - [ INFO ]  Activate slot 46827fcd58daec7152008dd28742381a.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11340 ] - [ INFO ]  Activate slot d4a609ea419f71be34cb5de07aa2a014.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11340 ] - [ INFO ]  Activate slot fdf2ddb6217f04cda389971a9af4a223.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11340 ] - [ INFO ]  Activate slot 9b0bc5af62966b679b2a4a0f2bfa483f.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11340 ] - [ INFO ]  Activate slot d0c10e9b833c29c8e13dc6e3b5cd71cb.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11340 ] - [ INFO ]  Activate slot e513fdd5c4b1f46fd684e15d482ed8d6.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11340 ] - [ INFO ]  Activate slot 11097abd215e6451f2e95b37fe86699a.
2023-03-31 11:26:06  [ flink-akka.actor.default-dispatcher-8:11340 ] - [ INFO ]  Activate slot e513fdd5c4b1f46fd684e15d482ed8d6.
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:11345 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:11347 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:11347 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:11347 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:11347 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:11347 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:11347 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:11347 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:11347 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:11347 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:11348 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:11348 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:11348 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:11348 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:11349 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:11349 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:11349 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:11349 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:11349 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:11349 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:11349 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:11349 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:11349 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:11349 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:11349 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0
2023-03-31 11:26:06  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:11349 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11410 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:11418 ] - [ DEBUG ]  Registered new allocation id e513fdd5c4b1f46fd684e15d482ed8d6 for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:11419 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_e513fdd5c4b1f46fd684e15d482ed8d6], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=34}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 34 under allocation id e513fdd5c4b1f46fd684e15d482ed8d6.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:11419 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:11421 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0 (3bae0b19ee60137b874880f7d35cdfca), deploy into slot with allocation id e513fdd5c4b1f46fd684e15d482ed8d6.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11421 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0 (3bae0b19ee60137b874880f7d35cdfca) switched from CREATED to DEPLOYING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11423 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0 (3bae0b19ee60137b874880f7d35cdfca) [DEPLOYING]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11423 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0 (3bae0b19ee60137b874880f7d35cdfca) [DEPLOYING].
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11423 ] - [ DEBUG ]  Getting user code class loader for task 3bae0b19ee60137b874880f7d35cdfca at library cache manager took 0 milliseconds
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11429 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0 (3bae0b19ee60137b874880f7d35cdfca) [DEPLOYING].
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:11429 ] - [ INFO ]  Activate slot 11097abd215e6451f2e95b37fe86699a.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:11435 ] - [ DEBUG ]  Registered new allocation id 11097abd215e6451f2e95b37fe86699a for local state stores for job ba766f85be2130d6661f17297a4a9f8b.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:11435 ] - [ DEBUG ]  Registered new local state store with configuration LocalRecoveryConfig{localRecoveryMode=false, localStateDirectories=LocalRecoveryDirectoryProvider{rootDirectories=[/var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/localState/aid_11097abd215e6451f2e95b37fe86699a], jobID=ba766f85be2130d6661f17297a4a9f8b, jobVertexID=cbc357ccb763df2852fee8c4fc7d55f2, subtaskIndex=35}} for ba766f85be2130d6661f17297a4a9f8b - cbc357ccb763df2852fee8c4fc7d55f2 - 35 under allocation id 11097abd215e6451f2e95b37fe86699a.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:11435 ] - [ DEBUG ]  Found existing state changelog storage for job ba766f85be2130d6661f17297a4a9f8b: org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogStorage@1ac3c079.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:11436 ] - [ INFO ]  Received task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0 (18427157dcd5156c436e5dfbf779c7ab), deploy into slot with allocation id 11097abd215e6451f2e95b37fe86699a.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11437 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0 (18427157dcd5156c436e5dfbf779c7ab) switched from CREATED to DEPLOYING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11437 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@2998217
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11437 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11437 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@6a6069c
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11437 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0 (3bae0b19ee60137b874880f7d35cdfca) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11440 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11441 ] - [ DEBUG ]  Creating FileSystem stream leak safety net for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0 (18427157dcd5156c436e5dfbf779c7ab) [DEPLOYING]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11442 ] - [ INFO ]  Loading JAR files for task Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0 (18427157dcd5156c436e5dfbf779c7ab) [DEPLOYING].
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11442 ] - [ DEBUG ]  Getting user code class loader for task 18427157dcd5156c436e5dfbf779c7ab at library cache manager took 0 milliseconds
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11442 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11442 ] - [ DEBUG ]  Registering task at network: Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0 (18427157dcd5156c436e5dfbf779c7ab) [DEPLOYING].
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11443 ] - [ INFO ]  Using application-defined state backend: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend@51bcac01
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11443 ] - [ INFO ]  State backend loader loads the state backend as BatchExecutionStateBackend
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11443 ] - [ INFO ]  Using application defined checkpoint storage: org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionCheckpointStorage@558322ee
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11443 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0 (18427157dcd5156c436e5dfbf779c7ab) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11443 ] - [ DEBUG ]  Initializing Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:11446 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36) (3bae0b19ee60137b874880f7d35cdfca) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:11448 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36) (18427157dcd5156c436e5dfbf779c7ab) switched from DEPLOYING to INITIALIZING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:11848 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(5/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:11853 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(26/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:11853 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(7/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:11853 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(10/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:11853 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(2/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:11853 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(17/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:11853 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(13/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:11853 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(16/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11852 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11852 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:11852 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(18/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:11852 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(11/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:11852 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(24/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:11852 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(4/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:11852 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(22/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:11852 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(6/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11852 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:11852 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(21/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:11852 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(25/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:11852 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(8/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:11848 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(9/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:11848 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(12/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:11848 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(14/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:11848 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(1/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11848 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(27/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:11851 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(23/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:11851 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(15/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:11851 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(20/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:11851 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(3/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:11848 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(19/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11851 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11900 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11901 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11901 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11913 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11913 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11921 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:11921 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(5/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11921 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:11921 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(17/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11922 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:11922 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(3/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:11922 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(13/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:11923 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(15/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11923 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(27/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:11923 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(20/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:11923 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(24/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:11923 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(12/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:11923 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(16/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:11923 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(2/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:11923 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(9/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:11923 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(4/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:11923 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(7/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:11924 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(1/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:11924 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(22/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:11924 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(10/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:11925 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(26/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:11925 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(11/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:11925 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(18/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:11925 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(8/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:11925 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(23/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:11925 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(25/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:11926 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(14/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:11925 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(21/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:11925 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(19/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:11925 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(6/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11932 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(29/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11932 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(29/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11932 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(32/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11937 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(28/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11937 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(34/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11932 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(31/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11939 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(32/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11939 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(31/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11939 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(28/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11939 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(34/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11940 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11940 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11940 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(33/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11940 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(33/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11946 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(30/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11952 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11952 ] - [ WARN ]  The operator name Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) exceeded the 80 characters length limit and was truncated.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11957 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(30/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11965 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11966 ] - [ DEBUG ]  Invoking Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11967 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(36/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11968 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(36/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11968 ] - [ DEBUG ]  Creating operator state backend for SinkOperator_7df19f87deec5680128845fd9a6ca18d_(35/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11968 ] - [ DEBUG ]  Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(35/36) with empty state.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:11995 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0 (1eb215c73dfbc39e6702cf343ce8d121) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:11995 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0 (4d8d3851631b53958f2950b993e951c3) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:11995 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0 (138b4422f4982380d457251dd8595f35) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:11996 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0 (38f97eb8a1c5de44871786bc3329ddac) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:11995 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0 (ba08ed9470b8b256110a141383e411b8) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:11995 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0 (9f4f3e1ecc6ba91aa045b2a561e97ef6) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:11995 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0 (536a25360f375ad9517b80540b08d145) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:11995 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0 (2f88ff53e91055cd65a3913274b72538) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:11995 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0 (a1bb9f26bcffcad198f52a482fb53559) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:11995 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0 (0e4277a1f6d404bc71bb79fcfbc9769e) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:11995 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0 (4cacc0cf82a95a1a2846442152dd1ebd) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:11995 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0 (7cbc3523540446665872efda7f69c476) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:11996 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0 (6e822a8a973db92453d7d0dd1fdc15cc) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:11996 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0 (986a109fb0d2fe0a94164c7afedc55e7) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:11996 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0 (c19840797252b89377f7ef7ba4fcf5d4) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:11997 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0 (4fcda378a5d3eb0b9a9f36477c5ea9bc) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:11997 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0 (0f08aa732c67bb70a34b87c43db14906) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:11997 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0 (3a4a32942dddaf0385d40bde5fc34a86) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:11997 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0 (0c9a99f0839bbedbc59a320c8e177bf5) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:11997 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0 (ddd59049b3eb442e5b9545c88ad0890a) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:11997 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0 (eb234fe2b78cf50d28a21c5e8428c8e2) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:11997 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0 (b3f0a8b054e16515fd877f40d8750540) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0 (737edf5169bbacb604ef4156b6663334) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0 (0a0c6c9c1d45808bef65560eb7afc889) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0 (3757ee42c112a6cecf5c7cdc072c08b6) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0 (bbaa5006ecd904ef625977413144688b) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0 (93d69acaf74237fbf2c6e56c068e29bc) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0 (05715cb683fac77f419f61068d36469e) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0 (cd37ec48093b17fb0d6fc7ba3aa250f4) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0 (9f77d72911b3e515bb1f11ed3d21b925) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0 (a804fa908af17cf17211c1700cf07411) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0 (3bae0b19ee60137b874880f7d35cdfca) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0 (0180ce402fb520a6d0325667c4c9f55b) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0 (71d31e42db0cfc094428f464d12688f8) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0 (4a8bef25bef4f0beec2b6df232b4aefc) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:11998 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0 (18427157dcd5156c436e5dfbf779c7ab) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12021 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 23 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12023 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 35 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12024 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 7 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12025 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 22 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12025 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 8 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12027 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 5 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12027 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 3 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12027 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 29 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12027 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 15 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12027 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 10 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12027 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 18 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12028 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 33 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12028 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 19 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12028 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 21 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12028 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 1 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12029 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 25 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12030 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 30 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12031 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 0 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12035 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 26 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12036 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 6 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12036 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 24 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12036 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 27 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12037 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 9 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12038 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 4 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12038 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 2 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12038 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 17 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12038 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 32 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12039 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 12 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12039 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 16 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12039 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 28 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12039 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 34 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12039 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 11 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12039 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 14 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12039 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 13 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12040 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 20 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12040 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 registering reader for parallel task 31 @ 
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12040 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 13
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12040 ] - [ INFO ]  Subtask 13 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12057 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36) (1eb215c73dfbc39e6702cf343ce8d121) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12058 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36) (4d8d3851631b53958f2950b993e951c3) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12060 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36) (138b4422f4982380d457251dd8595f35) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12061 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36) (38f97eb8a1c5de44871786bc3329ddac) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12061 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36) (ba08ed9470b8b256110a141383e411b8) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12062 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000028_0, Offset=0, Length=29017181, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12062 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36) (9f4f3e1ecc6ba91aa045b2a561e97ef6) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12063 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36) (536a25360f375ad9517b80540b08d145) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12063 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36) (2f88ff53e91055cd65a3913274b72538) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12063 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36) (a1bb9f26bcffcad198f52a482fb53559) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12063 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36) (0e4277a1f6d404bc71bb79fcfbc9769e) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12063 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36) (4cacc0cf82a95a1a2846442152dd1ebd) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12063 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36) (7cbc3523540446665872efda7f69c476) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12064 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36) (6e822a8a973db92453d7d0dd1fdc15cc) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12064 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36) (986a109fb0d2fe0a94164c7afedc55e7) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12064 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36) (c19840797252b89377f7ef7ba4fcf5d4) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12064 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36) (4fcda378a5d3eb0b9a9f36477c5ea9bc) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12064 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36) (0f08aa732c67bb70a34b87c43db14906) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12064 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36) (3a4a32942dddaf0385d40bde5fc34a86) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12064 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36) (0c9a99f0839bbedbc59a320c8e177bf5) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12067 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36) (b3f0a8b054e16515fd877f40d8750540) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12068 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36) (737edf5169bbacb604ef4156b6663334) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12069 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36) (0a0c6c9c1d45808bef65560eb7afc889) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12069 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36) (3757ee42c112a6cecf5c7cdc072c08b6) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12069 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36) (bbaa5006ecd904ef625977413144688b) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12070 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36) (eb234fe2b78cf50d28a21c5e8428c8e2) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12070 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36) (93d69acaf74237fbf2c6e56c068e29bc) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12070 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36) (ddd59049b3eb442e5b9545c88ad0890a) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12070 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36) (05715cb683fac77f419f61068d36469e) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12070 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36) (cd37ec48093b17fb0d6fc7ba3aa250f4) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12071 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36) (9f77d72911b3e515bb1f11ed3d21b925) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12071 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36) (a804fa908af17cf17211c1700cf07411) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12071 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36) (3bae0b19ee60137b874880f7d35cdfca) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12071 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36) (0180ce402fb520a6d0325667c4c9f55b) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12072 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36) (71d31e42db0cfc094428f464d12688f8) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12073 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36) (4a8bef25bef4f0beec2b6df232b4aefc) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12073 ] - [ INFO ]  Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36) (18427157dcd5156c436e5dfbf779c7ab) switched from INITIALIZING to RUNNING.
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12080 ] - [ INFO ]  Assigned split to subtask 13 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000028_0, Offset=0, Length=29017181, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12081 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 20
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12081 ] - [ INFO ]  Subtask 20 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12081 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000031_0, Offset=0, Length=28740636, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12082 ] - [ INFO ]  Assigned split to subtask 20 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000031_0, Offset=0, Length=28740636, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12082 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 29
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12082 ] - [ INFO ]  Subtask 29 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12082 ] - [ DEBUG ]  Operator event for 0e4277a1f6d404bc71bb79fcfbc9769e - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12085 ] - [ DEBUG ]  Operator event for 986a109fb0d2fe0a94164c7afedc55e7 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12088 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000021_0, Offset=0, Length=28449222, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12088 ] - [ DEBUG ]  Operator event for 38f97eb8a1c5de44871786bc3329ddac - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12094 ] - [ INFO ]  Assigned split to subtask 29 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000021_0, Offset=0, Length=28449222, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12094 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 22
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12094 ] - [ INFO ]  Subtask 22 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12098 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000019_0, Offset=0, Length=28613008, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12099 ] - [ DEBUG ]  Operator event for 536a25360f375ad9517b80540b08d145 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12099 ] - [ INFO ]  Assigned split to subtask 22 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000019_0, Offset=0, Length=28613008, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12099 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 1
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12099 ] - [ INFO ]  Subtask 1 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12099 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000011_0, Offset=0, Length=28480412, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12102 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000028_0, Offset=0, Length=29017181, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12106 ] - [ DEBUG ]  Operator event for 4fcda378a5d3eb0b9a9f36477c5ea9bc - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12103 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000031_0, Offset=0, Length=28740636, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12103 ] - [ INFO ]  Assigned split to subtask 1 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000011_0, Offset=0, Length=28480412, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12107 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000019_0, Offset=0, Length=28613008, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12110 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 32
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12111 ] - [ INFO ]  Subtask 32 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12112 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000009_0, Offset=0, Length=28626964, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12113 ] - [ INFO ]  Assigned split to subtask 32 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000009_0, Offset=0, Length=28626964, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12113 ] - [ DEBUG ]  Operator event for c19840797252b89377f7ef7ba4fcf5d4 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12113 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 8
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12113 ] - [ INFO ]  Subtask 8 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12114 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000010_0, Offset=0, Length=28170588, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12114 ] - [ INFO ]  Assigned split to subtask 8 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000010_0, Offset=0, Length=28170588, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12114 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 19
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12114 ] - [ INFO ]  Subtask 19 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12114 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000034_0, Offset=0, Length=28904813, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12114 ] - [ DEBUG ]  Operator event for 05715cb683fac77f419f61068d36469e - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12116 ] - [ INFO ]  Assigned split to subtask 19 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000034_0, Offset=0, Length=28904813, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12116 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 5
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12116 ] - [ INFO ]  Subtask 5 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12117 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000021_0, Offset=0, Length=28449222, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12120 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000011_0, Offset=0, Length=28480412, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12120 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000018_0, Offset=0, Length=28639264, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12120 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000010_0, Offset=0, Length=28170588, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-7:12121 ] - [ DEBUG ]  Operator event for 9f77d72911b3e515bb1f11ed3d21b925 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12122 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000009_0, Offset=0, Length=28626964, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12122 ] - [ INFO ]  Assigned split to subtask 5 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000018_0, Offset=0, Length=28639264, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-9:12124 ] - [ DEBUG ]  Operator event for eb234fe2b78cf50d28a21c5e8428c8e2 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12125 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 3
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12125 ] - [ INFO ]  Subtask 3 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12125 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000013_0, Offset=0, Length=28557648, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12127 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000034_0, Offset=0, Length=28904813, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12127 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000018_0, Offset=0, Length=28639264, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12129 ] - [ INFO ]  Assigned split to subtask 3 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000013_0, Offset=0, Length=28557648, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12129 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 4
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12129 ] - [ INFO ]  Subtask 4 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12129 ] - [ DEBUG ]  Operator event for 9f4f3e1ecc6ba91aa045b2a561e97ef6 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12130 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000024_0, Offset=0, Length=28533916, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-7:12130 ] - [ DEBUG ]  Operator event for 3a4a32942dddaf0385d40bde5fc34a86 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12131 ] - [ INFO ]  Assigned split to subtask 4 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000024_0, Offset=0, Length=28533916, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12131 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 21
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12131 ] - [ INFO ]  Subtask 21 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12131 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000008_0, Offset=0, Length=28638234, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12134 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12134 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12139 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000018_0, Offset=0, Length=28639264, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12139 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12138 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12137 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12135 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12134 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12135 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12140 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000009_0, Offset=0, Length=28626964, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12140 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000031_0, Offset=0, Length=28740636, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12140 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000034_0, Offset=0, Length=28904813, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12140 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000034_0, Offset=0, Length=28904813, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12139 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000019_0, Offset=0, Length=28613008, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12139 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12139 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000010_0, Offset=0, Length=28170588, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12139 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000018_0, Offset=0, Length=28639264, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12139 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000011_0, Offset=0, Length=28480412, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12142 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12142 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000010_0, Offset=0, Length=28170588, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12141 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000021_0, Offset=0, Length=28449222, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12141 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000019_0, Offset=0, Length=28613008, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12141 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12140 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000013_0, Offset=0, Length=28557648, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12140 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000024_0, Offset=0, Length=28533916, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12140 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000031_0, Offset=0, Length=28740636, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12140 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000009_0, Offset=0, Length=28626964, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12140 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000028_0, Offset=0, Length=29017181, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12154 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12154 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12153 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12149 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000034_0, Offset=0, Length=28904813, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12149 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12147 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000021_0, Offset=0, Length=28449222, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12145 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12158 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000010_0, Offset=0, Length=28170588, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12144 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000018_0, Offset=0, Length=28639264, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12144 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000011_0, Offset=0, Length=28480412, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12157 ] - [ INFO ]  Assigned split to subtask 21 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000008_0, Offset=0, Length=28638234, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12156 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12155 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000019_0, Offset=0, Length=28613008, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12155 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000034_0, Offset=0, Length=28904813, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12155 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000028_0, Offset=0, Length=29017181, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12155 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000009_0, Offset=0, Length=28626964, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12155 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000024_0, Offset=0, Length=28533916, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12155 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000013_0, Offset=0, Length=28557648, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12154 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12158 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000010_0, Offset=0, Length=28170588, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12162 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 15
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12162 ] - [ INFO ]  Subtask 15 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12162 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000018_0, Offset=0, Length=28639264, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-9:12163 ] - [ DEBUG ]  Operator event for 0a0c6c9c1d45808bef65560eb7afc889 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12163 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000024_0, Offset=0, Length=28533916, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12165 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12165 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000019_0, Offset=0, Length=28613008, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12165 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000018_0, Offset=0, Length=28639264, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12165 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12166 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12166 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000024_0, Offset=0, Length=28533916, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12168 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12166 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12177 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000009_0, Offset=0, Length=28626964, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12179 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000021_0, Offset=0, Length=28449222, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12179 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000013_0, Offset=0, Length=28557648, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12180 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12181 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000005_0, Offset=0, Length=28386755, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12181 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000010_0, Offset=0, Length=28170588, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12182 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12182 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12182 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000034_0, Offset=0, Length=28904813, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12182 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12182 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000010_0, Offset=0, Length=28170588, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12182 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12182 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000019_0, Offset=0, Length=28613008, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12182 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12182 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12182 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000024_0, Offset=0, Length=28533916, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12182 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000019_0, Offset=0, Length=28613008, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12182 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000008_0, Offset=0, Length=28638234, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12183 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000018_0, Offset=0, Length=28639264, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12183 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000031_0, Offset=0, Length=28740636, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12182 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000024_0, Offset=0, Length=28533916, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12183 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12184 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12182 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000009_0, Offset=0, Length=28626964, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12185 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12185 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12184 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000031_0, Offset=0, Length=28740636, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12184 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000013_0, Offset=0, Length=28557648, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12183 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000028_0, Offset=0, Length=29017181, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12183 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000011_0, Offset=0, Length=28480412, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12183 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000021_0, Offset=0, Length=28449222, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12185 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12190 ] - [ DEBUG ]  Operator event for 6e822a8a973db92453d7d0dd1fdc15cc - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12203 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000021_0, Offset=0, Length=28449222, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12206 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12205 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000028_0, Offset=0, Length=29017181, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12205 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000009_0, Offset=0, Length=28626964, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12205 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000013_0, Offset=0, Length=28557648, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12205 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000008_0, Offset=0, Length=28638234, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12205 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000024_0, Offset=0, Length=28533916, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12205 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000034_0, Offset=0, Length=28904813, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12204 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000011_0, Offset=0, Length=28480412, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12204 ] - [ INFO ]  Assigned split to subtask 15 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000005_0, Offset=0, Length=28386755, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12210 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12215 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000013_0, Offset=0, Length=28557648, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12215 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000021_0, Offset=0, Length=28449222, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12215 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12224 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000031_0, Offset=0, Length=28740636, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12226 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12225 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000008_0, Offset=0, Length=28638234, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12225 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12225 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 10
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12224 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000011_0, Offset=0, Length=28480412, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12231 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12224 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000028_0, Offset=0, Length=29017181, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12231 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12231 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12231 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12231 ] - [ INFO ]  Subtask 10 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12230 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12229 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000005_0, Offset=0, Length=28386755, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12228 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12231 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000008_0, Offset=0, Length=28638234, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12231 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000011_0, Offset=0, Length=28480412, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12231 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000029_0, Offset=0, Length=29007993, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12231 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000013_0, Offset=0, Length=28557648, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12231 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000008_0, Offset=0, Length=28638234, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12231 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000028_0, Offset=0, Length=29017181, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12231 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000031_0, Offset=0, Length=28740636, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12233 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000008_0, Offset=0, Length=28638234, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12233 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12233 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12233 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000008_0, Offset=0, Length=28638234, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12234 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12234 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000005_0, Offset=0, Length=28386755, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12234 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000005_0, Offset=0, Length=28386755, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12235 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12238 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000005_0, Offset=0, Length=28386755, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12238 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000005_0, Offset=0, Length=28386755, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12238 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000005_0, Offset=0, Length=28386755, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12238 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12238 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12238 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000005_0, Offset=0, Length=28386755, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12240 ] - [ INFO ]  Assigned split to subtask 10 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000029_0, Offset=0, Length=29007993, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12240 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 0
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12240 ] - [ INFO ]  Subtask 0 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12240 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000002_0, Offset=0, Length=28412300, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12242 ] - [ INFO ]  Assigned split to subtask 0 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000002_0, Offset=0, Length=28412300, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12242 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 23
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12242 ] - [ DEBUG ]  Operator event for ba08ed9470b8b256110a141383e411b8 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12242 ] - [ INFO ]  Subtask 23 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12242 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000014_0, Offset=0, Length=28525288, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12242 ] - [ INFO ]  Assigned split to subtask 23 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000014_0, Offset=0, Length=28525288, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12242 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 7
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12242 ] - [ INFO ]  Subtask 7 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12242 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000000_0, Offset=0, Length=28765749, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12243 ] - [ INFO ]  Assigned split to subtask 7 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000000_0, Offset=0, Length=28765749, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12243 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 25
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12243 ] - [ INFO ]  Subtask 25 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12243 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000025_0, Offset=0, Length=28489809, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12243 ] - [ INFO ]  Assigned split to subtask 25 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000025_0, Offset=0, Length=28489809, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12243 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 12
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12243 ] - [ INFO ]  Subtask 12 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12243 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000007_0, Offset=0, Length=28945776, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12244 ] - [ INFO ]  Assigned split to subtask 12 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000007_0, Offset=0, Length=28945776, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12244 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 35
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12244 ] - [ INFO ]  Subtask 35 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12244 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000001_0, Offset=0, Length=28579410, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12244 ] - [ INFO ]  Assigned split to subtask 35 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000001_0, Offset=0, Length=28579410, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12244 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 33
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12244 ] - [ INFO ]  Subtask 33 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12244 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000022_0, Offset=0, Length=28475604, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12244 ] - [ INFO ]  Assigned split to subtask 33 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000022_0, Offset=0, Length=28475604, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12244 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 16
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12244 ] - [ INFO ]  Subtask 16 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12244 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000035_0, Offset=0, Length=28503292, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12245 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000029_0, Offset=0, Length=29007993, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12245 ] - [ INFO ]  Assigned split to subtask 16 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000035_0, Offset=0, Length=28503292, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12245 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12246 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000029_0, Offset=0, Length=29007993, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12245 ] - [ DEBUG ]  Operator event for a804fa908af17cf17211c1700cf07411 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12246 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000029_0, Offset=0, Length=29007993, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12245 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 14
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12252 ] - [ DEBUG ]  Operator event for cd37ec48093b17fb0d6fc7ba3aa250f4 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12248 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12252 ] - [ INFO ]  Subtask 14 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12256 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000015_0, Offset=0, Length=28694742, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12257 ] - [ DEBUG ]  Operator event for 138b4422f4982380d457251dd8595f35 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12257 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000029_0, Offset=0, Length=29007993, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12256 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000002_0, Offset=0, Length=28412300, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12258 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000029_0, Offset=0, Length=29007993, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12258 ] - [ DEBUG ]  Operator event for 0180ce402fb520a6d0325667c4c9f55b - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12258 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000014_0, Offset=0, Length=28525288, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12259 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12259 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12259 ] - [ DEBUG ]  Operator event for bbaa5006ecd904ef625977413144688b - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12261 ] - [ DEBUG ]  Operator event for 18427157dcd5156c436e5dfbf779c7ab - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12261 ] - [ DEBUG ]  Operator event for 4a8bef25bef4f0beec2b6df232b4aefc - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12261 ] - [ DEBUG ]  Operator event for 4cacc0cf82a95a1a2846442152dd1ebd - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-8:12261 ] - [ DEBUG ]  Operator event for 7cbc3523540446665872efda7f69c476 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12269 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000000_0, Offset=0, Length=28765749, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12271 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12277 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000014_0, Offset=0, Length=28525288, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12278 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000029_0, Offset=0, Length=29007993, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12279 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12279 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12278 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000007_0, Offset=0, Length=28945776, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12278 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000000_0, Offset=0, Length=28765749, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12277 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000001_0, Offset=0, Length=28579410, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12279 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000002_0, Offset=0, Length=28412300, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12279 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000015_0, Offset=0, Length=28694742, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12278 ] - [ INFO ]  Assigned split to subtask 14 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000015_0, Offset=0, Length=28694742, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12278 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000025_0, Offset=0, Length=28489809, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12280 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000022_0, Offset=0, Length=28475604, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12279 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000035_0, Offset=0, Length=28503292, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12280 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12280 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000015_0, Offset=0, Length=28694742, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12280 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000015_0, Offset=0, Length=28694742, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12279 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000014_0, Offset=0, Length=28525288, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12279 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12279 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000002_0, Offset=0, Length=28412300, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12279 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000000_0, Offset=0, Length=28765749, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12279 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12279 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000029_0, Offset=0, Length=29007993, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12284 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12282 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12282 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12282 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 11
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12281 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12297 ] - [ INFO ]  Subtask 11 (on host '') is requesting a file source split
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12292 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12290 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12297 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12303 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000000_0, Offset=0, Length=28765749, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12303 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000025_0, Offset=0, Length=28489809, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12303 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000001_0, Offset=0, Length=28579410, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12303 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000015_0, Offset=0, Length=28694742, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ SourceCoordinator-Source: HiveSource-ods.test_01:12305 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000032_0, Offset=0, Length=28815653, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12305 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000002_0, Offset=0, Length=28412300, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12306 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000007_0, Offset=0, Length=28945776, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12306 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000022_0, Offset=0, Length=28475604, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12306 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000035_0, Offset=0, Length=28503292, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12307 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000014_0, Offset=0, Length=28525288, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12310 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000015_0, Offset=0, Length=28694742, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12312 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000025_0, Offset=0, Length=28489809, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12313 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000007_0, Offset=0, Length=28945776, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12318 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000000_0, Offset=0, Length=28765749, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12317 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000035_0, Offset=0, Length=28503292, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12322 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12317 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000015_0, Offset=0, Length=28694742, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12322 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12322 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12317 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12317 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000014_0, Offset=0, Length=28525288, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12317 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000001_0, Offset=0, Length=28579410, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12322 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12316 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000022_0, Offset=0, Length=28475604, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12322 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12317 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000002_0, Offset=0, Length=28412300, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12320 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12327 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000000_0, Offset=0, Length=28765749, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12327 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12327 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12327 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000007_0, Offset=0, Length=28945776, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12327 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000000_0, Offset=0, Length=28765749, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12330 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000022_0, Offset=0, Length=28475604, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12330 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000022_0, Offset=0, Length=28475604, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12330 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000022_0, Offset=0, Length=28475604, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12330 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12331 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12331 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000022_0, Offset=0, Length=28475604, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12330 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000001_0, Offset=0, Length=28579410, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12333 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000025_0, Offset=0, Length=28489809, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12333 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000015_0, Offset=0, Length=28694742, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12334 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000007_0, Offset=0, Length=28945776, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12335 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000007_0, Offset=0, Length=28945776, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12335 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12335 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12336 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000007_0, Offset=0, Length=28945776, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12336 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000025_0, Offset=0, Length=28489809, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12336 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000014_0, Offset=0, Length=28525288, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12336 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12336 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12336 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000025_0, Offset=0, Length=28489809, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12336 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12336 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12336 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000025_0, Offset=0, Length=28489809, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12336 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000014_0, Offset=0, Length=28525288, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12336 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000001_0, Offset=0, Length=28579410, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12337 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000001_0, Offset=0, Length=28579410, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12337 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12337 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12337 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000001_0, Offset=0, Length=28579410, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12339 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000002_0, Offset=0, Length=28412300, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12339 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12339 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12339 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000002_0, Offset=0, Length=28412300, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12339 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000035_0, Offset=0, Length=28503292, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12339 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000035_0, Offset=0, Length=28503292, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12339 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000035_0, Offset=0, Length=28503292, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12339 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12339 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:07  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12339 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000035_0, Offset=0, Length=28503292, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:07  [ flink-akka.actor.default-dispatcher-5:12371 ] - [ DEBUG ]  Operator event for 0f08aa732c67bb70a34b87c43db14906 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #0:12387 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #13 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12390 ] - [ INFO ]  Assigned split to subtask 11 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000032_0, Offset=0, Length=28815653, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12396 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 17
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12397 ] - [ INFO ]  Subtask 17 (on host '') is requesting a file source split
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #0:12406 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #17 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12414 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #11 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12415 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000027_0, Offset=0, Length=28527908, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ flink-akka.actor.default-dispatcher-5:12420 ] - [ DEBUG ]  Operator event for 0c9a99f0839bbedbc59a320c8e177bf5 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12421 ] - [ INFO ]  Assigned split to subtask 17 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000027_0, Offset=0, Length=28527908, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12421 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 28
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12421 ] - [ INFO ]  Subtask 28 (on host '') is requesting a file source split
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12421 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000016_0, Offset=0, Length=28297244, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12423 ] - [ INFO ]  Assigned split to subtask 28 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000016_0, Offset=0, Length=28297244, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12423 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 6
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12423 ] - [ INFO ]  Subtask 6 (on host '') is requesting a file source split
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12423 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000006_0, Offset=0, Length=28675845, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12423 ] - [ INFO ]  Assigned split to subtask 6 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000006_0, Offset=0, Length=28675845, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12423 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 2
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12423 ] - [ INFO ]  Subtask 2 (on host '') is requesting a file source split
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12423 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000004_0, Offset=0, Length=28184777, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12424 ] - [ INFO ]  Assigned split to subtask 2 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000004_0, Offset=0, Length=28184777, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12424 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 26
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12424 ] - [ INFO ]  Subtask 26 (on host '') is requesting a file source split
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12424 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000033_0, Offset=0, Length=28422744, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ flink-akka.actor.default-dispatcher-7:12426 ] - [ DEBUG ]  Operator event for 1eb215c73dfbc39e6702cf343ce8d121 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12427 ] - [ INFO ]  Assigned split to subtask 26 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000033_0, Offset=0, Length=28422744, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12427 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 24
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12427 ] - [ INFO ]  Subtask 24 (on host '') is requesting a file source split
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12427 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000003_0, Offset=0, Length=28377404, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12427 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12427 ] - [ INFO ]  Assigned split to subtask 24 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000003_0, Offset=0, Length=28377404, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12430 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 34
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12430 ] - [ INFO ]  Subtask 34 (on host '') is requesting a file source split
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12431 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000030_0, Offset=0, Length=28620373, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12432 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #14 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ flink-akka.actor.default-dispatcher-7:12433 ] - [ DEBUG ]  Operator event for b3f0a8b054e16515fd877f40d8750540 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:08  [ flink-akka.actor.default-dispatcher-7:12439 ] - [ DEBUG ]  Operator event for 93d69acaf74237fbf2c6e56c068e29bc - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:08  [ flink-akka.actor.default-dispatcher-7:12440 ] - [ DEBUG ]  Operator event for 2f88ff53e91055cd65a3913274b72538 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12441 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #10 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12443 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #15 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12444 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #9 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12449 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #16 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12452 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #21 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12454 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #12 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12461 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #18 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12462 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #19 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12463 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #20 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12463 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #30 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12463 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #29 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ flink-akka.actor.default-dispatcher-9:12464 ] - [ DEBUG ]  Operator event for 71d31e42db0cfc094428f464d12688f8 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12467 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #26 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12467 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #24 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12467 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #23 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12468 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #25 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12468 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #27 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12468 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #28 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12468 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #22 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12470 ] - [ INFO ]  Assigned split to subtask 34 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000030_0, Offset=0, Length=28620373, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12470 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 30
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12470 ] - [ INFO ]  Subtask 30 (on host '') is requesting a file source split
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12471 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000012_0, Offset=0, Length=28744332, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12472 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #13
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12472 ] - [ DEBUG ]  Call: getBlockLocations took 101ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12472 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #17
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12473 ] - [ DEBUG ]  Call: getBlockLocations took 102ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12473 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #11
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12473 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #8
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12473 ] - [ DEBUG ]  Call: getBlockLocations took 103ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12473 ] - [ DEBUG ]  Call: getBlockLocations took 103ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12473 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #14
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12473 ] - [ DEBUG ]  Call: getBlockLocations took 103ms
2023-03-31 11:26:08  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12475 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000033_0, Offset=0, Length=28422744, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12476 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000016_0, Offset=0, Length=28297244, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12477 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000032_0, Offset=0, Length=28815653, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ flink-akka.actor.default-dispatcher-9:12477 ] - [ DEBUG ]  Operator event for 3bae0b19ee60137b874880f7d35cdfca - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:08  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12477 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000006_0, Offset=0, Length=28675845, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12478 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #15
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12478 ] - [ DEBUG ]  Call: getBlockLocations took 107ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12478 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #10
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12478 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #9
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12479 ] - [ DEBUG ]  Call: getBlockLocations took 109ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12480 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12482 ] - [ DEBUG ]  Call: getBlockLocations took 111ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12482 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #16
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12482 ] - [ DEBUG ]  Call: getBlockLocations took 110ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12482 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #21
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12482 ] - [ DEBUG ]  Call: getBlockLocations took 112ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12482 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #12
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12482 ] - [ DEBUG ]  Call: getBlockLocations took 111ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12483 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12484 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12484 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:08  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12520 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000004_0, Offset=0, Length=28184777, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12520 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000003_0, Offset=0, Length=28377404, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12521 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000027_0, Offset=0, Length=28527908, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12521 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12521 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12522 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12525 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000004_0, Offset=0, Length=28184777, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12525 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000003_0, Offset=0, Length=28377404, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12526 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000016_0, Offset=0, Length=28297244, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12526 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000004_0, Offset=0, Length=28184777, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12526 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000032_0, Offset=0, Length=28815653, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12531 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28740636
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854169_11113412; getBlockSize()=28740636; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854169_11113412; getBlockSize()=28740636; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12531 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28480412
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854012_11113255; getBlockSize()=28480412; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854012_11113255; getBlockSize()=28480412; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12531 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28904813
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854181_11113424; getBlockSize()=28904813; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854181_11113424; getBlockSize()=28904813; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12531 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28170588
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854011_11113254; getBlockSize()=28170588; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854011_11113254; getBlockSize()=28170588; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ flink-akka.actor.default-dispatcher-5:12531 ] - [ DEBUG ]  Operator event for 737edf5169bbacb604ef4156b6663334 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12531 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=29007993
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854161_11113404; getBlockSize()=29007993; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854161_11113404; getBlockSize()=29007993; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12531 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=29017181
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854158_11113401; getBlockSize()=29017181; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854158_11113401; getBlockSize()=29017181; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12531 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28626964
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854005_11113248; getBlockSize()=28626964; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854005_11113248; getBlockSize()=28626964; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12531 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28386755
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853990_11113233; getBlockSize()=28386755; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853990_11113233; getBlockSize()=28386755; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12531 ] - [ INFO ]  Assigned split to subtask 30 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000012_0, Offset=0, Length=28744332, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12531 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28533916
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854130_11113373; getBlockSize()=28533916; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854130_11113373; getBlockSize()=28533916; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12533 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 31
2023-03-31 11:26:08  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12531 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000030_0, Offset=0, Length=28620373, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12530 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000003_0, Offset=0, Length=28377404, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12529 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000016_0, Offset=0, Length=28297244, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12528 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000033_0, Offset=0, Length=28422744, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12528 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000027_0, Offset=0, Length=28527908, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12528 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12536 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12537 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12537 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000016_0, Offset=0, Length=28297244, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12533 ] - [ INFO ]  Subtask 31 (on host '') is requesting a file source split
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12532 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000006_0, Offset=0, Length=28675845, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12532 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28613008
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854056_11113299; getBlockSize()=28613008; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854056_11113299; getBlockSize()=28613008; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12532 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28449222
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854112_11113355; getBlockSize()=28449222; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854112_11113355; getBlockSize()=28449222; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12531 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000032_0, Offset=0, Length=28815653, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12538 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000004_0, Offset=0, Length=28184777, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12538 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000026_0, Offset=0, Length=28844730, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12537 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000006_0, Offset=0, Length=28675845, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12537 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000027_0, Offset=0, Length=28527908, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12537 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000016_0, Offset=0, Length=28297244, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12537 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000030_0, Offset=0, Length=28620373, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12537 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000033_0, Offset=0, Length=28422744, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12546 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000030_0, Offset=0, Length=28620373, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12545 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000016_0, Offset=0, Length=28297244, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12548 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12545 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12543 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12542 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12541 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000004_0, Offset=0, Length=28184777, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12540 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12548 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000004_0, Offset=0, Length=28184777, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12550 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12550 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12548 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12548 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000032_0, Offset=0, Length=28815653, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12548 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000006_0, Offset=0, Length=28675845, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12548 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000027_0, Offset=0, Length=28527908, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12550 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000006_0, Offset=0, Length=28675845, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12550 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000006_0, Offset=0, Length=28675845, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12550 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12550 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12548 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12547 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12550 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000016_0, Offset=0, Length=28297244, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12550 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000006_0, Offset=0, Length=28675845, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:08  [ flink-akka.actor.default-dispatcher-9:12550 ] - [ DEBUG ]  Operator event for 3757ee42c112a6cecf5c7cdc072c08b6 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:08  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12550 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000012_0, Offset=0, Length=28744332, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12550 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000027_0, Offset=0, Length=28527908, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12550 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000004_0, Offset=0, Length=28184777, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12550 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000032_0, Offset=0, Length=28815653, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12550 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000030_0, Offset=0, Length=28620373, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12550 ] - [ INFO ]  Assigned split to subtask 31 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000026_0, Offset=0, Length=28844730, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12548 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000003_0, Offset=0, Length=28377404, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12551 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 18
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12551 ] - [ INFO ]  Subtask 18 (on host '') is requesting a file source split
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12551 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12551 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000030_0, Offset=0, Length=28620373, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12551 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000032_0, Offset=0, Length=28815653, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12551 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12551 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12551 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000027_0, Offset=0, Length=28527908, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12551 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000026_0, Offset=0, Length=28844730, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12552 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000017_0, Offset=0, Length=28437496, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12550 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000033_0, Offset=0, Length=28422744, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12551 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12552 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12552 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000033_0, Offset=0, Length=28422744, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12552 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000033_0, Offset=0, Length=28422744, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12552 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12552 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12552 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12552 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000033_0, Offset=0, Length=28422744, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12552 ] - [ INFO ]  Assigned split to subtask 18 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000017_0, Offset=0, Length=28437496, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:08  [ flink-akka.actor.default-dispatcher-7:12552 ] - [ DEBUG ]  Operator event for ddd59049b3eb442e5b9545c88ad0890a - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12551 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000032_0, Offset=0, Length=28815653, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12551 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000030_0, Offset=0, Length=28620373, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12553 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12553 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12551 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000012_0, Offset=0, Length=28744332, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12551 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000003_0, Offset=0, Length=28377404, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12553 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 27
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12553 ] - [ INFO ]  Subtask 27 (on host '') is requesting a file source split
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12552 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000026_0, Offset=0, Length=28844730, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12552 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000027_0, Offset=0, Length=28527908, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12567 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000026_0, Offset=0, Length=28844730, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12568 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12569 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000023_0, Offset=0, Length=28740313, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12569 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000030_0, Offset=0, Length=28620373, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12570 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000012_0, Offset=0, Length=28744332, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12571 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12572 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000017_0, Offset=0, Length=28437496, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12573 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000012_0, Offset=0, Length=28744332, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12573 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000012_0, Offset=0, Length=28744332, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12573 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000012_0, Offset=0, Length=28744332, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12573 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12573 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12573 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000003_0, Offset=0, Length=28377404, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12573 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000012_0, Offset=0, Length=28744332, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12573 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12573 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12573 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000003_0, Offset=0, Length=28377404, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12573 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000026_0, Offset=0, Length=28844730, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12573 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000026_0, Offset=0, Length=28844730, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12573 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000026_0, Offset=0, Length=28844730, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12573 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12573 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12573 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000026_0, Offset=0, Length=28844730, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12575 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12576 ] - [ INFO ]  Assigned split to subtask 27 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000023_0, Offset=0, Length=28740313, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12581 ] - [ INFO ]  Source Source: HiveSource-ods.test_01 received split request from parallel task 9
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12581 ] - [ INFO ]  Subtask 9 (on host '') is requesting a file source split
2023-03-31 11:26:08  [ flink-akka.actor.default-dispatcher-9:12575 ] - [ DEBUG ]  Operator event for a1bb9f26bcffcad198f52a482fb53559 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12577 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000017_0, Offset=0, Length=28437496, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12582 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000017_0, Offset=0, Length=28437496, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12583 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12585 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000017_0, Offset=0, Length=28437496, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12585 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000017_0, Offset=0, Length=28437496, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12585 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000017_0, Offset=0, Length=28437496, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12585 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12585 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12585 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000017_0, Offset=0, Length=28437496, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12585 ] - [ INFO ]  Assigning split to non-localized request: Optional[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000020_0, Offset=0, Length=28522397, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ SourceCoordinator-Source: HiveSource-ods.test_01:12589 ] - [ INFO ]  Assigned split to subtask 9 : HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000020_0, Offset=0, Length=28522397, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}
2023-03-31 11:26:08  [ flink-akka.actor.default-dispatcher-9:12589 ] - [ DEBUG ]  Operator event for 4d8d3851631b53958f2950b993e951c3 - cbc357ccb763df2852fee8c4fc7d55f2
2023-03-31 11:26:08  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12590 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000020_0, Offset=0, Length=28522397, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12591 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #32 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12593 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:08  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12594 ] - [ INFO ]  Adding split(s) to reader: [HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000023_0, Offset=0, Length=28740313, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12595 ] - [ INFO ]  Starting split fetcher 0
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12595 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000020_0, Offset=0, Length=28522397, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12595 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000020_0, Offset=0, Length=28522397, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12596 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12598 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000023_0, Offset=0, Length=28740313, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12599 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000020_0, Offset=0, Length=28522397, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12599 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000020_0, Offset=0, Length=28522397, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12600 ] - [ DEBUG ]  Enqueued task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000023_0, Offset=0, Length=28740313, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12600 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000020_0, Offset=0, Length=28522397, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12604 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12603 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #31 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12602 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12605 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12605 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000020_0, Offset=0, Length=28522397, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12605 ] - [ DEBUG ]  Prepare to run AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000023_0, Offset=0, Length=28740313, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12605 ] - [ DEBUG ]  Handling split change SplitAddition:[[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000023_0, Offset=0, Length=28740313, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12607 ] - [ DEBUG ]  Finished running task AddSplitsTask: [[HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000023_0, Offset=0, Length=28740313, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}]]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12608 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12608 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12608 ] - [ DEBUG ]  Use MapReduce RecordReader reader because the conditions of vectorized read are not met for HiveSourceSplit{Path=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01/000023_0, Offset=0, Length=28740313, Position=null, HiveTablePartition=HiveTablePartition{PartitionSpec={}, Location=hdfs://Tdsop/user/hive/warehouse/ods.db/test_01, InputFormat=org.apache.hadoop.mapred.TextInputFormat}}.
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12613 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #34 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12618 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #33 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12620 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #37 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12622 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #40 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12623 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #36 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12627 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #38 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12631 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #39 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12636 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #35 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12636 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #30
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12639 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #41 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12644 ] - [ DEBUG ]  Call: getBlockLocations took 226ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12644 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #18
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12645 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #19
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12645 ] - [ DEBUG ]  Call: getBlockLocations took 274ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12645 ] - [ DEBUG ]  Call: getBlockLocations took 275ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12645 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #20
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12645 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #29
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12645 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #24
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12645 ] - [ DEBUG ]  Call: getBlockLocations took 228ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12645 ] - [ DEBUG ]  Call: getBlockLocations took 274ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12645 ] - [ DEBUG ]  Call: getBlockLocations took 238ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12646 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #26
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12646 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #23
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12646 ] - [ DEBUG ]  Call: getBlockLocations took 239ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12646 ] - [ DEBUG ]  Call: getBlockLocations took 239ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12646 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28557648
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854019_11113262; getBlockSize()=28557648; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854019_11113262; getBlockSize()=28557648; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12646 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28525288
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854024_11113267; getBlockSize()=28525288; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854024_11113267; getBlockSize()=28525288; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12646 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28638234
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854006_11113249; getBlockSize()=28638234; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854006_11113249; getBlockSize()=28638234; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12646 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28489809
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854133_11113376; getBlockSize()=28489809; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854133_11113376; getBlockSize()=28489809; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12647 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28639264
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854040_11113283; getBlockSize()=28639264; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854040_11113283; getBlockSize()=28639264; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12646 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28475604
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854118_11113361; getBlockSize()=28475604; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854118_11113361; getBlockSize()=28475604; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12647 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28579410
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853982_11113225; getBlockSize()=28579410; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853982_11113225; getBlockSize()=28579410; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12647 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28765749
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853981_11113224; getBlockSize()=28765749; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853981_11113224; getBlockSize()=28765749; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12658 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #42 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:12662 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #43 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12681 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #25
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12682 ] - [ DEBUG ]  Call: getBlockLocations took 275ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12682 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #27
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12682 ] - [ DEBUG ]  Call: getBlockLocations took 276ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12682 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #28
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12682 ] - [ DEBUG ]  Call: getBlockLocations took 276ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12682 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #22
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12682 ] - [ DEBUG ]  Call: getBlockLocations took 280ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12682 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #32
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12683 ] - [ DEBUG ]  Call: getBlockLocations took 96ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12684 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28184777
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853991_11113234; getBlockSize()=28184777; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853991_11113234; getBlockSize()=28184777; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12687 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28412300
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853980_11113223; getBlockSize()=28412300; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853980_11113223; getBlockSize()=28412300; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12687 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28945776
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853999_11113242; getBlockSize()=28945776; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853999_11113242; getBlockSize()=28945776; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12689 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28503292
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854187_11113430; getBlockSize()=28503292; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854187_11113430; getBlockSize()=28503292; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12689 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28694742
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854027_11113270; getBlockSize()=28694742; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854027_11113270; getBlockSize()=28694742; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12692 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #31
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12692 ] - [ DEBUG ]  Call: getBlockLocations took 106ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12693 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #34
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12693 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #33
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12693 ] - [ DEBUG ]  Call: getBlockLocations took 106ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12693 ] - [ DEBUG ]  Call: getBlockLocations took 106ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12693 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #40
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12693 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #37
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12693 ] - [ DEBUG ]  Call: getBlockLocations took 80ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12693 ] - [ DEBUG ]  Call: getBlockLocations took 80ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12693 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #38
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12693 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #36
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12693 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #39
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12693 ] - [ DEBUG ]  Call: getBlockLocations took 80ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12694 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #35
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12694 ] - [ DEBUG ]  Call: getBlockLocations took 81ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12693 ] - [ DEBUG ]  Call: getBlockLocations took 80ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12694 ] - [ DEBUG ]  Call: getBlockLocations took 83ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12695 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28422744
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854173_11113416; getBlockSize()=28422744; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854173_11113416; getBlockSize()=28422744; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12695 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28620373
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854166_11113409; getBlockSize()=28620373; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854166_11113409; getBlockSize()=28620373; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12697 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28297244
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854032_11113275; getBlockSize()=28297244; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854032_11113275; getBlockSize()=28297244; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12698 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28815653
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854179_11113422; getBlockSize()=28815653; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854179_11113422; getBlockSize()=28815653; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12698 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28377404
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853983_11113226; getBlockSize()=28377404; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853983_11113226; getBlockSize()=28377404; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12698 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28437496
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854034_11113277; getBlockSize()=28437496; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854034_11113277; getBlockSize()=28437496; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12698 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28675845
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853998_11113241; getBlockSize()=28675845; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084853998_11113241; getBlockSize()=28675845; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12698 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28527908
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854141_11113384; getBlockSize()=28527908; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854141_11113384; getBlockSize()=28527908; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12700 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28844730
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854138_11113381; getBlockSize()=28844730; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854138_11113381; getBlockSize()=28844730; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12707 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #41
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12707 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #42
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:12707 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #43
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12707 ] - [ DEBUG ]  Call: getBlockLocations took 50ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12707 ] - [ DEBUG ]  Call: getBlockLocations took 94ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12707 ] - [ DEBUG ]  Call: getBlockLocations took 49ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12714 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28740313
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854119_11113362; getBlockSize()=28740313; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854119_11113362; getBlockSize()=28740313; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12720 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28522397
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854110_11113353; getBlockSize()=28522397; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854110_11113353; getBlockSize()=28522397; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12720 ] - [ DEBUG ]  newInfo = LocatedBlocks{
  fileLength=28744332
  underConstruction=false
  blocks=[LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854018_11113261; getBlockSize()=28744332; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-1011842029-172.22.17.20-1626943979455:blk_1084854018_11113261; getBlockSize()=28744332; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK], DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK], DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]]}
  isLastBlockComplete=true
  ecPolicy=null}
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12748 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@294dd1d6] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12748 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@1e4f892c] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12748 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@3cb9a124] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12748 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@1c3d38d7] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12752 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@1667a90d] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12752 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@12420046] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12752 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@ddf79c2] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12752 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@378f7c6d] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12756 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@4083ab35] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12758 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@f1aeba8] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12761 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@187a29dd] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12768 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@305b149f] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12768 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@7850fc8f] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12768 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@32386177] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12768 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@79059a9a] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12800 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@69ef880d] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12801 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@755cfcf3] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12801 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@349851da] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12801 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@2009d28c] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12801 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@2aba2be3] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12801 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@2ef19525] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12801 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@42b0be7e] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12801 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@73067fee] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12801 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@2b19d6b8] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12801 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@5cf458e8] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12801 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@61e90525] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12802 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@2f8f0ced] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12802 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@4e292005] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12802 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@75901efd] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12802 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@1751a179] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12802 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@4cf873e5] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12820 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@47a1987b] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12820 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@46422513] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12820 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@59a39b4f] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12820 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@5fcd5dcb] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12820 ] - [ DEBUG ]  org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[stamp, event, credit_number] columnTypes=[string, string, string] separator=[[B@5c070468] nullstring=\N lastColumnTakesRest=false timestampFormats=null
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:12908 ] - [ DEBUG ]  Connecting to datanode 172.22.17.19:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:12912 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:12912 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:12910 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:12909 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:12909 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:12909 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:12909 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:12908 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:12908 ] - [ DEBUG ]  Connecting to datanode 172.22.17.19:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:12908 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:12908 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:12917 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:12917 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:12917 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:12917 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:12917 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:12916 ] - [ DEBUG ]  Connecting to datanode 172.22.17.19:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:12916 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:12915 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:12916 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:12916 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:12916 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:12916 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:12916 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:12916 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:12916 ] - [ DEBUG ]  Connecting to datanode 172.22.17.19:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:12916 ] - [ DEBUG ]  Connecting to datanode 172.22.17.19:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:12915 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:12913 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:12913 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:12912 ] - [ DEBUG ]  Connecting to datanode 172.22.17.19:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:12912 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:12912 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:12912 ] - [ DEBUG ]  Connecting to datanode 172.22.17.23:9866
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:12912 ] - [ DEBUG ]  Connecting to datanode 172.22.17.21:9866
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13228 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #50 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13241 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #78 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13242 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #66 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13243 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #77 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13244 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #65 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13245 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #75 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13245 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #76 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13247 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #74 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13247 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #73 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13247 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #72 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13248 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #71 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13248 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #70 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13248 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #69 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13248 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #68 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13248 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #67 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13248 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #64 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13248 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #63 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13248 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #62 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13248 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #61 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13249 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #60 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13249 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #55 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13249 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #56 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13249 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #59 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13249 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #53 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13249 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #47 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13249 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #54 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13249 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #46 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13249 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #48 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13249 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #57 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13249 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #51 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13250 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #58 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13250 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #52 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13250 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #45 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13250 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #44 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13250 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #49 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Parameter Sending Thread #1:13251 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo sending #79 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13254 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #50
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:13255 ] - [ DEBUG ]  Call: getServerDefaults took 29ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13255 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #78
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:13256 ] - [ DEBUG ]  Call: getServerDefaults took 18ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13258 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #66
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13258 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #77
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:13258 ] - [ DEBUG ]  Call: getServerDefaults took 20ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:13258 ] - [ DEBUG ]  Call: getServerDefaults took 21ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13261 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #65
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:13261 ] - [ DEBUG ]  Call: getServerDefaults took 24ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13261 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #75
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:13261 ] - [ DEBUG ]  Call: getServerDefaults took 23ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13264 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #76
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13264 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #73
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:13264 ] - [ DEBUG ]  Call: getServerDefaults took 26ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:13264 ] - [ DEBUG ]  Call: getServerDefaults took 27ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:13264 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.19, datanodeId = DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:13265 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:13265 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:13265 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:13265 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:13265 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:13265 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:13267 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13267 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #72
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:13267 ] - [ DEBUG ]  Call: getServerDefaults took 30ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13267 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #71
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13268 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #69
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:13268 ] - [ DEBUG ]  Call: getServerDefaults took 31ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:13268 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:13268 ] - [ DEBUG ]  Call: getServerDefaults took 31ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13268 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #70
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:13268 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:13268 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:13268 ] - [ DEBUG ]  Call: getServerDefaults took 31ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13268 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #74
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:13268 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:13268 ] - [ DEBUG ]  Call: getServerDefaults took 30ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:13271 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13268 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #68
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13271 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #64
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13271 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #67
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13271 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #62
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13271 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #60
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13271 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #61
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13271 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #55
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:13271 ] - [ DEBUG ]  Call: getServerDefaults took 34ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:13271 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.19, datanodeId = DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:13272 ] - [ DEBUG ]  Call: getServerDefaults took 34ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:13272 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13271 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #63
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13272 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #56
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:13272 ] - [ DEBUG ]  Call: getServerDefaults took 47ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:13272 ] - [ DEBUG ]  Call: getServerDefaults took 35ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:13272 ] - [ DEBUG ]  Call: getServerDefaults took 35ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:13276 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:13272 ] - [ DEBUG ]  Call: getServerDefaults took 35ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:13276 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:13271 ] - [ DEBUG ]  Call: getServerDefaults took 34ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:13276 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:13276 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:13275 ] - [ DEBUG ]  Call: getServerDefaults took 36ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:13275 ] - [ DEBUG ]  Call: getServerDefaults took 47ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13272 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #59
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:13277 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:13276 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.19, datanodeId = DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:13276 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:13277 ] - [ DEBUG ]  Call: getServerDefaults took 45ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13277 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #53
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13277 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #54
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:13277 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:13277 ] - [ DEBUG ]  Call: getServerDefaults took 52ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13277 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #46
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:13277 ] - [ DEBUG ]  Call: getServerDefaults took 52ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13277 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #47
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:13277 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.19, datanodeId = DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:13277 ] - [ DEBUG ]  Call: getServerDefaults took 52ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:13277 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:13278 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:13277 ] - [ DEBUG ]  Call: getServerDefaults took 52ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:13278 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13277 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #48
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:13278 ] - [ DEBUG ]  Call: getServerDefaults took 49ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:13278 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.19, datanodeId = DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13278 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #57
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13278 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #51
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13278 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #52
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:13278 ] - [ DEBUG ]  Call: getServerDefaults took 52ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:13278 ] - [ DEBUG ]  Call: getServerDefaults took 53ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:13278 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13278 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #44
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13279 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #45
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:13279 ] - [ DEBUG ]  Call: getServerDefaults took 51ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:13278 ] - [ DEBUG ]  Call: getServerDefaults took 49ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:13279 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.19, datanodeId = DatanodeInfoWithStorage[172.22.17.19:9866,DS-d08d0a39-d62e-4937-adde-28a2c4a40adc,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:13279 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:13279 ] - [ DEBUG ]  Call: getServerDefaults took 51ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13279 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #58
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13279 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #79
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:13278 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:13279 ] - [ DEBUG ]  Call: getServerDefaults took 41ms
2023-03-31 11:26:08  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:13279 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo got value #49
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:13279 ] - [ DEBUG ]  Call: getServerDefaults took 53ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:13279 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:13279 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:13279 ] - [ DEBUG ]  Call: getServerDefaults took 52ms
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:13279 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.23, datanodeId = DatanodeInfoWithStorage[172.22.17.23:9866,DS-b05aa133-a29b-4483-b5b9-e286eaa6bc08,DISK]
2023-03-31 11:26:08  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:13280 ] - [ DEBUG ]  SASL client skipping handshake in unsecured configuration for addr = /172.22.17.21, datanodeId = DatanodeInfoWithStorage[172.22.17.21:9866,DS-2051ea86-8ac3-4b95-8f51-b5d181ea1b8a,DISK]
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:13550 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:13555 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:13555 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:13564 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:13565 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:13566 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:13780 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:13788 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:13782 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:13791 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:13792 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:13780 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:13793 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:13791 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:13793 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:13837 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:13837 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:13837 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:13844 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:13844 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:13844 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:13844 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:13845 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:13845 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:13850 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:13850 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:13850 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:13850 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:13850 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:13851 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:13868 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:13869 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:13869 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:13868 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:13870 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:13869 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:13870 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:13871 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:13870 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:13937 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:13938 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:13938 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:13937 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:13938 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:13939 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:13969 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:13969 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:13970 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:13979 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:13979 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:13979 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:14270 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:14271 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:14271 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:14274 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:14274 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:14275 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:14274 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:14275 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:14275 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:14278 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:14278 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:14278 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:14283 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:14283 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:14283 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:14292 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:14292 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:14292 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:14306 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:14308 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:14308 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:14310 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:14310 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:14309 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:14308 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:14311 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:14311 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:14319 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:14319 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:14319 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:14345 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:14347 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:14348 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:14346 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:14350 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:14351 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:14355 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:14357 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:09  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:14357 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:14394 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:14394 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:14394 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:14410 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:14410 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:14411 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:14416 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:14416 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:14416 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:14468 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:14469 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:14469 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:14469 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:14470 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:14470 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:14475 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:14475 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:14476 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:14480 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:14480 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:14480 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:14490 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:14492 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:14491 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:14492 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:14493 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:14492 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:14499 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:14499 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:14499 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:14505 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:14505 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:14506 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:14832 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:14833 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:14833 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:14833 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:14833 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:14834 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:14832 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:14835 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:14835 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:14832 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:14835 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:14835 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:14837 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:14837 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:14837 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:14838 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:14838 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:14840 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:14839 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:14841 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:14842 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:14839 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:14839 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:14841 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:14845 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:14846 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:14847 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:14846 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:14848 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:14848 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:14846 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:14849 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:14850 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:14852 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:14853 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:14853 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:14853 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:14853 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:14854 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:14857 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:14857 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:14858 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:14865 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:14866 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:14866 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:14869 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:14869 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:14869 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:14870 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:14870 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:14871 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:14939 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:14942 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:14941 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:14942 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:14944 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:14941 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:14939 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:14956 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:14947 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:14958 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:14942 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:14960 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:14961 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:14962 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:14957 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:14965 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:14966 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:14965 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:14966 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:14966 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:14965 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:14963 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:14971 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:14971 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:14963 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:14974 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:14974 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:14963 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:14975 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:14978 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:14962 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:14979 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:14979 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:14961 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:14979 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:14976 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:14979 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:14981 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:14976 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:14984 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:14984 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:14971 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:14984 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:14985 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:14970 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:14986 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:14988 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:14969 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:14993 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:14993 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:14969 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:14993 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:14966 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:14998 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:14998 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:14966 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:14966 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:15003 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:15005 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:15002 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:15000 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:15013 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:15013 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:14997 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:15016 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:15017 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:14994 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:15017 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:14991 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:15020 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:15021 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:14991 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:15025 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:15025 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:14987 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:14979 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:15028 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:15031 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:15027 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:15034 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:15034 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:15023 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:15034 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:15034 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:15019 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:15037 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:15019 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:15017 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:15040 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:15015 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:15042 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:15042 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:15009 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:15043 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:15006 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:15043 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:15043 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:15041 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:15039 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15109 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15110 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15110 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:15136 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:15138 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:10  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:15138 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:15440 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:15441 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:15441 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:15444 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:15444 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:15445 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:15449 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:15449 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:15449 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:15451 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:15451 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:15451 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:15466 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:15466 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:15466 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:15474 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:15474 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:15474 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:15477 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:15477 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:15478 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:15483 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:15484 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:15484 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15488 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15488 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15488 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:15491 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:15491 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:15491 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:15491 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:15491 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:15491 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ IPC Client (294309744) connection to t-d-datastorage-srv01/172.22.17.20:8020 from renzhuo:15499 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv01/172.22.17.20:8020 from renzhuo: closed
2023-03-31 11:26:11  [ IPC Client (294309744) connection to t-d-datastorage-srv01/172.22.17.20:8020 from renzhuo:15500 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv01/172.22.17.20:8020 from renzhuo: stopped, remaining connections 1
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:15503 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:15503 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:15504 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:15510 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:15510 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:15510 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:15519 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:15519 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:15521 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:15524 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:15524 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:15525 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:15526 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:15526 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:15526 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:15538 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:15539 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:15540 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:15544 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:15545 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:15546 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:15547 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:15546 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:15549 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:15557 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:15558 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:15558 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:15559 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:15559 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:15559 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:15572 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:15572 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:15573 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:15577 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:15578 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:15578 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:15603 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:15604 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:15604 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:15606 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:15607 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:15607 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:15608 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:15609 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:15609 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15608 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15610 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15610 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:15612 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:15614 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:15614 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:15616 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:15617 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:15619 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:15616 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:15619 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:15619 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:15837 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:15837 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:15837 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:15840 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:15840 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:15840 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:15843 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:15847 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:15849 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:15847 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:15849 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:15849 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:15846 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:15849 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:15849 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:15865 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:15868 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:15868 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:15879 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:15879 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:15880 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:15882 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:15882 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:15883 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:15883 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:15885 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:15886 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:15885 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:15886 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:15886 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:15885 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:15886 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:15888 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:15894 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:15895 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:15895 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:15897 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:15899 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:15896 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:15904 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:15905 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:15902 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15914 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15914 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15914 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:15915 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:15916 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:15915 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:15922 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:15922 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:15915 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:15925 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:15925 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:15920 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:15918 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:15925 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:15925 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:15916 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:15926 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:15927 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:15935 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:15937 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:15937 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:15937 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:15937 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:15939 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:15939 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:15938 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:15942 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:15942 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:15937 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:15944 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:15945 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:15945 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15946 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:15949 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15949 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:15951 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:15959 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:15960 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:15960 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:15961 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:15961 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:15962 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:15975 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:15975 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:15977 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:15980 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:15982 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:15985 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:15984 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:15986 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:15989 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16006 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16006 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16006 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16009 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16011 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16011 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:16014 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:16014 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:16015 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:16014 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:16016 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16019 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16017 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:16020 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16023 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16023 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16023 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16024 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:16030 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:16030 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:16030 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:16049 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:16049 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:16049 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16049 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16050 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16051 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:16059 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:16059 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:16065 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:16067 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:16069 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:16069 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:16360 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:16360 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:16360 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:16361 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:16361 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:16361 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:16363 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:16364 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:16364 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16364 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16365 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:11  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16365 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:16424 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16428 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:16428 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:16429 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16428 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16429 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16427 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16435 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16438 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:16427 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:16441 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:16443 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:16426 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:16446 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:16447 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:16426 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:16447 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:16447 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:16426 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:16448 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:16450 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:16425 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:16452 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:16453 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:16424 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:16437 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:16456 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:16456 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16432 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16432 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:16430 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:16429 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:16464 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16429 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16466 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:16465 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:16468 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:16468 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16459 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16471 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:16456 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:16471 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:16472 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:16454 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:16475 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:16473 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:16478 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:16479 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16472 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16482 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16483 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:16469 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:16485 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:16482 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:16491 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:16493 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:16477 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:16495 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:16497 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:16489 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:16498 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:16486 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:16502 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:16514 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:16514 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:16518 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:16515 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:16518 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:16520 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:16514 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:16523 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:16524 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:16527 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:16528 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:16528 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16531 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16531 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16531 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:16541 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:16545 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16542 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16549 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16549 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:16545 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:16551 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:16552 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:16556 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:16563 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:16563 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:16563 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16564 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:16564 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:16569 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:16569 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:16564 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:16574 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:16574 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:16571 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:16575 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:16577 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16567 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16579 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16565 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16580 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16581 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:16602 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:16602 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:16602 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:16604 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:16606 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:16611 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:16610 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:16621 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:16623 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:16610 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:16626 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:16629 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:16633 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:16637 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:16626 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:16644 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16619 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:16617 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:16648 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:16649 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:16615 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:16651 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:16652 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:16611 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:16652 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:16647 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:16655 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:16655 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16646 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16656 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16658 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16646 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16658 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:16645 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:16639 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:16634 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:16662 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16657 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16664 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:16653 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16665 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:16664 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:16708 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:16708 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:16708 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16781 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16784 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16785 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:16882 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:16882 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:16883 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:16884 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:16884 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:16883 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:16883 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:16884 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:16885 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:16892 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:16892 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:16893 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16893 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16893 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16894 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:16912 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:16912 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:16913 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:16920 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:16920 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:16920 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16936 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16939 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:16937 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:16943 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:16941 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:16943 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:16943 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:16941 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16939 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16944 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:16939 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:16944 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:16944 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:16945 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:16945 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:16969 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:16970 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:16970 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:16970 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:16971 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:16971 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:16970 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:16972 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:16972 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16970 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16972 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:16972 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:16969 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:16973 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:16973 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:16992 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:16992 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:16993 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16993 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16995 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:16995 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17006 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17006 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17006 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17009 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17011 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17012 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:17018 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:17021 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:17024 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:17034 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:17034 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:17036 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:17034 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:17037 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:17037 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:17036 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:17037 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:17044 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:17041 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:17045 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:17045 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:17045 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:17048 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:17048 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:17048 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:17050 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:17050 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17052 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17052 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17052 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:17058 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:17058 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:17060 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17065 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17065 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17066 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:17075 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:17075 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:17077 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:17085 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:17087 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:17090 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:17093 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:17097 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:17092 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:17101 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:17101 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:17098 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17095 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17102 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17102 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:17103 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:17103 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:17103 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17095 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17104 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17105 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:17106 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:17107 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:17107 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:17109 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:17109 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:17109 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:17118 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:17118 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:17119 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:17118 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:17123 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:17127 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:17127 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:17121 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:17127 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:17127 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:17124 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:12  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:17127 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17423 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17425 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17425 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:17426 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:17427 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:17427 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:17431 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:17431 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:17431 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:17441 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:17441 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:17441 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17455 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17455 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:17455 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17455 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:17455 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:17456 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:17459 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:17460 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:17460 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:17475 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:17475 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:17476 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:17496 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:17496 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:17497 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:17500 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:17500 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:17500 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17501 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17501 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17502 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:17501 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:17503 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:17504 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17506 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17506 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:17507 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:17508 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:17509 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17510 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:17515 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:17516 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:17516 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17516 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17518 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17519 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:17522 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17523 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:17524 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17524 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17525 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:17524 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17531 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17533 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17536 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:17537 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:17538 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:17538 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:17538 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:17538 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:17538 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:17552 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:17552 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:17553 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:17553 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:17555 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:17556 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:17552 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:17556 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:17556 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:17553 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:17557 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:17557 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:17561 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:17562 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:17562 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:17569 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:17569 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:17569 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:17576 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:17576 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:17576 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:17580 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:17580 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:17581 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17581 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17581 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17581 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17584 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17584 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17584 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:17595 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:17597 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:17597 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:17597 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:17645 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:17645 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:17657 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:17657 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:17663 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:17656 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:17670 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:17670 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:17670 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:17672 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:17674 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:17650 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:17675 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:17675 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17649 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17680 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17710 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:17679 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:17711 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:17712 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:17674 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:17717 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:17674 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:17673 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:17725 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:17673 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:17725 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:17672 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:17725 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:17725 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:17671 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:17728 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:17728 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:17670 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:17730 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:17730 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:17670 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:17669 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:17732 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:17732 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:17669 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:17735 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:17736 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:17668 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:17739 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:17739 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:17668 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:17743 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:17744 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17667 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17744 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17744 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:17666 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:17744 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:17744 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:17662 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:17745 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17660 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17745 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17745 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:17660 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:17748 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:17752 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:17746 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:17753 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:17753 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:17725 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:17725 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:17724 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:17757 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:17718 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:17712 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:17758 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:17759 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:17711 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:17760 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:17760 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:17696 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:17761 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:17762 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:17694 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:17762 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:17762 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:17687 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:17766 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:17766 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17840 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17842 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:17847 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:17959 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:17959 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:17959 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17960 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17962 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:17962 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17961 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17964 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:17964 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17976 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17976 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:17976 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:17978 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:17978 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:17978 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:17994 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:18003 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:18000 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:18006 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:18008 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:17997 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:18008 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:18009 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:18008 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:18010 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:18012 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18006 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18013 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18013 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:18005 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:18015 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:18016 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:18019 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:18013 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:18019 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:18019 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:18008 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:18022 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:18022 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:18025 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:18025 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:18027 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18029 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18033 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18034 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:18032 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:18037 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:18040 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:18037 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:18047 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:18047 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:18045 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:18051 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:18052 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:18045 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:18053 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:18053 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:18042 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:18053 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:18054 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:18051 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:18054 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:18054 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:18049 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:18055 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:18056 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:18060 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:18061 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:18064 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:18060 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:18064 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:18064 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:18092 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:18094 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:18096 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18096 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18101 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18101 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18108 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18108 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18109 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18111 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18113 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18113 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:18113 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:18114 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:18119 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:18121 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:18122 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:18125 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:18127 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:18127 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:18128 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:18133 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:18133 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:18130 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:18149 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:18152 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:18152 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:18156 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:18156 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:18156 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:18159 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:18163 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:18163 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18332 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18332 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:13  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18332 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18463 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:18465 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:18465 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:18464 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:18466 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:18463 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:18466 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:18466 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:18466 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18465 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18466 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:18466 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18468 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18468 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18468 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:18469 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:18469 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:18469 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:18479 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:18479 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:18480 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:18491 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:18492 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:18494 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:18496 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:18496 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:18496 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:18499 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:18499 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:18500 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:18499 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:18502 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:18502 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:18511 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:18511 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:18511 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18519 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18520 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18520 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:18519 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:18520 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:18520 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:18530 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:18532 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:18533 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18536 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18538 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18538 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:18544 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:18545 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:18546 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18545 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18549 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:18547 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:18550 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:18550 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18550 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:18553 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:18555 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:18555 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:18561 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:18561 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:18561 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:18576 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:18578 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:18578 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:18577 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:18580 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:18581 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:18583 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:18585 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:18585 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:18595 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:18595 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:18595 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18604 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18606 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18607 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:18630 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:18632 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:18632 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:18649 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:18652 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:18652 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:18652 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:18655 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:18657 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:18657 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:18658 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:18661 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:18660 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:18662 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:18665 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18661 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18665 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:18662 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:18668 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:18669 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18665 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:18675 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:18676 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:18678 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:18684 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:18686 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:18686 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:18689 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:18691 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:18692 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:18694 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:18696 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:18698 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18697 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18698 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18699 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:18703 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:18704 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:18708 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:18704 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:18711 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:18712 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:18714 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:18715 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:18718 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:18742 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:18743 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:18745 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:18745 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:18746 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:18748 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:18748 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:18752 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:18752 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:18748 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:18754 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:18755 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18767 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18768 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:18768 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18769 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18769 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18769 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ flink-akka.actor.default-dispatcher-7:18907 ] - [ DEBUG ]  Trigger heartbeat request.
2023-03-31 11:26:14  [ flink-akka.actor.default-dispatcher-8:18913 ] - [ DEBUG ]  Received heartbeat request from 92cab976fd905fb7e0259b0d2181c44e.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:18921 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:18922 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:18922 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:18921 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:18922 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:18922 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ flink-akka.actor.default-dispatcher-7:18929 ] - [ DEBUG ]  Trigger heartbeat request.
2023-03-31 11:26:14  [ flink-akka.actor.default-dispatcher-10:18931 ] - [ DEBUG ]  Received heartbeat request from 92cab976fd905fb7e0259b0d2181c44e.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:18942 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:18943 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:18943 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:18944 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:18944 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:18944 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18947 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18947 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:18947 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ flink-akka.actor.default-dispatcher-7:18950 ] - [ DEBUG ]  Received heartbeat from 702a68294258f44743070553da3cd928.
2023-03-31 11:26:14  [ flink-akka.actor.default-dispatcher-7:18950 ] - [ DEBUG ]  Received heartbeat from 66527030-9c98-43a4-b1f5-6190c00b39ae.
2023-03-31 11:26:14  [ flink-akka.actor.default-dispatcher-7:18966 ] - [ DEBUG ]  Received slot report from instance 36c46d75f0d6709eed7e91e0e296f01a: SlotReport{SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_0, allocationID=1115d5439ef2cf0f882dd10b19cda519, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_1, allocationID=e513fdd5c4b1f46fd684e15d482ed8d6, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_2, allocationID=43352989c37f53018f9b85386d62156a, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_3, allocationID=ad3de297d0b909d6b7bc2396ceaa2e26, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_4, allocationID=9b0bc5af62966b679b2a4a0f2bfa483f, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_5, allocationID=96e72c04c02b602173c636ffb4b26af7, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_6, allocationID=f47ce2af1de0ed2306225306c3d26dfe, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_7, allocationID=30eb83318128e345d3f24db5bb02ea14, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_8, allocationID=4cf541597e0efa5c4c02d18777aa0ff3, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_9, allocationID=32204247f2a9dcbb3f64030bc03c896c, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_10, allocationID=ad9cd0d0c392e6fd77603f6af858c8d0, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_11, allocationID=916a0304e1b3a002e37bce0a5a3d6a91, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_12, allocationID=c2a6a50e69242d36c1517984ba118d27, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_13, allocationID=1c857d3bd2c64719d4eb43e86d804337, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_14, allocationID=27393d5d22fc71fa3675aa19b69683f2, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_15, allocationID=529db54fdf957f05051f2a9705e53dbb, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_16, allocationID=22a91adbd20099800ddd036173df6f0c, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_17, allocationID=11097abd215e6451f2e95b37fe86699a, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_18, allocationID=b0ffa6ffd647ebc3dd5e4a07d4f4a3b6, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_19, allocationID=51ca3690186c6f89309474a25879bbf5, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_20, allocationID=36086a0f68b34902fe232fae71009313, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_21, allocationID=d4a609ea419f71be34cb5de07aa2a014, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_22, allocationID=275e91a727866e4043757d69ead9e2ab, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_23, allocationID=0cf73c7f9304a1056efe98b0f6f517df, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_24, allocationID=4f34c734ce3549c59d170e43098814ec, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_25, allocationID=178afb1de8d4b4f843316406dbcc6c67, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_26, allocationID=06d1567f24d75f9b5dae99b52084c1a4, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_27, allocationID=cd29661e846913336458ae880a376e5b, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_28, allocationID=ecf8a41a8ff33e11a5134e31acbb5e89, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_29, allocationID=3fa0eff69efa0bd8ffb5d90f845c4693, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_30, allocationID=29bff3f54d94023cdd813b9ee98997b1, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_31, allocationID=106f0ba9ba38260c4ed7492d2955d689, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_32, allocationID=46827fcd58daec7152008dd28742381a, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_33, allocationID=19602b51c9127c777128cba1de2ca8db, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_34, allocationID=fdf2ddb6217f04cda389971a9af4a223, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}SlotStatus{slotID=66527030-9c98-43a4-b1f5-6190c00b39ae_35, allocationID=d0c10e9b833c29c8e13dc6e3b5cd71cb, jobID=ba766f85be2130d6661f17297a4a9f8b, resourceProfile=ResourceProfile{taskHeapMemory=28.444gb (30541989660 bytes), taskOffHeapMemory=28.444gb (30541989660 bytes), managedMemory=3.556mb (3728270 bytes), networkMemory=1.778mb (1864135 bytes)}}}.
2023-03-31 11:26:14  [ flink-akka.actor.default-dispatcher-7:18969 ] - [ DEBUG ]  Processing cluster partition report from task executor 66527030-9c98-43a4-b1f5-6190c00b39ae: PartitionReport{entries=[]}.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:18979 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:18979 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:18979 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18979 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18981 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:18981 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:19003 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:19003 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:19003 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:19012 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:19012 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:19012 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:19013 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:19013 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:19013 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:19018 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:19018 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:19019 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:19022 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:19024 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:19025 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:19023 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:19026 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:19027 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:19032 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:19033 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:19033 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:19038 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:19039 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:19039 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:19038 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:19043 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:19043 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:19047 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:19048 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:19050 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:19084 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:19085 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:19085 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:19089 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:19090 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:19121 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:19122 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:19122 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:19119 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:19123 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:19123 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:19119 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:19125 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:19119 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:19127 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:19128 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:19118 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:19130 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:19132 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:19118 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:19133 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:19133 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:19116 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:19134 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:19134 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:19115 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:19134 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:19135 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:19129 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:19139 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:19143 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:19129 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:19143 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:19144 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:19128 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:19145 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:19145 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:19125 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:19121 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:19154 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:19154 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:19154 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:19158 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:19159 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:19158 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:19161 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:19161 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:19159 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:19165 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:19166 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:19168 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:19167 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:19168 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:19169 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:19171 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:19171 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:19174 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:19191 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:19194 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:19197 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:19209 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:19211 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:19209 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:19212 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:19214 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:14  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:19211 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:19452 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:19453 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:19453 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:19453 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:19453 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:19453 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:19453 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:19454 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:19454 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:19457 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:19457 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:19457 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:19467 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:19467 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:19468 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:19479 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:19480 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:19481 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:19481 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:19484 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:19484 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:19501 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:19503 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:19503 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:19505 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:19505 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:19505 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:19508 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:19510 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:19510 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:19521 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:19521 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:19522 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:19536 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:19536 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:19536 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:19547 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:19547 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:19548 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:19548 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:19549 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:19550 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:19626 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:19626 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:19626 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ flink-akka.actor.default-dispatcher-7:19691 ] - [ DEBUG ]  Trigger heartbeat request.
2023-03-31 11:26:15  [ flink-akka.actor.default-dispatcher-8:19693 ] - [ DEBUG ]  Received heartbeat request from 702a68294258f44743070553da3cd928.
2023-03-31 11:26:15  [ flink-akka.actor.default-dispatcher-7:19726 ] - [ DEBUG ]  Received heartbeat from 66527030-9c98-43a4-b1f5-6190c00b39ae.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:19742 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:19742 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:19744 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:19746 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:19746 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:19746 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:19750 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:19750 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:19750 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:19789 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:19789 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:19789 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:19793 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:19794 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:19794 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:19798 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:19798 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:19798 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:19801 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:19801 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:19801 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:19809 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:19810 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:19809 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:19810 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:19810 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:19810 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:19965 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:19965 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:19966 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:19968 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:19968 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:19968 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:20003 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:20003 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:20003 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20030 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20030 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20030 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:20030 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:20031 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:20031 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:20038 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:20038 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:20039 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:20041 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:20042 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:20042 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:20109 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:20109 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:20109 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:20111 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:20111 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:20111 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:20117 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:20117 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:20118 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:20133 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:20133 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:20134 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:20133 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:20135 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:20135 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:20136 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:20136 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:20136 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:20137 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:20138 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:20139 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:20141 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:20141 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:20141 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:20143 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:20143 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:20144 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:20143 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:20146 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:20147 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:20147 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:20147 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:20148 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20148 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20150 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20150 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:20149 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:20150 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:20150 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:20164 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:20164 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:20164 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:20187 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:20189 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:20189 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:20188 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:20190 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:20190 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:20196 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:20196 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:20198 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:20208 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:20208 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:20210 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:20248 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:20250 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:20250 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:20249 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:20251 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:20251 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:20252 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:20255 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:15  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:20256 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:20557 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:20558 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:20558 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:20564 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:20564 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:20564 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20593 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20593 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20593 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:20607 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:20607 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:20608 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:20612 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:20612 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:20613 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:20619 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:20619 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:20620 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:20623 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:20623 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:20624 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:20656 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:20656 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:20656 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:20663 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:20663 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:20663 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:20669 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:20670 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:20670 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:20680 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:20680 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:20680 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:20689 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:20689 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:20689 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:20693 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:20694 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:20695 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:20694 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:20695 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:20695 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:20715 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:20716 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:20716 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:20720 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:20720 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:20720 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20720 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20720 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20720 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:20727 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:20728 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:20728 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:20770 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:20771 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:20771 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:20778 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:20779 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:20781 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:20780 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:20783 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:20783 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:20783 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:20784 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:20785 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:20799 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:20800 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:20800 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:20805 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:20805 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:20805 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:20812 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:20815 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:20816 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:20814 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:20817 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:20817 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:20813 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:20817 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:20817 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20819 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20819 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:20821 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:20819 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:20822 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:20823 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:20835 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:20839 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:20840 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:20845 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:20846 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:20846 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:20851 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:20851 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:20853 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:20851 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:20857 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:20859 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:20853 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:20861 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:20861 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:20853 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:20862 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:20862 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:21088 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:21088 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:21088 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:21089 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:21089 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:21089 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:21096 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:21096 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:21096 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:21100 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:21101 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:21101 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:21100 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:21101 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:21101 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21118 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21118 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21118 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:21121 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:21122 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:21122 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:21148 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:21148 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:21148 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:21159 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:21159 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:21159 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:21159 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:21160 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:21160 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:21161 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:21161 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:21162 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:21168 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:21169 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:21169 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:21182 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:21183 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:21184 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:21186 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:21186 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:21186 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:21196 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:21196 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:21196 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:21196 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:21197 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:21197 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21203 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21203 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21203 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:21215 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:21215 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:21216 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:21218 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:21218 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:21218 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:21221 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:21222 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:21223 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:21224 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:21226 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:21226 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:21226 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:21226 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:21227 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:21239 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:21242 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:21242 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:21241 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:21244 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:21244 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:21245 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:21245 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:21245 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:21246 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:21246 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:21247 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:21253 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:21255 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:21255 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:21259 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:21255 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:21260 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:21258 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:21260 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:21260 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:21272 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:21273 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:21273 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21304 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21305 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21306 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:21311 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:21313 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:16  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:21313 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:21602 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:21607 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:21602 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:21608 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:21607 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:21609 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:21607 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:21609 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:21609 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:21608 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:21610 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:21610 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:21619 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:21620 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:21620 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:21619 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:21620 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:21620 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:21660 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:21660 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:21661 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:21665 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:21665 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:21665 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:21668 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:21668 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:21668 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:21668 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:21669 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:21669 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:21674 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:21675 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:21675 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:21689 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:21689 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:21689 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:21689 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:21690 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:21690 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:21695 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:21696 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:21696 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:21699 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:21699 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:21699 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:21713 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:21713 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:21713 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:21713 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:21715 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:21715 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:21715 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:21716 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:21717 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:21716 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:21718 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:21719 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:21717 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:21721 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:21721 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21721 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21721 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21723 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:21721 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:21728 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:21731 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:21726 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:21732 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:21732 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:21747 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:21749 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:21751 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:21758 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:21759 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:21759 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:21759 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:21762 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:21762 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:21758 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:21764 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:21764 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21760 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21764 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:21764 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:21795 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:21796 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:21796 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:21802 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:21803 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:21807 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:21803 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:21811 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:21814 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:21803 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:21814 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:21817 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:21814 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:21820 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:21822 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:21831 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:21832 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:21832 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:21831 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:21835 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:21835 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:21837 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:21837 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:21839 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:21837 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:21842 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:21844 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:21846 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:21851 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:21846 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:21856 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:21858 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:21855 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:21859 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:21861 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:21861 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:21862 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:21862 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:21862 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:21871 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:21873 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:21873 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:21888 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:21888 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:21888 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:21898 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:21899 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:21898 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:21902 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:21902 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:21898 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:21905 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:21905 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:21901 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:21907 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:21910 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:21908 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:21912 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:21910 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:21914 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:22124 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:22124 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:22124 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:22126 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:22126 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:22126 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:22127 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:22128 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:22128 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:22132 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:22133 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:22133 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:22138 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:22138 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:22138 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:22143 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:22143 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:22144 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:22143 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:22145 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:22145 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:22143 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:22146 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:22146 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:22157 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:22157 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:22158 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:22167 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:22167 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:22167 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:22177 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:22177 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:22177 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:22186 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:22366 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:22553 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:22553 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:22366 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:22554 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:22554 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:22351 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:22558 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:22558 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:22344 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:22563 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:22563 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:22339 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:22567 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:22567 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:22337 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:22567 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:22330 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:22577 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:22318 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:22579 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:22317 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:22581 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:22583 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:22316 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:22586 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:22586 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:22300 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:22591 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:22592 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:22300 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:22595 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:22598 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:22295 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:22602 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:22602 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:22284 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:22603 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:22283 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:22605 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:22605 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:22261 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:22606 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:22258 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:22606 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:22606 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:22252 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:22609 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:22609 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:22251 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:22609 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:22610 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:22247 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:22612 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:22612 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:22240 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:22614 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:22614 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:22237 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:22617 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:22617 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:22236 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:22621 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:22621 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:22236 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:22625 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:22625 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:22230 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:22627 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:22628 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:22229 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:22631 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:22631 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:22229 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:22631 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:22634 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:22225 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:22637 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:22638 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:22222 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:22643 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:22643 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:22220 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:22647 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:22648 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:22201 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:22649 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:22649 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:22194 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:22649 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:22649 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:17  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:22186 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:22650 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:22650 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:22645 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:22656 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:22658 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:22606 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:22603 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:22579 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:22577 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:22574 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:22552 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:22671 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:22672 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:22667 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:22675 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:22660 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:22676 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:22677 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:22660 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:22682 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:22656 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:22682 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:22684 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:22651 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:22684 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:22684 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:22650 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:22685 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:22683 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:22686 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:22686 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:22682 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:22679 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:22691 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:22693 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:22676 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:22672 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:22694 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:22692 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:22697 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:22699 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:22686 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:22701 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:22701 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:22685 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:22718 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:22721 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:22721 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:22725 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:22725 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:22725 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:22728 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:22733 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:22733 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:22728 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:22736 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:22741 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:22728 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:22743 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:22745 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:22745 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:22750 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:22750 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:22754 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:22755 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:22755 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:22763 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:22763 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:22763 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:22768 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:22768 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:22768 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:22775 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:22778 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:22779 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:22780 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:22780 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:22781 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:22795 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:22798 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:22799 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:22813 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:22815 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:22819 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:22818 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:22820 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:22822 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:22816 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:22824 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:22827 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:22820 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:22828 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:22831 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:22834 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:22836 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:22837 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:22835 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:22837 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:22837 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:22845 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:22848 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:22847 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:22852 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:22847 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:22852 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:22855 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:22852 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:22852 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:22859 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:22859 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:22851 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:22862 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:22862 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:22848 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:22863 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:22864 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:22864 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:22866 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:22869 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:22873 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:22876 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:22877 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:22877 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:22892 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:22892 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:22894 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:22909 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:22910 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:22911 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:22913 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:22919 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:22919 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:22918 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:22922 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:22925 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:22921 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:22930 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:22930 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:22926 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:22930 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:22933 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:22925 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:22936 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:22936 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:22940 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:22944 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:22944 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:22938 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:22948 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:22948 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:22947 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:22949 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:22949 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:22947 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:22952 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:22952 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:22956 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:22956 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:22956 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:22956 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:22956 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:22956 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:22989 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:22989 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:22989 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:23085 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:23086 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:23086 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:23178 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:23180 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:23180 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:23184 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:23184 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:23184 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:23180 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:23190 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:23192 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:23194 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:23195 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:23195 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:23195 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:23195 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:23197 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:23195 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:23197 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:23197 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:23200 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:23201 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:23203 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:23200 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:23206 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:23206 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:23205 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:23208 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:23208 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:23202 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:23208 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:23208 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:23220 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:23220 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:23221 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:23220 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:23222 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:23223 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:23345 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:23350 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:23349 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo: closed
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:23349 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:23357 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:23359 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:23349 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:23364 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:23364 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:23348 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:23364 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:23367 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:23348 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:23370 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:23370 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:23347 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:23370 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:23370 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:23346 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:23370 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:23370 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:23346 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:23370 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:23370 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:23346 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:23371 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:23371 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:23345 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:23371 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:23372 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:23345 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:23378 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:23381 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:23370 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:23386 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:23386 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:23366 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:23391 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:23395 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:23363 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:23395 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:23397 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:18  [ IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo:23355 ] - [ DEBUG ]  IPC Client (294309744) connection to t-d-datastorage-srv02/172.22.17.21:8020 from renzhuo: stopped, remaining connections 0
2023-03-31 11:26:18  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:23352 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:23390 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:23397 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:23400 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:23387 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:23406 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:23407 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:23381 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:23412 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:23413 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:23409 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:23416 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:23416 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:23407 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:23416 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:23416 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:23400 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:23416 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:23416 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:23414 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:23417 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:23417 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:23493 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:23494 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:23494 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:23651 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:23652 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:23652 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:23708 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:23708 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:23710 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:23731 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:23732 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:23732 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:23732 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:23733 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:23733 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:23734 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:23735 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:23735 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:23744 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:23745 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:23745 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:23746 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:23749 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:23749 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:23752 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:23753 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:23753 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:23760 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:23760 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:23760 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:23760 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:23765 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:23765 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:23769 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:23773 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:23763 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:23761 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:23778 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:23779 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:23761 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:23782 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:23782 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:23760 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:23782 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:23786 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:23781 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:23788 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:23788 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:23778 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:23789 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:23789 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:23775 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:23791 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:23793 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:23771 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:23793 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:23795 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:23771 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:23795 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:23795 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:23771 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:23802 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:23803 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:23769 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:23805 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:23805 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:23799 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:23805 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:23805 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:23799 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:23808 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:23808 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:23792 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:23811 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:23811 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:23818 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:23819 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:23823 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:23824 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:23827 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:23827 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:23827 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:23831 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:23833 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:23834 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:23838 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:23838 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:23835 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:23838 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:23842 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:23850 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:23853 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:23853 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:23858 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:23860 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:23862 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:23859 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:23864 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:23864 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:23870 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:23870 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:23874 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:23889 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:23894 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:23895 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:23891 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:23901 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:23901 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:23907 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:23907 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:23907 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:23908 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:23909 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:23909 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:23911 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:23913 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:23914 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:23921 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:23922 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:23924 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:23921 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:23927 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:23927 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:23927 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:23929 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:23929 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:23956 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:23956 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:23959 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:23958 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:23963 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:23964 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:23967 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:23968 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:23967 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:23969 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:23969 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:23969 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:23973 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:23977 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:23979 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:23973 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:23983 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:23984 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:23971 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:23990 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:23990 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:23970 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:23995 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:23990 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:24000 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:23986 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:24014 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:24022 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:23986 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:24038 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:24038 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:23984 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:24043 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:24043 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:23981 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:24058 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:24058 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:23978 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:24073 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:24081 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:23976 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:24087 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:24007 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:24004 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:24095 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:24000 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:24100 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:24088 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:24168 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:24169 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:24170 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:24181 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:24183 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:24183 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:24182 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:24190 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:24190 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:24188 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:24210 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:24208 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:24218 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:24207 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:24237 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:24196 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:24240 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:24240 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:24196 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:24240 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:24240 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:24238 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:24247 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:24247 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:24237 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:24234 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:24261 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:24226 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:24295 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:24307 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:24224 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:24311 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:24311 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:24222 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:24320 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:24217 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:24320 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:24293 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:24334 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:24338 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:24293 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:24344 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:24349 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:24292 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:24353 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:24353 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:24263 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:24242 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:24362 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:24369 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:24369 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:24359 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:24375 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:24378 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:24359 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:24378 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:24378 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:24343 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:24386 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:24396 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:24343 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:24416 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:24416 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:24341 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:24417 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:24417 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:24418 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:24418 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:24418 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:24340 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:24421 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:24437 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:24403 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:24439 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:24439 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:24401 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:24447 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:24447 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:24377 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:24448 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:24448 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:19  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:24372 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:24449 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:24449 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:24441 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:24450 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:24463 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:24435 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:24478 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:24494 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:24433 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:24494 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:24502 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:24554 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:24555 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:24555 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:24569 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:24570 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:24571 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:24578 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:24579 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:24579 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:24600 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:24601 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:24602 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:24687 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:24688 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:24688 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:24691 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:24693 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:24696 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:24692 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:24696 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:24696 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:24696 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:24703 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:24705 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:24706 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:24706 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:24708 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:24720 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:24723 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:24724 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:24724 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:24724 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:24726 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:24737 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:24737 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:24737 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:24743 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:24745 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:24746 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:24786 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:25417 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:25412 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:25419 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:25419 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:25045 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:25438 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:25438 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:25044 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:25451 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:25451 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:24975 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:24969 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:25457 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:24965 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:25461 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:25462 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:24964 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:25472 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:24957 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:25479 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:25493 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:24956 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:25503 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:25508 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:24951 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:25514 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:25517 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:24950 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:25517 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:25519 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:24946 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:25523 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:24938 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:25530 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:24933 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:25537 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:25545 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:24933 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:25547 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:25547 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:24911 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:25547 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:25550 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:24911 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:25551 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:25551 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:24878 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:25559 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:25559 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:24821 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:25559 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:24819 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:25693 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:24814 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:25695 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:25696 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:24812 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:25698 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:25700 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:20  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:24791 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:25701 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:25703 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:25695 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:25693 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:25536 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:25525 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:25503 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:25711 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:25491 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:25714 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:25714 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:25474 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:25458 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:25457 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:25715 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:25451 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:25718 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:25718 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:25419 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:25718 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:25718 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:25718 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:25713 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:25712 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:25723 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:25723 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:25704 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:25723 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:25727 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:25733 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:25734 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:25734 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:25741 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:25742 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:25743 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:25773 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:25774 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:25774 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:25785 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:25786 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:25789 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:25793 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:25797 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:25797 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:25798 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:25802 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:25802 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:25821 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:25821 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:25822 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:25822 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:25825 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:25825 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:25833 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:25834 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:25834 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:25852 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:25852 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:25852 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:25870 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:25874 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:25874 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:25898 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:25901 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:25901 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:25899 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:25905 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:25904 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:25905 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:25906 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:25906 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:25906 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:25905 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:25906 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:25911 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:25916 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:25920 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:25917 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:25924 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:25917 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:25928 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:25930 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:25917 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:25933 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:25933 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:25917 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:25933 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:25933 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:25928 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:25935 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:25938 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:25940 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:25952 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:25953 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:25957 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:25955 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:25960 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:25960 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:25973 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:25976 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:25976 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:25974 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:25976 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:25976 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:25988 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:25989 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:25991 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:25988 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:25991 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:25991 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:25989 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:25996 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:25996 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:26006 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:26006 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:26006 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:26042 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:26046 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:26047 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:26071 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:26072 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:26073 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26075 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26077 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26077 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:26088 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:26091 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:26089 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:26094 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:26095 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:26092 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:26098 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:26101 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:26102 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:26136 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:26140 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:26144 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:26219 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:26221 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:26221 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:26233 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:26233 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:26233 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:26242 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:26243 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:26243 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:26254 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:26254 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:26254 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:26256 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:26258 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:26260 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:26293 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:26293 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:21  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:26294 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:26383 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:26385 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:26386 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:26405 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:26405 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:26406 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:26408 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:26410 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:26411 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:26411 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:26411 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:26413 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:26414 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:26414 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:26415 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26436 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26436 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26437 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:26469 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:26470 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:26470 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:26470 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:26472 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:26474 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:26474 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:26475 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:26475 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:26485 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:26487 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:26487 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:26490 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:26491 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:26491 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:26493 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:26494 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:26494 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:26502 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:26503 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:26505 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:26523 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:26524 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:26524 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:26547 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:26547 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:26547 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:26597 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:26598 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:26598 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:26648 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:26650 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:26651 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:26666 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:26667 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:26668 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:26678 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:26678 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:26678 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:26728 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:26729 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:26730 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26773 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26774 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26774 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:26785 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:26786 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:26786 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:26801 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:26801 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:26801 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:26820 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:26820 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:26821 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:26851 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:26852 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:26852 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:26888 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:26889 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:26890 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26896 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26897 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26898 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:26920 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:26921 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:26923 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:26973 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:26974 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:26975 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:26975 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:26975 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:26975 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:26984 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:26985 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:26984 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:26986 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:26984 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:26987 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:26987 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:26986 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:26988 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26990 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26991 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:26992 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:27002 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:27003 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:27003 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:27014 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:27017 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:27018 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:27015 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:27018 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:27019 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:27014 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:27022 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:27022 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:27024 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:27024 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:27025 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:27044 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:27045 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:27045 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:27045 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:27045 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:27045 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:27050 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:27051 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:27052 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:27065 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:27067 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:27067 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27067 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27067 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27067 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:27078 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:27078 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:27078 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:27119 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:27119 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:27121 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:27126 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:27126 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:27126 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:27127 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:27129 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:27129 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:27127 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:27133 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:27133 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:27133 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:27143 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:27129 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:27144 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:27145 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:27143 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:27147 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:27148 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:27148 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27262 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27263 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27263 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:27321 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:27322 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:27323 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27340 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27341 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27341 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27364 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27364 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:22  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27364 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:27375 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:27375 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:27375 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27378 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27378 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27378 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27393 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27393 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:27393 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:27428 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:27428 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:27428 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:27428 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:27428 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:27429 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:27448 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:27448 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:27448 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:27479 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:27479 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:27479 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:27505 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:27505 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (11/36)#0:27505 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:27507 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:27507 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:27507 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:27511 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:27511 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:27511 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:27529 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:27529 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:27530 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:27551 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:27551 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:27551 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:27558 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:27558 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:27558 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:27590 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:27590 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:27590 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:27943 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:27944 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:27944 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:27957 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:27957 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:27957 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:27962 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:27962 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:27962 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:27990 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:27990 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:27990 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:27994 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:27994 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (21/36)#0:27994 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:28001 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:28001 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:28001 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:28016 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:28016 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:28016 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:28016 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:28016 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:28017 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:28017 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:28017 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:28018 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:28018 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:28019 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:28018 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:28020 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:28021 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:28022 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:28028 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:28029 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:28029 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:28030 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:28031 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:28030 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:28032 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:28032 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:28033 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:28036 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:28037 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:28037 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:28048 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:28049 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:28049 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:28071 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:28072 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:28072 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:28077 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:28077 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (10/36)#0:28079 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:28081 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:28083 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (16/36)#0:28083 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:28106 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:28108 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:28109 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:28112 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:28113 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (27/36)#0:28113 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:28119 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:28119 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:28120 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:28153 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:28153 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:28154 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:28154 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:28154 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:28154 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:28187 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:28187 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:28188 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:28192 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:28192 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (23/36)#0:28192 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:28195 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:28195 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:23  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (33/36)#0:28195 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:28390 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:28390 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (26/36)#0:28390 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:28390 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:28391 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (30/36)#0:28391 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:28396 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:28397 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:28397 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:28417 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:28418 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (15/36)#0:28418 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:28428 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:28429 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (8/36)#0:28429 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:28428 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:28430 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (18/36)#0:28430 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:28434 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:28434 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (5/36)#0:28434 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:28447 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:28447 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (22/36)#0:28447 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:28457 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:28457 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (32/36)#0:28457 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:28462 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:28463 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (3/36)#0:28464 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:28491 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:28491 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (6/36)#0:28491 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:28503 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:28503 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (4/36)#0:28503 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:28512 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:28512 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (7/36)#0:28512 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:28517 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:28517 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:28517 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:28524 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:28524 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:28524 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:28525 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:28526 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (13/36)#0:28526 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:28529 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:28529 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:28529 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:28547 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:28547 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (31/36)#0:28547 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:28547 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:28548 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (19/36)#0:28548 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:28558 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:28561 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:28560 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:28564 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:28559 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:28569 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (24/36)#0:28569 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:28566 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (1/36)#0:28562 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:28589 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:28591 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (35/36)#0:28591 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ PermanentBlobCache shutdown hook:28600 ] - [ INFO ]  Shutting down BLOB cache
2023-03-31 11:26:24  [ IOManagerAsync shutdown hook:28607 ] - [ DEBUG ]  Shutting down I/O manager.
2023-03-31 11:26:24  [ TransientBlobCache shutdown hook:28605 ] - [ INFO ]  Shutting down BLOB cache
2023-03-31 11:26:24  [ TaskExecutorStateChangelogStoragesManager shutdown hook:28613 ] - [ INFO ]  Shutting down TaskExecutorStateChangelogStoragesManager.
2023-03-31 11:26:24  [ TaskExecutorLocalStateStoresManager shutdown hook:28616 ] - [ INFO ]  Shutting down TaskExecutorLocalStateStoresManager.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:28612 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:28628 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (17/36)#0:28629 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:28629 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:28629 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (28/36)#0:28629 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:28630 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:28631 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (14/36)#0:28631 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:28631 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:28633 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (34/36)#0:28633 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:28631 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:28635 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (9/36)#0:28635 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:28638 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:28639 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:28639 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:28640 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (29/36)#0:28640 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (12/36)#0:28640 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:28642 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:28642 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (2/36)#0:28642 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:28642 ] - [ DEBUG ]  Finished running task FetchTask
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:28646 ] - [ DEBUG ]  Cleaned wakeup flag.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (20/36)#0:28648 ] - [ DEBUG ]  Prepare to run FetchTask
2023-03-31 11:26:24  [ pool-1-thread-1:28669 ] - [ DEBUG ]  stopping client from cache: org.apache.hadoop.ipc.Client@5c313224
2023-03-31 11:26:24  [ pool-1-thread-1:28675 ] - [ DEBUG ]  stopping client from cache: org.apache.hadoop.ipc.Client@5c313224
2023-03-31 11:26:24  [ pool-1-thread-1:28675 ] - [ DEBUG ]  removing client from cache: org.apache.hadoop.ipc.Client@5c313224
2023-03-31 11:26:24  [ pool-1-thread-1:28676 ] - [ DEBUG ]  stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@5c313224
2023-03-31 11:26:24  [ pool-1-thread-1:28678 ] - [ DEBUG ]  Stopping client
2023-03-31 11:26:24  [ Thread-2:28702 ] - [ DEBUG ]  ShutdownHookManger complete shutdown.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:28705 ] - [ ERROR ]  Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:474)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:740)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:813)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:218)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:176)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:255)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.flink.connectors.hive.read.HiveMapredSplitReader.reachedEnd(HiveMapredSplitReader.java:166)
	at org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter$HiveReader.readBatch(HiveBulkFormatAdapter.java:332)
	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
	... 6 more
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:28704 ] - [ ERROR ]  Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:474)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:740)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:813)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:218)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:176)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:255)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.flink.connectors.hive.read.HiveMapredSplitReader.reachedEnd(HiveMapredSplitReader.java:166)
	at org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter$HiveReader.readBatch(HiveBulkFormatAdapter.java:332)
	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
	... 6 more
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:28737 ] - [ ERROR ]  Received uncaught exception.
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:474)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:654)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at org.apache.hadoop.util.LineReader.close(LineReader.java:152)
	at org.apache.hadoop.mapred.LineRecordReader.close(LineRecordReader.java:291)
	at org.apache.flink.connectors.hive.read.HiveMapredSplitReader.close(HiveMapredSplitReader.java:205)
	at org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter$HiveReader.close(HiveBulkFormatAdapter.java:346)
	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.close(FileSourceSplitReader.java:92)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:111)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-03-31 11:26:24  [ BlobServer shutdown hook:28714 ] - [ INFO ]  Stopped BLOB server at 0.0.0.0:64726
2023-03-31 11:26:24  [ FileChannelManagerImpl-io shutdown hook:28709 ] - [ INFO ]  FileChannelManager removed spill file directory /var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/flink-io-781524aa-3237-45ec-894c-0d6f2c700e32
2023-03-31 11:26:24  [ FileChannelManagerImpl-netty-shuffle shutdown hook:28707 ] - [ INFO ]  FileChannelManager removed spill file directory /var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/flink-netty-shuffle-f8c39543-2025-440d-bb7e-e48ce3832b09
2023-03-31 11:26:24  [ FileCache shutdown hook:28705 ] - [ INFO ]  removed file cache directory /var/folders/0q/6gj5hsjn61s17q78d8lj21980000gn/T/flink-dist-cache-3e67afd9-4929-4121-b541-34bad06b44fe
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:28752 ] - [ INFO ]  Split fetcher 0 exited.
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:28750 ] - [ ERROR ]  Received uncaught exception.
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:474)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:654)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at org.apache.hadoop.util.LineReader.close(LineReader.java:152)
	at org.apache.hadoop.mapred.LineRecordReader.close(LineRecordReader.java:291)
	at org.apache.flink.connectors.hive.read.HiveMapredSplitReader.close(HiveMapredSplitReader.java:205)
	at org.apache.flink.connectors.hive.read.HiveBulkFormatAdapter$HiveReader.close(HiveBulkFormatAdapter.java:346)
	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.close(FileSourceSplitReader.java:92)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:111)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
2023-03-31 11:26:24  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:28749 ] - [ DEBUG ]  Cleanup StreamTask (operators closed: false, cancelled: false)
2023-03-31 11:26:24  [ Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (25/36)#0:28749 ] - [ DEBUG ]  Cleanup StreamTask (operators closed: false, cancelled: false)
2023-03-31 11:26:24  [ Source Data Fetcher for Source: HiveSource-ods.test_01 -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[stamp, event, credit_number]) (36/36)#0:28762 ] - [ INFO ]  Split fetcher 0 exited.
